#!/usr/bin/env python3
"""
ğŸ“Š 888-888-88 ç”Ÿäº§çº§è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨
Production-Grade System Evaluation Report Generator
"""

import os
import sys
import json
import asyncio
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class ProductionEvaluator:
    """ç”Ÿäº§çº§è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.evaluation_time = datetime.now()
        self.report_data = {}
        
        logger.info("ğŸ“Š ç”Ÿäº§çº§è¯„ä¼°å™¨åˆå§‹åŒ–")
    
    async def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        try:
            logger.info("ğŸ” å¼€å§‹ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
            
            # 1. ç³»ç»Ÿæ¶æ„è¯„ä¼°
            architecture_score = await self._evaluate_architecture()
            
            # 2. ä»£ç è´¨é‡è¯„ä¼°
            code_quality_score = await self._evaluate_code_quality()
            
            # 3. åŠŸèƒ½å®Œæ•´æ€§è¯„ä¼°
            functionality_score = await self._evaluate_functionality()
            
            # 4. æ€§èƒ½è¯„ä¼°
            performance_score = await self._evaluate_performance()
            
            # 5. å®‰å…¨æ€§è¯„ä¼°
            security_score = await self._evaluate_security()
            
            # 6. å¯ç»´æŠ¤æ€§è¯„ä¼°
            maintainability_score = await self._evaluate_maintainability()
            
            # 7. ç”Ÿäº§å°±ç»ªåº¦è¯„ä¼°
            production_readiness_score = await self._evaluate_production_readiness()
            
            # è®¡ç®—æ€»åˆ†
            total_score = (
                architecture_score * 0.15 +
                code_quality_score * 0.20 +
                functionality_score * 0.25 +
                performance_score * 0.15 +
                security_score * 0.10 +
                maintainability_score * 0.10 +
                production_readiness_score * 0.05
            )
            
            # ç”ŸæˆæŠ¥å‘Š
            report = {
                "evaluation_info": {
                    "evaluation_time": self.evaluation_time.isoformat(),
                    "system_name": "888-888-88 é‡åŒ–äº¤æ˜“ç³»ç»Ÿ",
                    "version": "2.0.0",
                    "evaluator": "Production Evaluation System"
                },
                "overall_score": {
                    "total_score": round(total_score, 1),
                    "grade": self._get_grade(total_score),
                    "status": self._get_status(total_score)
                },
                "detailed_scores": {
                    "architecture": {
                        "score": architecture_score,
                        "weight": "15%",
                        "description": "ç³»ç»Ÿæ¶æ„è®¾è®¡è´¨é‡"
                    },
                    "code_quality": {
                        "score": code_quality_score,
                        "weight": "20%",
                        "description": "ä»£ç è´¨é‡å’Œè§„èŒƒæ€§"
                    },
                    "functionality": {
                        "score": functionality_score,
                        "weight": "25%",
                        "description": "åŠŸèƒ½å®Œæ•´æ€§å’Œæ­£ç¡®æ€§"
                    },
                    "performance": {
                        "score": performance_score,
                        "weight": "15%",
                        "description": "ç³»ç»Ÿæ€§èƒ½å’Œæ•ˆç‡"
                    },
                    "security": {
                        "score": security_score,
                        "weight": "10%",
                        "description": "å®‰å…¨æ€§å’Œæ•°æ®ä¿æŠ¤"
                    },
                    "maintainability": {
                        "score": maintainability_score,
                        "weight": "10%",
                        "description": "å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§"
                    },
                    "production_readiness": {
                        "score": production_readiness_score,
                        "weight": "5%",
                        "description": "ç”Ÿäº§ç¯å¢ƒå°±ç»ªåº¦"
                    }
                },
                "system_analysis": await self._analyze_system_components(),
                "web_interface_analysis": await self._analyze_web_interface(),
                "recommendations": await self._generate_recommendations(total_score),
                "deployment_guide": await self._generate_deployment_guide()
            }
            
            self.report_data = report
            return report
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šå¤±è´¥: {e}")
            raise
    
    async def _evaluate_architecture(self) -> float:
        """è¯„ä¼°ç³»ç»Ÿæ¶æ„"""
        try:
            logger.info("ğŸ—ï¸ è¯„ä¼°ç³»ç»Ÿæ¶æ„...")
            
            score = 0.0
            max_score = 100.0
            
            # æ£€æŸ¥æ¨¡å—åŒ–è®¾è®¡
            if Path("src").exists():
                modules = list(Path("src").iterdir())
                if len(modules) >= 5:  # è‡³å°‘5ä¸ªä¸»è¦æ¨¡å—
                    score += 20
                    logger.info("âœ… æ¨¡å—åŒ–è®¾è®¡è‰¯å¥½")
                else:
                    score += 10
                    logger.warning("âš ï¸ æ¨¡å—åŒ–è®¾è®¡ä¸€èˆ¬")
            
            # æ£€æŸ¥åˆ†å±‚æ¶æ„
            expected_layers = ["core", "ai", "trading", "monitoring", "web", "config"]
            existing_layers = [d.name for d in Path("src").iterdir() if d.is_dir()]
            layer_coverage = len(set(expected_layers) & set(existing_layers)) / len(expected_layers)
            score += layer_coverage * 25
            logger.info(f"âœ… åˆ†å±‚æ¶æ„è¦†ç›–ç‡: {layer_coverage:.1%}")
            
            # æ£€æŸ¥é…ç½®ç®¡ç†
            if Path("src/config").exists():
                score += 15
                logger.info("âœ… é…ç½®ç®¡ç†æ¨¡å—å­˜åœ¨")
            
            # æ£€æŸ¥é”™è¯¯å¤„ç†
            if Path("src/core/error_handling_system.py").exists():
                score += 20
                logger.info("âœ… é”™è¯¯å¤„ç†ç³»ç»Ÿå®Œæ•´")
            
            # æ£€æŸ¥ç›‘æ§ç³»ç»Ÿ
            if Path("src/monitoring").exists():
                score += 20
                logger.info("âœ… ç›‘æ§ç³»ç»Ÿå®Œæ•´")
            
            logger.info(f"ğŸ—ï¸ æ¶æ„è¯„ä¼°å¾—åˆ†: {score:.1f}/{max_score}")
            return score
            
        except Exception as e:
            logger.error(f"âŒ æ¶æ„è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    async def _evaluate_code_quality(self) -> float:
        """è¯„ä¼°ä»£ç è´¨é‡"""
        try:
            logger.info("ğŸ“ è¯„ä¼°ä»£ç è´¨é‡...")
            
            score = 0.0
            max_score = 100.0
            
            # ç»Ÿè®¡ä»£ç æ–‡ä»¶
            python_files = list(Path(".").rglob("*.py"))
            total_files = len(python_files)
            
            if total_files == 0:
                return 0.0
            
            # æ£€æŸ¥æ–‡æ¡£å­—ç¬¦ä¸²
            documented_files = 0
            type_annotated_files = 0
            error_handled_files = 0
            
            for file_path in python_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æ£€æŸ¥æ–‡æ¡£å­—ç¬¦ä¸²
                    if '"""' in content or "'''" in content:
                        documented_files += 1
                    
                    # æ£€æŸ¥ç±»å‹æ³¨è§£
                    if "from typing import" in content or ": str" in content or ": int" in content:
                        type_annotated_files += 1
                    
                    # æ£€æŸ¥é”™è¯¯å¤„ç†
                    if "try:" in content and "except" in content:
                        error_handled_files += 1
                        
                except Exception:
                    continue
            
            # è®¡ç®—è¦†ç›–ç‡
            doc_coverage = documented_files / total_files
            type_coverage = type_annotated_files / total_files
            error_coverage = error_handled_files / total_files
            
            score += doc_coverage * 30  # æ–‡æ¡£è¦†ç›–ç‡
            score += type_coverage * 30  # ç±»å‹æ³¨è§£è¦†ç›–ç‡
            score += error_coverage * 25  # é”™è¯¯å¤„ç†è¦†ç›–ç‡
            
            # æ£€æŸ¥ä»£ç è§„èŒƒ
            if any("from loguru import logger" in f.read_text(encoding='utf-8', errors='ignore') 
                   for f in python_files[:5]):  # æ£€æŸ¥å‰5ä¸ªæ–‡ä»¶
                score += 15
                logger.info("âœ… ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ")
            
            logger.info(f"ğŸ“ ä»£ç è´¨é‡è¯„ä¼°:")
            logger.info(f"   æ–‡æ¡£è¦†ç›–ç‡: {doc_coverage:.1%}")
            logger.info(f"   ç±»å‹æ³¨è§£è¦†ç›–ç‡: {type_coverage:.1%}")
            logger.info(f"   é”™è¯¯å¤„ç†è¦†ç›–ç‡: {error_coverage:.1%}")
            logger.info(f"   æ€»å¾—åˆ†: {score:.1f}/{max_score}")
            
            return score
            
        except Exception as e:
            logger.error(f"âŒ ä»£ç è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    async def _evaluate_functionality(self) -> float:
        """è¯„ä¼°åŠŸèƒ½å®Œæ•´æ€§"""
        try:
            logger.info("âš™ï¸ è¯„ä¼°åŠŸèƒ½å®Œæ•´æ€§...")
            
            score = 0.0
            max_score = 100.0
            
            # æ ¸å¿ƒåŠŸèƒ½æ£€æŸ¥
            core_features = {
                "AIæ¨¡å‹ç®¡ç†": Path("src/ai/ai_model_manager.py").exists(),
                "AIæ€§èƒ½ç›‘æ§": Path("src/ai/ai_performance_monitor.py").exists(),
                "AIèåˆå¼•æ“": Path("src/ai/enhanced_ai_fusion_engine.py").exists(),
                "é”™è¯¯å¤„ç†ç³»ç»Ÿ": Path("src/core/error_handling_system.py").exists(),
                "ç³»ç»Ÿç›‘æ§": Path("src/monitoring/system_monitor.py").exists(),
                "é…ç½®ç®¡ç†": Path("src/config/api_config.py").exists(),
                "Webç•Œé¢": Path("src/web/enhanced_app.py").exists(),
                "ä¸€é”®å¯åŠ¨": Path("one_click_start.py").exists()
            }
            
            implemented_features = sum(core_features.values())
            total_features = len(core_features)
            feature_coverage = implemented_features / total_features
            
            score += feature_coverage * 60  # æ ¸å¿ƒåŠŸèƒ½è¦†ç›–ç‡
            
            # é«˜çº§åŠŸèƒ½æ£€æŸ¥
            advanced_features = {
                "å®æ—¶æ•°æ®å¤„ç†": "WebSocket" in Path("src/web/enhanced_app.py").read_text(encoding='utf-8', errors='ignore'),
                "æ‰¹é‡é¢„æµ‹": "batch" in Path("src/ai/ai_model_manager.py").read_text(encoding='utf-8', errors='ignore'),
                "è‡ªåŠ¨æ¢å¤": "recovery" in Path("src/core/error_handling_system.py").read_text(encoding='utf-8', errors='ignore'),
                "æ€§èƒ½ä¼˜åŒ–": "asyncio" in Path("src/ai/enhanced_ai_fusion_engine.py").read_text(encoding='utf-8', errors='ignore')
            }
            
            advanced_implemented = sum(advanced_features.values())
            advanced_total = len(advanced_features)
            advanced_coverage = advanced_implemented / advanced_total
            
            score += advanced_coverage * 25  # é«˜çº§åŠŸèƒ½è¦†ç›–ç‡
            
            # é›†æˆåº¦æ£€æŸ¥
            if Path("start_production_system.py").exists():
                score += 15
                logger.info("âœ… ç³»ç»Ÿé›†æˆå®Œæ•´")
            
            logger.info(f"âš™ï¸ åŠŸèƒ½å®Œæ•´æ€§è¯„ä¼°:")
            logger.info(f"   æ ¸å¿ƒåŠŸèƒ½: {implemented_features}/{total_features} ({feature_coverage:.1%})")
            logger.info(f"   é«˜çº§åŠŸèƒ½: {advanced_implemented}/{advanced_total} ({advanced_coverage:.1%})")
            logger.info(f"   æ€»å¾—åˆ†: {score:.1f}/{max_score}")
            
            return score
            
        except Exception as e:
            logger.error(f"âŒ åŠŸèƒ½å®Œæ•´æ€§è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    async def _evaluate_performance(self) -> float:
        """è¯„ä¼°ç³»ç»Ÿæ€§èƒ½"""
        try:
            logger.info("âš¡ è¯„ä¼°ç³»ç»Ÿæ€§èƒ½...")
            
            score = 0.0
            max_score = 100.0
            
            # å¼‚æ­¥ç¼–ç¨‹æ£€æŸ¥
            python_files = list(Path("src").rglob("*.py"))
            async_files = 0
            
            for file_path in python_files:
                try:
                    content = file_path.read_text(encoding='utf-8', errors='ignore')
                    if "async def" in content or "await" in content:
                        async_files += 1
                except Exception:
                    continue
            
            if python_files:
                async_coverage = async_files / len(python_files)
                score += async_coverage * 30
                logger.info(f"âœ… å¼‚æ­¥ç¼–ç¨‹è¦†ç›–ç‡: {async_coverage:.1%}")
            
            # å¹¶å‘å¤„ç†æ£€æŸ¥
            if any("asyncio.gather" in f.read_text(encoding='utf-8', errors='ignore') 
                   for f in python_files if f.exists()):
                score += 20
                logger.info("âœ… æ”¯æŒå¹¶å‘å¤„ç†")
            
            # ç¼“å­˜æœºåˆ¶æ£€æŸ¥
            if any("cache" in f.read_text(encoding='utf-8', errors='ignore').lower() 
                   for f in python_files if f.exists()):
                score += 15
                logger.info("âœ… å®ç°ç¼“å­˜æœºåˆ¶")
            
            # å†…å­˜ç®¡ç†æ£€æŸ¥
            if Path("src/ai/ai_model_manager.py").exists():
                content = Path("src/ai/ai_model_manager.py").read_text(encoding='utf-8', errors='ignore')
                if "memory" in content.lower() and "unload" in content.lower():
                    score += 20
                    logger.info("âœ… æ™ºèƒ½å†…å­˜ç®¡ç†")
            
            # æ‰¹å¤„ç†æ£€æŸ¥
            if any("batch" in f.read_text(encoding='utf-8', errors='ignore').lower() 
                   for f in python_files if f.exists()):
                score += 15
                logger.info("âœ… æ”¯æŒæ‰¹å¤„ç†")
            
            logger.info(f"âš¡ æ€§èƒ½è¯„ä¼°å¾—åˆ†: {score:.1f}/{max_score}")
            return score
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    async def _evaluate_security(self) -> float:
        """è¯„ä¼°å®‰å…¨æ€§"""
        try:
            logger.info("ğŸ”’ è¯„ä¼°ç³»ç»Ÿå®‰å…¨æ€§...")
            
            score = 0.0
            max_score = 100.0
            
            # ç¯å¢ƒå˜é‡ä½¿ç”¨æ£€æŸ¥
            python_files = list(Path("src").rglob("*.py"))
            env_usage = 0
            
            for file_path in python_files:
                try:
                    content = file_path.read_text(encoding='utf-8', errors='ignore')
                    if "os.getenv" in content or "os.environ" in content:
                        env_usage += 1
                except Exception:
                    continue
            
            if env_usage > 0:
                score += 25
                logger.info("âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†æ•æ„Ÿä¿¡æ¯")
            
            # å¯†é’¥ç®¡ç†æ£€æŸ¥
            if Path("src/config/api_config.py").exists():
                content = Path("src/config/api_config.py").read_text(encoding='utf-8', errors='ignore')
                if "api_key" in content and "secret" in content:
                    score += 25
                    logger.info("âœ… å®ç°APIå¯†é’¥ç®¡ç†")
            
            # è¾“å…¥éªŒè¯æ£€æŸ¥
            if any("validate" in f.read_text(encoding='utf-8', errors='ignore').lower() 
                   for f in python_files if f.exists()):
                score += 20
                logger.info("âœ… å®ç°è¾“å…¥éªŒè¯")
            
            # é”™è¯¯å¤„ç†å®‰å…¨æ£€æŸ¥
            if Path("src/core/error_handling_system.py").exists():
                score += 20
                logger.info("âœ… å®‰å…¨çš„é”™è¯¯å¤„ç†")
            
            # æ—¥å¿—å®‰å…¨æ£€æŸ¥
            if any("logger" in f.read_text(encoding='utf-8', errors='ignore') 
                   for f in python_files if f.exists()):
                score += 10
                logger.info("âœ… å®‰å…¨æ—¥å¿—è®°å½•")
            
            logger.info(f"ğŸ”’ å®‰å…¨æ€§è¯„ä¼°å¾—åˆ†: {score:.1f}/{max_score}")
            return score
            
        except Exception as e:
            logger.error(f"âŒ å®‰å…¨æ€§è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    async def _evaluate_maintainability(self) -> float:
        """è¯„ä¼°å¯ç»´æŠ¤æ€§"""
        try:
            logger.info("ğŸ”§ è¯„ä¼°å¯ç»´æŠ¤æ€§...")
            
            score = 0.0
            max_score = 100.0
            
            # æ¨¡å—åŒ–ç¨‹åº¦
            if Path("src").exists():
                modules = [d for d in Path("src").iterdir() if d.is_dir()]
                if len(modules) >= 5:
                    score += 25
                    logger.info("âœ… é«˜åº¦æ¨¡å—åŒ–")
            
            # é…ç½®å¤–éƒ¨åŒ–
            if Path("config").exists() or Path("src/config").exists():
                score += 20
                logger.info("âœ… é…ç½®å¤–éƒ¨åŒ–")
            
            # æ–‡æ¡£å®Œæ•´æ€§
            readme_exists = Path("README.md").exists()
            docs_exist = any(Path(".").glob("*.md"))
            
            if readme_exists:
                score += 15
                logger.info("âœ… READMEæ–‡æ¡£å­˜åœ¨")
            
            if docs_exist:
                score += 10
                logger.info("âœ… æ–‡æ¡£å®Œæ•´")
            
            # æµ‹è¯•è¦†ç›–
            test_files = list(Path(".").rglob("test_*.py")) + list(Path(".").rglob("*_test.py"))
            if test_files:
                score += 15
                logger.info("âœ… åŒ…å«æµ‹è¯•æ–‡ä»¶")
            
            # ç‰ˆæœ¬æ§åˆ¶
            if Path(".git").exists():
                score += 15
                logger.info("âœ… ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶")
            
            logger.info(f"ğŸ”§ å¯ç»´æŠ¤æ€§è¯„ä¼°å¾—åˆ†: {score:.1f}/{max_score}")
            return score
            
        except Exception as e:
            logger.error(f"âŒ å¯ç»´æŠ¤æ€§è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    async def _evaluate_production_readiness(self) -> float:
        """è¯„ä¼°ç”Ÿäº§å°±ç»ªåº¦"""
        try:
            logger.info("ğŸš€ è¯„ä¼°ç”Ÿäº§å°±ç»ªåº¦...")
            
            score = 0.0
            max_score = 100.0
            
            # å¯åŠ¨è„šæœ¬
            if Path("one_click_start.py").exists():
                score += 30
                logger.info("âœ… ä¸€é”®å¯åŠ¨è„šæœ¬")
            
            # å¥åº·æ£€æŸ¥
            if Path("src/web/enhanced_app.py").exists():
                content = Path("src/web/enhanced_app.py").read_text(encoding='utf-8', errors='ignore')
                if "/health" in content:
                    score += 25
                    logger.info("âœ… å¥åº·æ£€æŸ¥ç«¯ç‚¹")
            
            # ç›‘æ§ç³»ç»Ÿ
            if Path("src/monitoring").exists():
                score += 25
                logger.info("âœ… ç›‘æ§ç³»ç»Ÿ")
            
            # æ—¥å¿—ç³»ç»Ÿ
            if Path("logs").exists() or any("logger" in f.read_text(encoding='utf-8', errors='ignore') 
                                           for f in Path("src").rglob("*.py") if f.exists()):
                score += 20
                logger.info("âœ… æ—¥å¿—ç³»ç»Ÿ")
            
            logger.info(f"ğŸš€ ç”Ÿäº§å°±ç»ªåº¦è¯„ä¼°å¾—åˆ†: {score:.1f}/{max_score}")
            return score
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿäº§å°±ç»ªåº¦è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    def _get_grade(self, score: float) -> str:
        """è·å–è¯„çº§"""
        if score >= 90:
            return "A+"
        elif score >= 85:
            return "A"
        elif score >= 80:
            return "A-"
        elif score >= 75:
            return "B+"
        elif score >= 70:
            return "B"
        elif score >= 65:
            return "B-"
        elif score >= 60:
            return "C+"
        elif score >= 55:
            return "C"
        elif score >= 50:
            return "C-"
        else:
            return "D"
    
    def _get_status(self, score: float) -> str:
        """è·å–çŠ¶æ€"""
        if score >= 85:
            return "ç”Ÿäº§å°±ç»ª"
        elif score >= 70:
            return "æ¥è¿‘ç”Ÿäº§å°±ç»ª"
        elif score >= 55:
            return "éœ€è¦æ”¹è¿›"
        else:
            return "ä¸é€‚åˆç”Ÿäº§"
    
    async def _analyze_system_components(self) -> Dict[str, Any]:
        """åˆ†æç³»ç»Ÿç»„ä»¶"""
        try:
            components = {}
            
            # AIç»„ä»¶åˆ†æ
            ai_dir = Path("src/ai")
            if ai_dir.exists():
                ai_files = list(ai_dir.glob("*.py"))
                components["ai_system"] = {
                    "files_count": len(ai_files),
                    "components": [f.stem for f in ai_files],
                    "status": "å®Œæ•´" if len(ai_files) >= 3 else "åŸºç¡€"
                }
            
            # æ ¸å¿ƒç»„ä»¶åˆ†æ
            core_dir = Path("src/core")
            if core_dir.exists():
                core_files = list(core_dir.glob("*.py"))
                components["core_system"] = {
                    "files_count": len(core_files),
                    "components": [f.stem for f in core_files],
                    "status": "å®Œæ•´" if len(core_files) >= 1 else "ç¼ºå¤±"
                }
            
            # Webç»„ä»¶åˆ†æ
            web_dir = Path("src/web")
            if web_dir.exists():
                web_files = list(web_dir.glob("*.py"))
                components["web_interface"] = {
                    "files_count": len(web_files),
                    "components": [f.stem for f in web_files],
                    "status": "å®Œæ•´" if len(web_files) >= 1 else "ç¼ºå¤±"
                }
            
            return components
            
        except Exception as e:
            logger.error(f"âŒ ç³»ç»Ÿç»„ä»¶åˆ†æå¤±è´¥: {e}")
            return {}
    
    async def _analyze_web_interface(self) -> Dict[str, Any]:
        """åˆ†æWebç•Œé¢"""
        try:
            web_analysis = {
                "status": "æœªå®ç°",
                "features": [],
                "endpoints": [],
                "technologies": []
            }
            
            web_file = Path("src/web/enhanced_app.py")
            if web_file.exists():
                content = web_file.read_text(encoding='utf-8', errors='ignore')
                
                web_analysis["status"] = "å·²å®ç°"
                
                # æ£€æŸ¥åŠŸèƒ½
                if "WebSocket" in content:
                    web_analysis["features"].append("å®æ—¶æ•°æ®æ¨é€")
                if "FastAPI" in content:
                    web_analysis["technologies"].append("FastAPI")
                if "/api/" in content:
                    web_analysis["features"].append("RESTful API")
                if "dashboard" in content.lower():
                    web_analysis["features"].append("ç®¡ç†ä»ªè¡¨æ¿")
                
                # æå–APIç«¯ç‚¹
                import re
                endpoints = re.findall(r'@app\.(get|post|put|delete)\("([^"]+)"', content)
                web_analysis["endpoints"] = [{"method": method.upper(), "path": path} 
                                           for method, path in endpoints]
            
            return web_analysis
            
        except Exception as e:
            logger.error(f"âŒ Webç•Œé¢åˆ†æå¤±è´¥: {e}")
            return {"status": "åˆ†æå¤±è´¥", "error": str(e)}
    
    async def _generate_recommendations(self, total_score: float) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if total_score < 85:
            recommendations.append("å»ºè®®å¢åŠ å•å…ƒæµ‹è¯•è¦†ç›–ç‡")
            recommendations.append("å®Œå–„APIæ–‡æ¡£å’Œç”¨æˆ·æ‰‹å†Œ")
        
        if total_score < 80:
            recommendations.append("åŠ å¼ºé”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•")
            recommendations.append("ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½å’Œå†…å­˜ä½¿ç”¨")
        
        if total_score < 75:
            recommendations.append("å¢å¼ºå®‰å…¨æ€§æªæ–½")
            recommendations.append("å®Œå–„ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ")
        
        if total_score < 70:
            recommendations.append("é‡æ„ä»£ç æé«˜å¯ç»´æŠ¤æ€§")
            recommendations.append("æ·»åŠ æ›´å¤šåŠŸèƒ½æµ‹è¯•")
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å®šæœŸè¿›è¡Œå®‰å…¨å®¡è®¡",
            "å»ºç«‹CI/CDæµæ°´çº¿",
            "åˆ¶å®šç¾éš¾æ¢å¤è®¡åˆ’",
            "ä¼˜åŒ–æ•°æ®åº“æ€§èƒ½",
            "å¢åŠ è´Ÿè½½æµ‹è¯•"
        ])
        
        return recommendations
    
    async def _generate_deployment_guide(self) -> Dict[str, Any]:
        """ç”Ÿæˆéƒ¨ç½²æŒ‡å—"""
        return {
            "prerequisites": [
                "Python 3.8+",
                "pipåŒ…ç®¡ç†å™¨",
                "è‡³å°‘4GBå†…å­˜",
                "ç¨³å®šçš„ç½‘ç»œè¿æ¥"
            ],
            "installation_steps": [
                "1. å…‹éš†é¡¹ç›®ä»£ç ",
                "2. å®‰è£…ä¾èµ–: pip install -r requirements.txt",
                "3. é…ç½®ç¯å¢ƒå˜é‡",
                "4. è¿è¡Œ: python one_click_start.py"
            ],
            "configuration": [
                "è®¾ç½®äº¤æ˜“æ‰€APIå¯†é’¥",
                "é…ç½®ç›‘æ§å‘Šè­¦",
                "è°ƒæ•´AIæ¨¡å‹å‚æ•°",
                "è®¾ç½®é£é™©ç®¡ç†è§„åˆ™"
            ],
            "monitoring": [
                "è®¿é—®Webç•Œé¢: http://localhost:8000",
                "æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—",
                "ç›‘æ§æ€§èƒ½æŒ‡æ ‡",
                "è®¾ç½®å‘Šè­¦é€šçŸ¥"
            ]
        }
    
    async def save_report(self, filename: str = None):
        """ä¿å­˜æŠ¥å‘Š"""
        try:
            if not filename:
                filename = f"production_evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.report_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            raise

async def main():
    """ä¸»å‡½æ•°"""
    try:
        evaluator = ProductionEvaluator()
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        report = await evaluator.generate_comprehensive_report()
        
        # ä¿å­˜æŠ¥å‘Š
        filename = await evaluator.save_report()
        
        # æ˜¾ç¤ºæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ‰ 888-888-88 ç”Ÿäº§çº§è¯„ä¼°æŠ¥å‘Š")
        print("="*60)
        print(f"ğŸ“Š æ€»ä½“è¯„åˆ†: {report['overall_score']['total_score']}/100")
        print(f"ğŸ† ç³»ç»Ÿç­‰çº§: {report['overall_score']['grade']}")
        print(f"âœ… ç³»ç»ŸçŠ¶æ€: {report['overall_score']['status']}")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {filename}")
        print("="*60)
        
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
