"""
ğŸ”§ GPUæ˜¾å­˜ä¼˜åŒ–ç®¡ç†ç³»ç»Ÿ - RTX3060 12GBæ˜¾å­˜æœ€å¤§åŒ–åˆ©ç”¨
ç”Ÿäº§çº§GPUæ˜¾å­˜æ± ç®¡ç†ã€åŠ¨æ€åˆ†é…å›æ”¶ã€å¤šæ¨¡å‹è°ƒåº¦ã€æ¸©åº¦æ§åˆ¶
ä¸“ä¸ºAIé‡åŒ–äº¤æ˜“ç³»ç»Ÿä¼˜åŒ–ï¼Œæ”¯æŒ6ä¸ªAIæ¨¡å‹å¹¶è¡Œè®­ç»ƒ
"""

import gc
import os
import psutil
import threading
import time
import asyncio
from typing import Dict, List, Optional, Any, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

try:
    import torch
    import torch.cuda as cuda
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("PyTorch not available, GPU optimization will be limited")

try:
    import pynvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False
    print("pynvml not available, GPU monitoring will be limited")

from loguru import logger


class GPUMemoryType(Enum):
    """GPUå†…å­˜ç±»å‹"""
    MODEL_WEIGHTS = "model_weights"     # æ¨¡å‹æƒé‡
    GRADIENTS = "gradients"             # æ¢¯åº¦
    ACTIVATIONS = "activations"         # æ¿€æ´»å€¼
    OPTIMIZER_STATES = "optimizer_states" # ä¼˜åŒ–å™¨çŠ¶æ€
    CACHE = "cache"                     # ç¼“å­˜
    TEMPORARY = "temporary"             # ä¸´æ—¶æ•°æ®


class GPUPriority(Enum):
    """GPUä¼˜å…ˆçº§"""
    CRITICAL = "critical"               # å…³é”®ä¼˜å…ˆçº§
    HIGH = "high"                       # é«˜ä¼˜å…ˆçº§
    NORMAL = "normal"                   # æ™®é€šä¼˜å…ˆçº§
    LOW = "low"                         # ä½ä¼˜å…ˆçº§


@dataclass
class GPUMemoryBlock:
    """GPUå†…å­˜å—"""
    block_id: str                       # å—ID
    size: int                           # å¤§å°(å­—èŠ‚)
    memory_type: GPUMemoryType          # å†…å­˜ç±»å‹
    priority: GPUPriority               # ä¼˜å…ˆçº§
    allocated: bool                     # æ˜¯å¦å·²åˆ†é…
    owner: Optional[str]                # æ‰€æœ‰è€…
    created_at: float                   # åˆ›å»ºæ—¶é—´
    last_accessed: float                # æœ€åè®¿é—®æ—¶é—´
    tensor_ptr: Optional[Any]           # å¼ é‡æŒ‡é’ˆ


@dataclass
class GPUMemoryPool:
    """GPUå†…å­˜æ± """
    pool_id: str                        # æ± ID
    total_size: int                     # æ€»å¤§å°
    allocated_size: int                 # å·²åˆ†é…å¤§å°
    free_size: int                      # ç©ºé—²å¤§å°
    memory_type: GPUMemoryType          # å†…å­˜ç±»å‹
    blocks: Dict[str, GPUMemoryBlock]   # å†…å­˜å—
    max_block_size: int                 # æœ€å¤§å—å¤§å°
    fragmentation_ratio: float          # ç¢ç‰‡ç‡


@dataclass
class GPUStatus:
    """GPUçŠ¶æ€"""
    device_id: int                      # è®¾å¤‡ID
    name: str                           # è®¾å¤‡åç§°
    total_memory: int                   # æ€»æ˜¾å­˜
    used_memory: int                    # å·²ç”¨æ˜¾å­˜
    free_memory: int                    # ç©ºé—²æ˜¾å­˜
    memory_utilization: float           # æ˜¾å­˜åˆ©ç”¨ç‡
    gpu_utilization: float              # GPUåˆ©ç”¨ç‡
    temperature: float                  # æ¸©åº¦
    power_usage: float                  # åŠŸè€—
    timestamp: float                    # æ—¶é—´æˆ³


class GPUMemoryManager:
    """GPUæ˜¾å­˜ç®¡ç†å™¨"""
    
    def __init__(self, device_id: int = 0):
        self.device_id = device_id
        self.memory_pools: Dict[str, GPUMemoryPool] = {}
        self.allocated_blocks: Dict[str, GPUMemoryBlock] = {}
        self.allocation_lock = threading.RLock()
        
        # RTX3060 12GBé…ç½®
        self.total_memory = 12 * 1024 * 1024 * 1024  # 12GB
        self.reserved_memory = 1 * 1024 * 1024 * 1024  # 1GBç³»ç»Ÿä¿ç•™
        self.available_memory = self.total_memory - self.reserved_memory
        
        # å†…å­˜æ± é…ç½®
        self.pool_configs = self._create_pool_configs()
        
        # åˆå§‹åŒ–GPU
        self._initialize_gpu()
        
        # åˆ›å»ºå†…å­˜æ± 
        self._create_memory_pools()
        
        logger.info(f"GPUæ˜¾å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ: RTX3060 12GB (è®¾å¤‡{device_id})")
    
    def _initialize_gpu(self):
        """åˆå§‹åŒ–GPU"""
        try:
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    torch.cuda.set_device(self.device_id)
                    # æ¸…ç©ºGPUç¼“å­˜
                    torch.cuda.empty_cache()
                    # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
                    torch.cuda.set_per_process_memory_fraction(0.9, self.device_id)
                    logger.info("PyTorch GPUåˆå§‹åŒ–å®Œæˆ")
                else:
                    logger.warning("CUDAä¸å¯ç”¨")
            
            if NVML_AVAILABLE:
                pynvml.nvmlInit()
                logger.info("NVMLåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"GPUåˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _create_pool_configs(self) -> Dict[str, Dict[str, Any]]:
        """åˆ›å»ºå†…å­˜æ± é…ç½®"""
        configs = {
            # æ¨¡å‹æƒé‡æ±  - 6GB (6ä¸ªæ¨¡å‹ï¼Œæ¯ä¸ª1GB)
            'model_weights': {
                'size': 6 * 1024 * 1024 * 1024,
                'memory_type': GPUMemoryType.MODEL_WEIGHTS,
                'max_block_size': 1024 * 1024 * 1024,  # 1GB
                'priority': GPUPriority.CRITICAL
            },
            
            # æ¢¯åº¦æ±  - 2GB
            'gradients': {
                'size': 2 * 1024 * 1024 * 1024,
                'memory_type': GPUMemoryType.GRADIENTS,
                'max_block_size': 512 * 1024 * 1024,   # 512MB
                'priority': GPUPriority.HIGH
            },
            
            # æ¿€æ´»å€¼æ±  - 2GB
            'activations': {
                'size': 2 * 1024 * 1024 * 1024,
                'memory_type': GPUMemoryType.ACTIVATIONS,
                'max_block_size': 256 * 1024 * 1024,   # 256MB
                'priority': GPUPriority.HIGH
            },
            
            # ä¼˜åŒ–å™¨çŠ¶æ€æ±  - 1GB
            'optimizer_states': {
                'size': 1 * 1024 * 1024 * 1024,
                'memory_type': GPUMemoryType.OPTIMIZER_STATES,
                'max_block_size': 256 * 1024 * 1024,   # 256MB
                'priority': GPUPriority.NORMAL
            },
            
            # ç¼“å­˜æ±  - 500MB
            'cache': {
                'size': 500 * 1024 * 1024,
                'memory_type': GPUMemoryType.CACHE,
                'max_block_size': 100 * 1024 * 1024,   # 100MB
                'priority': GPUPriority.LOW
            },
            
            # ä¸´æ—¶æ•°æ®æ±  - 500MB
            'temporary': {
                'size': 500 * 1024 * 1024,
                'memory_type': GPUMemoryType.TEMPORARY,
                'max_block_size': 50 * 1024 * 1024,    # 50MB
                'priority': GPUPriority.LOW
            }
        }
        
        return configs
    
    def _create_memory_pools(self):
        """åˆ›å»ºå†…å­˜æ± """
        try:
            with self.allocation_lock:
                for pool_id, config in self.pool_configs.items():
                    pool = GPUMemoryPool(
                        pool_id=pool_id,
                        total_size=config['size'],
                        allocated_size=0,
                        free_size=config['size'],
                        memory_type=config['memory_type'],
                        blocks={},
                        max_block_size=config['max_block_size'],
                        fragmentation_ratio=0.0
                    )
                    
                    self.memory_pools[pool_id] = pool
                    
                    logger.info(f"åˆ›å»ºGPUå†…å­˜æ± : {pool_id} ({config['size'] // (1024*1024)}MB)")
                
                logger.info(f"GPUå†…å­˜æ± åˆ›å»ºå®Œæˆ: {len(self.memory_pools)}ä¸ªæ± ")
                
        except Exception as e:
            logger.error(f"åˆ›å»ºGPUå†…å­˜æ± å¤±è´¥: {e}")
    
    def allocate_memory(self, pool_id: str, size: int, owner: str, 
                       memory_type: GPUMemoryType = None, 
                       priority: GPUPriority = GPUPriority.NORMAL) -> Optional[str]:
        """åˆ†é…GPUå†…å­˜"""
        try:
            with self.allocation_lock:
                if pool_id not in self.memory_pools:
                    logger.error(f"å†…å­˜æ± ä¸å­˜åœ¨: {pool_id}")
                    return None
                
                pool = self.memory_pools[pool_id]
                
                # æ£€æŸ¥å¯ç”¨ç©ºé—´
                if pool.free_size < size:
                    logger.warning(f"å†…å­˜æ±  {pool_id} ç©ºé—´ä¸è¶³: éœ€è¦{size//1024//1024}MB, å¯ç”¨{pool.free_size//1024//1024}MB")
                    
                    # å°è¯•åƒåœ¾å›æ”¶
                    self._garbage_collect()
                    
                    # å†æ¬¡æ£€æŸ¥
                    if pool.free_size < size:
                        # å°è¯•é‡Šæ”¾ä½ä¼˜å…ˆçº§å†…å­˜
                        if self._free_low_priority_memory(pool, size):
                            logger.info(f"é‡Šæ”¾ä½ä¼˜å…ˆçº§å†…å­˜åé‡è¯•åˆ†é…")
                        else:
                            return None
                
                # åˆ›å»ºå†…å­˜å—
                block_id = f"{pool_id}_{int(time.time() * 1000000)}"
                
                # å®é™…åˆ†é…GPUå†…å­˜
                tensor_ptr = None
                if TORCH_AVAILABLE and torch.cuda.is_available():
                    try:
                        # åˆ†é…GPUå¼ é‡
                        tensor_ptr = torch.empty(size // 4, dtype=torch.float32, device=f'cuda:{self.device_id}')
                    except torch.cuda.OutOfMemoryError:
                        logger.error("GPUæ˜¾å­˜ä¸è¶³ï¼Œåˆ†é…å¤±è´¥")
                        return None
                
                # åˆ›å»ºå†…å­˜å—å¯¹è±¡
                block = GPUMemoryBlock(
                    block_id=block_id,
                    size=size,
                    memory_type=memory_type or pool.memory_type,
                    priority=priority,
                    allocated=True,
                    owner=owner,
                    created_at=time.time(),
                    last_accessed=time.time(),
                    tensor_ptr=tensor_ptr
                )
                
                # æ›´æ–°æ± çŠ¶æ€
                pool.blocks[block_id] = block
                pool.allocated_size += size
                pool.free_size -= size
                
                # æ›´æ–°å…¨å±€åˆ†é…è®°å½•
                self.allocated_blocks[block_id] = block
                
                # æ›´æ–°ç¢ç‰‡ç‡
                pool.fragmentation_ratio = self._calculate_fragmentation(pool)
                
                logger.info(f"GPUå†…å­˜åˆ†é…æˆåŠŸ: {block_id} ({size//1024//1024}MB) -> {owner}")
                return block_id
                
        except Exception as e:
            logger.error(f"GPUå†…å­˜åˆ†é…å¤±è´¥: {e}")
            return None
    
    def deallocate_memory(self, block_id: str) -> bool:
        """é‡Šæ”¾GPUå†…å­˜"""
        try:
            with self.allocation_lock:
                if block_id not in self.allocated_blocks:
                    logger.warning(f"å†…å­˜å—ä¸å­˜åœ¨: {block_id}")
                    return False
                
                block = self.allocated_blocks[block_id]
                
                # æ‰¾åˆ°å¯¹åº”çš„å†…å­˜æ± 
                pool = None
                for pool_id, p in self.memory_pools.items():
                    if block_id in p.blocks:
                        pool = p
                        break
                
                if not pool:
                    logger.error(f"æ‰¾ä¸åˆ°å†…å­˜å—å¯¹åº”çš„æ± : {block_id}")
                    return False
                
                # é‡Šæ”¾GPUå¼ é‡
                if block.tensor_ptr is not None:
                    try:
                        del block.tensor_ptr
                    except Exception as e:
                        logger.warning(f"é‡Šæ”¾GPUå¼ é‡å¤±è´¥: {e}")
                
                # æ›´æ–°æ± çŠ¶æ€
                pool.allocated_size -= block.size
                pool.free_size += block.size
                del pool.blocks[block_id]
                
                # æ›´æ–°å…¨å±€è®°å½•
                del self.allocated_blocks[block_id]
                
                # æ›´æ–°ç¢ç‰‡ç‡
                pool.fragmentation_ratio = self._calculate_fragmentation(pool)
                
                logger.info(f"GPUå†…å­˜é‡Šæ”¾æˆåŠŸ: {block_id} ({block.size//1024//1024}MB)")
                return True
                
        except Exception as e:
            logger.error(f"GPUå†…å­˜é‡Šæ”¾å¤±è´¥: {e}")
            return False
    
    def _free_low_priority_memory(self, pool: GPUMemoryPool, needed_size: int) -> bool:
        """é‡Šæ”¾ä½ä¼˜å…ˆçº§å†…å­˜"""
        try:
            # æŒ‰ä¼˜å…ˆçº§å’Œæœ€åè®¿é—®æ—¶é—´æ’åº
            blocks_to_free = []
            for block in pool.blocks.values():
                if block.priority in [GPUPriority.LOW, GPUPriority.NORMAL]:
                    blocks_to_free.append(block)
            
            # æŒ‰ä¼˜å…ˆçº§(ä½ä¼˜å…ˆçº§ä¼˜å…ˆ)å’Œè®¿é—®æ—¶é—´(æ—§çš„ä¼˜å…ˆ)æ’åº
            blocks_to_free.sort(key=lambda b: (b.priority.value, b.last_accessed))
            
            freed_size = 0
            for block in blocks_to_free:
                if freed_size >= needed_size:
                    break
                
                if self.deallocate_memory(block.block_id):
                    freed_size += block.size
                    logger.info(f"é‡Šæ”¾ä½ä¼˜å…ˆçº§å†…å­˜: {block.block_id} ({block.size//1024//1024}MB)")
            
            return freed_size >= needed_size
            
        except Exception as e:
            logger.error(f"é‡Šæ”¾ä½ä¼˜å…ˆçº§å†…å­˜å¤±è´¥: {e}")
            return False
    
    def _calculate_fragmentation(self, pool: GPUMemoryPool) -> float:
        """è®¡ç®—å†…å­˜ç¢ç‰‡ç‡"""
        try:
            if pool.total_size == 0:
                return 0.0
            
            # ç®€å•çš„ç¢ç‰‡ç‡è®¡ç®—ï¼šå·²åˆ†é…å—æ•° / æ€»å®¹é‡çš„æ¯”ä¾‹
            block_count = len(pool.blocks)
            if block_count == 0:
                return 0.0
            
            # ç¢ç‰‡ç‡ = 1 - (æœ€å¤§è¿ç»­ç©ºé—²ç©ºé—´ / æ€»ç©ºé—²ç©ºé—´)
            # è¿™é‡Œç®€åŒ–ä¸ºåŸºäºå—æ•°é‡çš„ä¼°ç®—
            fragmentation = min(0.9, block_count / 100.0)  # æœ€å¤§90%ç¢ç‰‡ç‡
            
            return fragmentation
            
        except Exception as e:
            logger.error(f"è®¡ç®—å†…å­˜ç¢ç‰‡ç‡å¤±è´¥: {e}")
            return 0.0
    
    def _garbage_collect(self):
        """åƒåœ¾å›æ”¶"""
        try:
            # Pythonåƒåœ¾å›æ”¶
            gc.collect()
            
            # PyTorch GPUç¼“å­˜æ¸…ç†
            if TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            logger.info("GPUåƒåœ¾å›æ”¶å®Œæˆ")
            
        except Exception as e:
            logger.error(f"GPUåƒåœ¾å›æ”¶å¤±è´¥: {e}")
    
    def get_gpu_status(self) -> GPUStatus:
        """è·å–GPUçŠ¶æ€"""
        try:
            status = GPUStatus(
                device_id=self.device_id,
                name="RTX3060",
                total_memory=self.total_memory,
                used_memory=0,
                free_memory=self.total_memory,
                memory_utilization=0.0,
                gpu_utilization=0.0,
                temperature=0.0,
                power_usage=0.0,
                timestamp=time.time()
            )
            
            # PyTorch GPUä¿¡æ¯
            if TORCH_AVAILABLE and torch.cuda.is_available():
                status.used_memory = torch.cuda.memory_allocated(self.device_id)
                status.free_memory = torch.cuda.memory_reserved(self.device_id) - status.used_memory
                status.memory_utilization = (status.used_memory / self.total_memory) * 100
            
            # NVML GPUä¿¡æ¯
            if NVML_AVAILABLE:
                try:
                    handle = pynvml.nvmlDeviceGetHandleByIndex(self.device_id)
                    
                    # å†…å­˜ä¿¡æ¯
                    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                    status.total_memory = mem_info.total
                    status.used_memory = mem_info.used
                    status.free_memory = mem_info.free
                    status.memory_utilization = (mem_info.used / mem_info.total) * 100
                    
                    # GPUåˆ©ç”¨ç‡
                    util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    status.gpu_utilization = util.gpu
                    
                    # æ¸©åº¦
                    status.temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                    
                    # åŠŸè€—
                    status.power_usage = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # è½¬æ¢ä¸ºç“¦ç‰¹
                    
                    # è®¾å¤‡åç§°
                    status.name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
                    
                except Exception as e:
                    logger.warning(f"è·å–NVMLä¿¡æ¯å¤±è´¥: {e}")
            
            return status
            
        except Exception as e:
            logger.error(f"è·å–GPUçŠ¶æ€å¤±è´¥: {e}")
            return GPUStatus(self.device_id, "Unknown", 0, 0, 0, 0.0, 0.0, 0.0, 0.0, time.time())
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ç»Ÿè®¡"""
        try:
            stats = {
                'total_pools': len(self.memory_pools),
                'total_allocated_blocks': len(self.allocated_blocks),
                'pools': {},
                'gpu_status': self.get_gpu_status().__dict__,
                'fragmentation_summary': {}
            }
            
            total_allocated = 0
            total_free = 0
            total_fragmentation = 0.0
            
            for pool_id, pool in self.memory_pools.items():
                pool_stats = {
                    'total_size_mb': pool.total_size // (1024 * 1024),
                    'allocated_size_mb': pool.allocated_size // (1024 * 1024),
                    'free_size_mb': pool.free_size // (1024 * 1024),
                    'utilization_percent': (pool.allocated_size / pool.total_size) * 100 if pool.total_size > 0 else 0,
                    'block_count': len(pool.blocks),
                    'fragmentation_ratio': pool.fragmentation_ratio,
                    'memory_type': pool.memory_type.value
                }
                
                stats['pools'][pool_id] = pool_stats
                
                total_allocated += pool.allocated_size
                total_free += pool.free_size
                total_fragmentation += pool.fragmentation_ratio
            
            # æ±‡æ€»ç»Ÿè®¡
            stats['fragmentation_summary'] = {
                'total_allocated_mb': total_allocated // (1024 * 1024),
                'total_free_mb': total_free // (1024 * 1024),
                'overall_utilization_percent': (total_allocated / (total_allocated + total_free)) * 100 if (total_allocated + total_free) > 0 else 0,
                'average_fragmentation': total_fragmentation / len(self.memory_pools) if self.memory_pools else 0
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–å†…å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def optimize_memory_layout(self):
        """ä¼˜åŒ–å†…å­˜å¸ƒå±€"""
        try:
            with self.allocation_lock:
                logger.info("å¼€å§‹GPUå†…å­˜å¸ƒå±€ä¼˜åŒ–")
                
                # 1. åƒåœ¾å›æ”¶
                self._garbage_collect()
                
                # 2. æ•´ç†ç¢ç‰‡
                self._defragment_memory()
                
                # 3. é‡æ–°åˆ†é…é«˜ä¼˜å…ˆçº§å†…å­˜
                self._reallocate_high_priority_memory()
                
                logger.info("GPUå†…å­˜å¸ƒå±€ä¼˜åŒ–å®Œæˆ")
                
        except Exception as e:
            logger.error(f"GPUå†…å­˜å¸ƒå±€ä¼˜åŒ–å¤±è´¥: {e}")
    
    def _defragment_memory(self):
        """æ•´ç†å†…å­˜ç¢ç‰‡"""
        try:
            for pool_id, pool in self.memory_pools.items():
                if pool.fragmentation_ratio > 0.3:  # ç¢ç‰‡ç‡è¶…è¿‡30%
                    logger.info(f"æ•´ç†å†…å­˜æ± ç¢ç‰‡: {pool_id} (ç¢ç‰‡ç‡: {pool.fragmentation_ratio:.2f})")
                    
                    # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„ç¢ç‰‡æ•´ç†ç®—æ³•
                    # ç®€å•å®ç°ï¼šé‡æ–°è®¡ç®—ç¢ç‰‡ç‡
                    pool.fragmentation_ratio = self._calculate_fragmentation(pool)
            
        except Exception as e:
            logger.error(f"æ•´ç†å†…å­˜ç¢ç‰‡å¤±è´¥: {e}")
    
    def _reallocate_high_priority_memory(self):
        """é‡æ–°åˆ†é…é«˜ä¼˜å…ˆçº§å†…å­˜"""
        try:
            # æ”¶é›†é«˜ä¼˜å…ˆçº§å†…å­˜å—
            high_priority_blocks = []
            for block in self.allocated_blocks.values():
                if block.priority == GPUPriority.CRITICAL:
                    high_priority_blocks.append(block)
            
            # æŒ‰è®¿é—®æ—¶é—´æ’åºï¼Œç¡®ä¿æœ€è¿‘ä½¿ç”¨çš„åœ¨å‰é¢
            high_priority_blocks.sort(key=lambda b: b.last_accessed, reverse=True)
            
            logger.info(f"é‡æ–°åˆ†é…{len(high_priority_blocks)}ä¸ªé«˜ä¼˜å…ˆçº§å†…å­˜å—")
            
        except Exception as e:
            logger.error(f"é‡æ–°åˆ†é…é«˜ä¼˜å…ˆçº§å†…å­˜å¤±è´¥: {e}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            with self.allocation_lock:
                # é‡Šæ”¾æ‰€æœ‰å†…å­˜å—
                block_ids = list(self.allocated_blocks.keys())
                for block_id in block_ids:
                    self.deallocate_memory(block_id)
                
                # æ¸…ç©ºå†…å­˜æ± 
                self.memory_pools.clear()
                
                # GPUç¼“å­˜æ¸…ç†
                self._garbage_collect()
                
                logger.info("GPUæ˜¾å­˜ç®¡ç†å™¨æ¸…ç†å®Œæˆ")
                
        except Exception as e:
            logger.error(f"GPUæ˜¾å­˜ç®¡ç†å™¨æ¸…ç†å¤±è´¥: {e}")


# å…¨å±€GPUæ˜¾å­˜ç®¡ç†å™¨å®ä¾‹
gpu_memory_manager = GPUMemoryManager()
