"""
ğŸ¤– GPUå¤šæ¨¡å‹è°ƒåº¦å™¨ - 6ä¸ªAIæ¨¡å‹å¹¶è¡Œè®­ç»ƒè°ƒåº¦ç³»ç»Ÿ
æ™ºèƒ½è°ƒåº¦6ä¸ªAIæ¨¡å‹åœ¨RTX3060 12GBä¸Šå¹¶è¡Œè®­ç»ƒï¼Œæ”¯æŒåŠ¨æ€èµ„æºåˆ†é…ã€ä¼˜å…ˆçº§ç®¡ç†ã€æ¸©åº¦æ§åˆ¶
"""

import asyncio
import threading
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

from loguru import logger
from .gpu_memory_optimizer import GPUMemoryManager, GPUPriority, GPUMemoryType


class ModelType(Enum):
    """æ¨¡å‹ç±»å‹"""
    REINFORCEMENT_LEARNING = "reinforcement_learning"   # å¼ºåŒ–å­¦ä¹ 
    TRANSFORMER = "transformer"                         # Transformer
    LSTM = "lstm"                                       # LSTM
    ENSEMBLE = "ensemble"                               # é›†æˆå­¦ä¹ 
    META_LEARNING = "meta_learning"                     # å…ƒå­¦ä¹ 
    TRANSFER_LEARNING = "transfer_learning"             # è¿ç§»å­¦ä¹ 


class ModelStatus(Enum):
    """æ¨¡å‹çŠ¶æ€"""
    IDLE = "idle"                   # ç©ºé—²
    LOADING = "loading"             # åŠ è½½ä¸­
    TRAINING = "training"           # è®­ç»ƒä¸­
    INFERENCE = "inference"         # æ¨ç†ä¸­
    PAUSED = "paused"              # æš‚åœ
    ERROR = "error"                # é”™è¯¯


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    model_id: str                   # æ¨¡å‹ID
    model_type: ModelType           # æ¨¡å‹ç±»å‹
    memory_requirement: int         # å†…å­˜éœ€æ±‚(MB)
    priority: GPUPriority           # ä¼˜å…ˆçº§
    max_batch_size: int             # æœ€å¤§æ‰¹æ¬¡å¤§å°
    training_enabled: bool          # æ˜¯å¦å¯ç”¨è®­ç»ƒ
    inference_enabled: bool         # æ˜¯å¦å¯ç”¨æ¨ç†
    checkpoint_interval: int        # æ£€æŸ¥ç‚¹é—´éš”(ç§’)
    max_training_time: int          # æœ€å¤§è®­ç»ƒæ—¶é—´(ç§’)


@dataclass
class ModelInstance:
    """æ¨¡å‹å®ä¾‹"""
    config: ModelConfig             # æ¨¡å‹é…ç½®
    status: ModelStatus             # å½“å‰çŠ¶æ€
    model: Optional[Any]            # æ¨¡å‹å¯¹è±¡
    optimizer: Optional[Any]        # ä¼˜åŒ–å™¨
    memory_blocks: List[str]        # åˆ†é…çš„å†…å­˜å—
    gpu_utilization: float          # GPUåˆ©ç”¨ç‡
    memory_usage: int               # å†…å­˜ä½¿ç”¨é‡(MB)
    training_steps: int             # è®­ç»ƒæ­¥æ•°
    last_checkpoint: float          # æœ€åæ£€æŸ¥ç‚¹æ—¶é—´
    created_at: float               # åˆ›å»ºæ—¶é—´
    last_active: float              # æœ€åæ´»è·ƒæ—¶é—´
    error_count: int                # é”™è¯¯æ¬¡æ•°


class GPUModelScheduler:
    """GPUå¤šæ¨¡å‹è°ƒåº¦å™¨"""
    
    def __init__(self, memory_manager: GPUMemoryManager):
        self.memory_manager = memory_manager
        self.models: Dict[str, ModelInstance] = {}
        self.scheduler_lock = threading.RLock()
        self.running = False
        self.scheduler_task = None
        
        # è°ƒåº¦é…ç½®
        self.max_concurrent_models = 6         # æœ€å¤§å¹¶å‘æ¨¡å‹æ•°
        self.scheduling_interval = 1.0         # è°ƒåº¦é—´éš”(ç§’)
        self.temperature_threshold = 80.0      # æ¸©åº¦é˜ˆå€¼(Â°C)
        self.memory_threshold = 0.9            # å†…å­˜é˜ˆå€¼(90%)
        
        # é¢„å®šä¹‰æ¨¡å‹é…ç½®
        self.model_configs = self._create_model_configs()
        
        logger.info("GPUå¤šæ¨¡å‹è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _create_model_configs(self) -> Dict[str, ModelConfig]:
        """åˆ›å»ºæ¨¡å‹é…ç½®"""
        configs = {
            # å¼ºåŒ–å­¦ä¹ æ¨¡å‹ - é«˜ä¼˜å…ˆçº§
            'rl_model': ModelConfig(
                model_id='rl_model',
                model_type=ModelType.REINFORCEMENT_LEARNING,
                memory_requirement=1024,  # 1GB
                priority=GPUPriority.CRITICAL,
                max_batch_size=64,
                training_enabled=True,
                inference_enabled=True,
                checkpoint_interval=300,  # 5åˆ†é’Ÿ
                max_training_time=3600    # 1å°æ—¶
            ),
            
            # Transformeræ¨¡å‹ - é«˜ä¼˜å…ˆçº§
            'transformer_model': ModelConfig(
                model_id='transformer_model',
                model_type=ModelType.TRANSFORMER,
                memory_requirement=1024,  # 1GB
                priority=GPUPriority.CRITICAL,
                max_batch_size=32,
                training_enabled=True,
                inference_enabled=True,
                checkpoint_interval=300,
                max_training_time=3600
            ),
            
            # LSTMæ¨¡å‹ - ä¸­ç­‰ä¼˜å…ˆçº§
            'lstm_model': ModelConfig(
                model_id='lstm_model',
                model_type=ModelType.LSTM,
                memory_requirement=512,   # 512MB
                priority=GPUPriority.HIGH,
                max_batch_size=128,
                training_enabled=True,
                inference_enabled=True,
                checkpoint_interval=600,  # 10åˆ†é’Ÿ
                max_training_time=7200    # 2å°æ—¶
            ),
            
            # é›†æˆå­¦ä¹ æ¨¡å‹ - ä¸­ç­‰ä¼˜å…ˆçº§
            'ensemble_model': ModelConfig(
                model_id='ensemble_model',
                model_type=ModelType.ENSEMBLE,
                memory_requirement=768,   # 768MB
                priority=GPUPriority.HIGH,
                max_batch_size=96,
                training_enabled=True,
                inference_enabled=True,
                checkpoint_interval=600,
                max_training_time=7200
            ),
            
            # å…ƒå­¦ä¹ æ¨¡å‹ - ä½ä¼˜å…ˆçº§
            'meta_model': ModelConfig(
                model_id='meta_model',
                model_type=ModelType.META_LEARNING,
                memory_requirement=512,   # 512MB
                priority=GPUPriority.NORMAL,
                max_batch_size=64,
                training_enabled=True,
                inference_enabled=False,
                checkpoint_interval=900,  # 15åˆ†é’Ÿ
                max_training_time=10800   # 3å°æ—¶
            ),
            
            # è¿ç§»å­¦ä¹ æ¨¡å‹ - ä½ä¼˜å…ˆçº§
            'transfer_model': ModelConfig(
                model_id='transfer_model',
                model_type=ModelType.TRANSFER_LEARNING,
                memory_requirement=512,   # 512MB
                priority=GPUPriority.NORMAL,
                max_batch_size=64,
                training_enabled=True,
                inference_enabled=False,
                checkpoint_interval=900,
                max_training_time=10800
            )
        }
        
        return configs
    
    async def start_scheduler(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if self.running:
            return
        
        self.running = True
        self.scheduler_task = asyncio.create_task(self._scheduler_loop())
        
        # åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹
        await self._initialize_models()
        
        logger.info("GPUå¤šæ¨¡å‹è°ƒåº¦å™¨å·²å¯åŠ¨")
    
    async def stop_scheduler(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.running = False
        
        if self.scheduler_task:
            self.scheduler_task.cancel()
            try:
                await self.scheduler_task
            except asyncio.CancelledError:
                pass
        
        # æ¸…ç†æ‰€æœ‰æ¨¡å‹
        await self._cleanup_models()
        
        logger.info("GPUå¤šæ¨¡å‹è°ƒåº¦å™¨å·²åœæ­¢")
    
    async def _scheduler_loop(self):
        """è°ƒåº¦å¾ªç¯"""
        while self.running:
            try:
                # æ£€æŸ¥GPUçŠ¶æ€
                gpu_status = self.memory_manager.get_gpu_status()
                
                # æ¸©åº¦æ§åˆ¶
                if gpu_status.temperature > self.temperature_threshold:
                    await self._handle_overheating()
                
                # å†…å­˜ç®¡ç†
                if gpu_status.memory_utilization > self.memory_threshold * 100:
                    await self._handle_memory_pressure()
                
                # æ¨¡å‹è°ƒåº¦
                await self._schedule_models()
                
                # æ£€æŸ¥ç‚¹ä¿å­˜
                await self._save_checkpoints()
                
                await asyncio.sleep(self.scheduling_interval)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"è°ƒåº¦å¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(5)
    
    async def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        try:
            with self.scheduler_lock:
                for model_id, config in self.model_configs.items():
                    await self._load_model(model_id, config)
            
            logger.info(f"åˆå§‹åŒ–å®Œæˆ: {len(self.models)}ä¸ªæ¨¡å‹")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def _load_model(self, model_id: str, config: ModelConfig) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            if model_id in self.models:
                logger.warning(f"æ¨¡å‹å·²å­˜åœ¨: {model_id}")
                return False
            
            # åˆ†é…å†…å­˜
            memory_blocks = []
            
            # æ¨¡å‹æƒé‡å†…å­˜
            weight_block = self.memory_manager.allocate_memory(
                'model_weights',
                config.memory_requirement * 1024 * 1024,  # è½¬æ¢ä¸ºå­—èŠ‚
                model_id,
                GPUMemoryType.MODEL_WEIGHTS,
                config.priority
            )
            
            if weight_block:
                memory_blocks.append(weight_block)
            else:
                logger.error(f"æ¨¡å‹ {model_id} å†…å­˜åˆ†é…å¤±è´¥")
                return False
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model_instance = ModelInstance(
                config=config,
                status=ModelStatus.LOADING,
                model=None,
                optimizer=None,
                memory_blocks=memory_blocks,
                gpu_utilization=0.0,
                memory_usage=config.memory_requirement,
                training_steps=0,
                last_checkpoint=time.time(),
                created_at=time.time(),
                last_active=time.time(),
                error_count=0
            )
            
            # åˆ›å»ºå®é™…æ¨¡å‹
            if TORCH_AVAILABLE:
                model_instance.model = self._create_torch_model(config)
                if config.training_enabled:
                    model_instance.optimizer = self._create_optimizer(model_instance.model)
            
            model_instance.status = ModelStatus.IDLE
            self.models[model_id] = model_instance
            
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id} ({config.memory_requirement}MB)")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {model_id} - {e}")
            return False
    
    def _create_torch_model(self, config: ModelConfig) -> Optional[nn.Module]:
        """åˆ›å»ºPyTorchæ¨¡å‹"""
        try:
            if not TORCH_AVAILABLE:
                return None
            
            # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºä¸åŒçš„æ¨¡å‹æ¶æ„
            if config.model_type == ModelType.REINFORCEMENT_LEARNING:
                # ç®€å•çš„DQNç½‘ç»œ
                model = nn.Sequential(
                    nn.Linear(100, 256),
                    nn.ReLU(),
                    nn.Linear(256, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10)
                )
            
            elif config.model_type == ModelType.TRANSFORMER:
                # ç®€å•çš„Transformer
                model = nn.Sequential(
                    nn.Linear(100, 512),
                    nn.TransformerEncoderLayer(512, 8),
                    nn.Linear(512, 10)
                )
            
            elif config.model_type == ModelType.LSTM:
                # LSTMç½‘ç»œ
                model = nn.Sequential(
                    nn.LSTM(100, 256, batch_first=True),
                    nn.Linear(256, 10)
                )
            
            else:
                # é»˜è®¤å…¨è¿æ¥ç½‘ç»œ
                model = nn.Sequential(
                    nn.Linear(100, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10)
                )
            
            # ç§»åŠ¨åˆ°GPU
            device = f'cuda:{self.memory_manager.device_id}'
            model = model.to(device)
            
            return model
            
        except Exception as e:
            logger.error(f"åˆ›å»ºPyTorchæ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def _create_optimizer(self, model: nn.Module) -> Optional[Any]:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        try:
            if not TORCH_AVAILABLE or model is None:
                return None
            
            return torch.optim.Adam(model.parameters(), lr=0.001)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºä¼˜åŒ–å™¨å¤±è´¥: {e}")
            return None
    
    async def _schedule_models(self):
        """è°ƒåº¦æ¨¡å‹"""
        try:
            with self.scheduler_lock:
                # è·å–æ´»è·ƒæ¨¡å‹
                active_models = [m for m in self.models.values() 
                               if m.status in [ModelStatus.TRAINING, ModelStatus.INFERENCE]]
                
                # å¦‚æœæ´»è·ƒæ¨¡å‹æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œæš‚åœä½ä¼˜å…ˆçº§æ¨¡å‹
                if len(active_models) > self.max_concurrent_models:
                    await self._pause_low_priority_models(active_models)
                
                # å¯åŠ¨ç©ºé—²çš„é«˜ä¼˜å…ˆçº§æ¨¡å‹
                await self._activate_high_priority_models()
                
                # æ›´æ–°æ¨¡å‹çŠ¶æ€
                self._update_model_status()
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è°ƒåº¦å¤±è´¥: {e}")
    
    async def _pause_low_priority_models(self, active_models: List[ModelInstance]):
        """æš‚åœä½ä¼˜å…ˆçº§æ¨¡å‹"""
        try:
            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œä½ä¼˜å…ˆçº§åœ¨å‰
            sorted_models = sorted(active_models, 
                                 key=lambda m: (m.config.priority.value, m.last_active))
            
            paused_count = 0
            target_pause = len(active_models) - self.max_concurrent_models
            
            for model in sorted_models:
                if paused_count >= target_pause:
                    break
                
                if model.config.priority in [GPUPriority.LOW, GPUPriority.NORMAL]:
                    model.status = ModelStatus.PAUSED
                    paused_count += 1
                    logger.info(f"æš‚åœä½ä¼˜å…ˆçº§æ¨¡å‹: {model.config.model_id}")
            
        except Exception as e:
            logger.error(f"æš‚åœä½ä¼˜å…ˆçº§æ¨¡å‹å¤±è´¥: {e}")
    
    async def _activate_high_priority_models(self):
        """æ¿€æ´»é«˜ä¼˜å…ˆçº§æ¨¡å‹"""
        try:
            # è·å–ç©ºé—²å’Œæš‚åœçš„é«˜ä¼˜å…ˆçº§æ¨¡å‹
            inactive_models = [m for m in self.models.values() 
                             if m.status in [ModelStatus.IDLE, ModelStatus.PAUSED] 
                             and m.config.priority in [GPUPriority.CRITICAL, GPUPriority.HIGH]]
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œé«˜ä¼˜å…ˆçº§åœ¨å‰
            sorted_models = sorted(inactive_models, 
                                 key=lambda m: (m.config.priority.value, -m.last_active), 
                                 reverse=True)
            
            active_count = len([m for m in self.models.values() 
                              if m.status in [ModelStatus.TRAINING, ModelStatus.INFERENCE]])
            
            for model in sorted_models:
                if active_count >= self.max_concurrent_models:
                    break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿèµ„æº
                if self._has_sufficient_resources(model):
                    if model.config.training_enabled:
                        model.status = ModelStatus.TRAINING
                    else:
                        model.status = ModelStatus.INFERENCE
                    
                    model.last_active = time.time()
                    active_count += 1
                    logger.info(f"æ¿€æ´»é«˜ä¼˜å…ˆçº§æ¨¡å‹: {model.config.model_id}")
            
        except Exception as e:
            logger.error(f"æ¿€æ´»é«˜ä¼˜å…ˆçº§æ¨¡å‹å¤±è´¥: {e}")
    
    def _has_sufficient_resources(self, model: ModelInstance) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿèµ„æº"""
        try:
            gpu_status = self.memory_manager.get_gpu_status()
            
            # æ£€æŸ¥å†…å­˜
            required_memory = model.config.memory_requirement * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
            if gpu_status.free_memory < required_memory:
                return False
            
            # æ£€æŸ¥æ¸©åº¦
            if gpu_status.temperature > self.temperature_threshold * 0.9:  # 90%é˜ˆå€¼
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥èµ„æºå¤±è´¥: {e}")
            return False
    
    def _update_model_status(self):
        """æ›´æ–°æ¨¡å‹çŠ¶æ€"""
        try:
            current_time = time.time()
            
            for model in self.models.values():
                # æ£€æŸ¥è®­ç»ƒè¶…æ—¶
                if (model.status == ModelStatus.TRAINING and 
                    current_time - model.last_active > model.config.max_training_time):
                    model.status = ModelStatus.IDLE
                    logger.info(f"æ¨¡å‹è®­ç»ƒè¶…æ—¶ï¼Œåˆ‡æ¢ä¸ºç©ºé—²: {model.config.model_id}")
                
                # æ›´æ–°GPUåˆ©ç”¨ç‡ï¼ˆæ¨¡æ‹Ÿï¼‰
                if model.status in [ModelStatus.TRAINING, ModelStatus.INFERENCE]:
                    model.gpu_utilization = min(100.0, model.gpu_utilization + 5.0)
                else:
                    model.gpu_utilization = max(0.0, model.gpu_utilization - 10.0)
            
        except Exception as e:
            logger.error(f"æ›´æ–°æ¨¡å‹çŠ¶æ€å¤±è´¥: {e}")
    
    async def _handle_overheating(self):
        """å¤„ç†è¿‡çƒ­"""
        try:
            logger.warning("GPUæ¸©åº¦è¿‡é«˜ï¼Œå¯åŠ¨é™æ¸©æªæ–½")
            
            # æš‚åœæ‰€æœ‰ä½ä¼˜å…ˆçº§æ¨¡å‹
            with self.scheduler_lock:
                for model in self.models.values():
                    if (model.config.priority in [GPUPriority.LOW, GPUPriority.NORMAL] and
                        model.status in [ModelStatus.TRAINING, ModelStatus.INFERENCE]):
                        model.status = ModelStatus.PAUSED
                        logger.info(f"å› è¿‡çƒ­æš‚åœæ¨¡å‹: {model.config.model_id}")
            
            # ç­‰å¾…é™æ¸©
            await asyncio.sleep(10)
            
        except Exception as e:
            logger.error(f"å¤„ç†è¿‡çƒ­å¤±è´¥: {e}")
    
    async def _handle_memory_pressure(self):
        """å¤„ç†å†…å­˜å‹åŠ›"""
        try:
            logger.warning("GPUå†…å­˜å‹åŠ›è¿‡å¤§ï¼Œå¯åŠ¨å†…å­˜æ¸…ç†")
            
            # ä¼˜åŒ–å†…å­˜å¸ƒå±€
            self.memory_manager.optimize_memory_layout()
            
            # æš‚åœéƒ¨åˆ†æ¨¡å‹
            with self.scheduler_lock:
                paused_count = 0
                for model in sorted(self.models.values(), 
                                  key=lambda m: (m.config.priority.value, m.last_active)):
                    if paused_count >= 2:  # æœ€å¤šæš‚åœ2ä¸ªæ¨¡å‹
                        break
                    
                    if (model.config.priority in [GPUPriority.LOW, GPUPriority.NORMAL] and
                        model.status in [ModelStatus.TRAINING, ModelStatus.INFERENCE]):
                        model.status = ModelStatus.PAUSED
                        paused_count += 1
                        logger.info(f"å› å†…å­˜å‹åŠ›æš‚åœæ¨¡å‹: {model.config.model_id}")
            
        except Exception as e:
            logger.error(f"å¤„ç†å†…å­˜å‹åŠ›å¤±è´¥: {e}")
    
    async def _save_checkpoints(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            current_time = time.time()
            
            for model in self.models.values():
                if (model.status == ModelStatus.TRAINING and
                    current_time - model.last_checkpoint > model.config.checkpoint_interval):
                    
                    await self._save_model_checkpoint(model)
                    model.last_checkpoint = current_time
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    async def _save_model_checkpoint(self, model: ModelInstance):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        try:
            if not TORCH_AVAILABLE or model.model is None:
                return
            
            checkpoint_path = f"checkpoints/{model.config.model_id}_checkpoint.pth"
            
            # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
            import os
            os.makedirs("checkpoints", exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹çŠ¶æ€
            checkpoint = {
                'model_state_dict': model.model.state_dict(),
                'training_steps': model.training_steps,
                'timestamp': time.time()
            }
            
            if model.optimizer:
                checkpoint['optimizer_state_dict'] = model.optimizer.state_dict()
            
            torch.save(checkpoint, checkpoint_path)
            logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: {model.config.model_id}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    async def _cleanup_models(self):
        """æ¸…ç†æ‰€æœ‰æ¨¡å‹"""
        try:
            with self.scheduler_lock:
                for model_id, model in self.models.items():
                    # é‡Šæ”¾å†…å­˜å—
                    for block_id in model.memory_blocks:
                        self.memory_manager.deallocate_memory(block_id)
                    
                    # æ¸…ç†æ¨¡å‹å¯¹è±¡
                    if model.model:
                        del model.model
                    if model.optimizer:
                        del model.optimizer
                    
                    logger.info(f"æ¸…ç†æ¨¡å‹: {model_id}")
                
                self.models.clear()
            
            logger.info("æ‰€æœ‰æ¨¡å‹æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ¨¡å‹å¤±è´¥: {e}")
    
    def get_scheduler_stats(self) -> Dict[str, Any]:
        """è·å–è°ƒåº¦å™¨ç»Ÿè®¡"""
        try:
            stats = {
                'total_models': len(self.models),
                'running': self.running,
                'model_status': {},
                'resource_usage': {},
                'performance_summary': {}
            }
            
            # æ¨¡å‹çŠ¶æ€ç»Ÿè®¡
            status_counts = {}
            total_memory_usage = 0
            total_gpu_utilization = 0.0
            
            for model in self.models.values():
                status = model.status.value
                status_counts[status] = status_counts.get(status, 0) + 1
                total_memory_usage += model.memory_usage
                total_gpu_utilization += model.gpu_utilization
                
                stats['model_status'][model.config.model_id] = {
                    'status': status,
                    'priority': model.config.priority.value,
                    'memory_usage_mb': model.memory_usage,
                    'gpu_utilization': model.gpu_utilization,
                    'training_steps': model.training_steps,
                    'error_count': model.error_count,
                    'uptime': time.time() - model.created_at
                }
            
            stats['resource_usage'] = {
                'total_memory_usage_mb': total_memory_usage,
                'avg_gpu_utilization': total_gpu_utilization / len(self.models) if self.models else 0,
                'status_distribution': status_counts
            }
            
            # GPUçŠ¶æ€
            gpu_status = self.memory_manager.get_gpu_status()
            stats['performance_summary'] = {
                'gpu_temperature': gpu_status.temperature,
                'gpu_utilization': gpu_status.gpu_utilization,
                'memory_utilization': gpu_status.memory_utilization,
                'active_models': len([m for m in self.models.values() 
                                    if m.status in [ModelStatus.TRAINING, ModelStatus.INFERENCE]])
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–è°ƒåº¦å™¨ç»Ÿè®¡å¤±è´¥: {e}")
            return {}


# å…¨å±€GPUæ¨¡å‹è°ƒåº¦å™¨å®ä¾‹
gpu_model_scheduler = None

def initialize_gpu_scheduler(memory_manager: GPUMemoryManager):
    """åˆå§‹åŒ–GPUæ¨¡å‹è°ƒåº¦å™¨"""
    global gpu_model_scheduler
    gpu_model_scheduler = GPUModelScheduler(memory_manager)
    return gpu_model_scheduler
