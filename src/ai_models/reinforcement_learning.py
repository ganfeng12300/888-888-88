"""
ğŸ§  å¼ºåŒ–å­¦ä¹ äº¤æ˜“æ¨¡å‹
ç”Ÿäº§çº§PPO/SACå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ”¯æŒè¿ç»­åŠ¨ä½œç©ºé—´å’Œå¤šèµ„äº§äº¤æ˜“
å®ç°æ™ºèƒ½äº¤æ˜“å†³ç­–å’Œé£é™©ç®¡ç†
"""

import asyncio
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Normal
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import gymnasium as gym
from gymnasium import spaces
from collections import deque
import pickle
import json
from pathlib import Path

from loguru import logger
from src.hardware.gpu_manager import GPUTaskType, allocate_gpu_memory, deallocate_gpu_memory
from src.hardware.cpu_manager import CPUTaskType, assign_cpu_cores


class ActionType(Enum):
    """åŠ¨ä½œç±»å‹"""
    HOLD = 0
    BUY = 1
    SELL = 2


class ModelType(Enum):
    """æ¨¡å‹ç±»å‹"""
    PPO = "ppo"
    SAC = "sac"
    A3C = "a3c"
    DDPG = "ddpg"


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    model_type: ModelType = ModelType.PPO
    learning_rate: float = 3e-4
    batch_size: int = 256
    buffer_size: int = 100000
    gamma: float = 0.99
    tau: float = 0.005
    clip_epsilon: float = 0.2
    entropy_coef: float = 0.01
    value_coef: float = 0.5
    max_grad_norm: float = 0.5
    update_frequency: int = 2048
    epochs_per_update: int = 10
    target_kl: float = 0.01
    device: str = "cuda" if torch.cuda.is_available() else "cpu"


@dataclass
class ModelMetrics:
    """æ¨¡å‹æŒ‡æ ‡"""
    episode_reward: float = 0.0
    episode_length: int = 0
    total_trades: int = 0
    profitable_trades: int = 0
    win_rate: float = 0.0
    sharpe_ratio: float = 0.0
    max_drawdown: float = 0.0
    total_return: float = 0.0
    volatility: float = 0.0
    training_loss: float = 0.0
    policy_loss: float = 0.0
    value_loss: float = 0.0


class TradingEnvironment(gym.Env):
    """äº¤æ˜“ç¯å¢ƒ"""
    
    def __init__(self, data: np.ndarray, initial_balance: float = 10000.0,
                 transaction_cost: float = 0.001, max_position: float = 1.0):
        super().__init__()
        
        self.data = data
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost
        self.max_position = max_position
        
        # çŠ¶æ€ç©ºé—´ï¼šä»·æ ¼ç‰¹å¾ + æŠ€æœ¯æŒ‡æ ‡ + æŒä»“ä¿¡æ¯
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(data.shape[1] + 3,), dtype=np.float32
        )
        
        # åŠ¨ä½œç©ºé—´ï¼šè¿ç»­åŠ¨ä½œ [-1, 1] (å–å‡ºåˆ°ä¹°å…¥)
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(1,), dtype=np.float32
        )
        
        self.reset()
    
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        super().reset(seed=seed)
        
        self.current_step = 0
        self.balance = self.initial_balance
        self.position = 0.0  # æŒä»“æ¯”ä¾‹ [-1, 1]
        self.portfolio_value = self.initial_balance
        self.trade_history = []
        self.returns = []
        
        return self._get_observation(), {}
    
    def step(self, action: np.ndarray):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        if self.current_step >= len(self.data) - 1:
            return self._get_observation(), 0, True, True, {}
        
        # è§£æåŠ¨ä½œ
        target_position = np.clip(action[0], -self.max_position, self.max_position)
        
        # è®¡ç®—å½“å‰ä»·æ ¼å’Œæ”¶ç›Š
        current_price = self.data[self.current_step, 0]  # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯ä»·æ ¼
        next_price = self.data[self.current_step + 1, 0]
        
        # è®¡ç®—æŒä»“æ”¶ç›Š
        position_return = (next_price - current_price) / current_price * self.position
        
        # è®¡ç®—äº¤æ˜“æˆæœ¬
        position_change = abs(target_position - self.position)
        transaction_cost = position_change * self.transaction_cost
        
        # æ›´æ–°æŠ•èµ„ç»„åˆä»·å€¼
        self.portfolio_value *= (1 + position_return - transaction_cost)
        
        # è®°å½•äº¤æ˜“
        if abs(target_position - self.position) > 0.01:
            self.trade_history.append({
                'step': self.current_step,
                'action': target_position,
                'price': current_price,
                'position_change': target_position - self.position
            })
        
        # æ›´æ–°æŒä»“
        self.position = target_position
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(position_return, transaction_cost)
        
        # è®°å½•æ”¶ç›Š
        portfolio_return = (self.portfolio_value - self.initial_balance) / self.initial_balance
        self.returns.append(portfolio_return)
        
        self.current_step += 1
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = self.current_step >= len(self.data) - 1
        truncated = self.portfolio_value <= self.initial_balance * 0.5  # 50%æ­¢æŸ
        
        info = {
            'portfolio_value': self.portfolio_value,
            'position': self.position,
            'total_return': portfolio_return,
            'num_trades': len(self.trade_history)
        }
        
        return self._get_observation(), reward, done, truncated, info
    
    def _get_observation(self):
        """è·å–è§‚å¯ŸçŠ¶æ€"""
        if self.current_step >= len(self.data):
            self.current_step = len(self.data) - 1
        
        # å¸‚åœºç‰¹å¾
        market_features = self.data[self.current_step]
        
        # æŒä»“ä¿¡æ¯
        portfolio_features = np.array([
            self.position,  # å½“å‰æŒä»“
            (self.portfolio_value - self.initial_balance) / self.initial_balance,  # æ€»æ”¶ç›Šç‡
            len(self.trade_history) / 100.0  # äº¤æ˜“æ¬¡æ•°ï¼ˆå½’ä¸€åŒ–ï¼‰
        ])
        
        return np.concatenate([market_features, portfolio_features]).astype(np.float32)
    
    def _calculate_reward(self, position_return: float, transaction_cost: float):
        """è®¡ç®—å¥–åŠ±å‡½æ•°"""
        # åŸºç¡€æ”¶ç›Šå¥–åŠ±
        reward = position_return * 100  # æ”¾å¤§æ”¶ç›Šä¿¡å·
        
        # äº¤æ˜“æˆæœ¬æƒ©ç½š
        reward -= transaction_cost * 50
        
        # é£é™©è°ƒæ•´
        if len(self.returns) > 20:
            volatility = np.std(self.returns[-20:])
            if volatility > 0:
                reward -= volatility * 10  # æ³¢åŠ¨ç‡æƒ©ç½š
        
        # æŒä»“è¿‡åº¦æƒ©ç½š
        if abs(self.position) > 0.8:
            reward -= abs(self.position) * 5
        
        return reward


class ActorNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œï¼ˆActorï¼‰"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super().__init__()
        
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        
        # ç­–ç•¥è¾“å‡º
        self.mean_head = nn.Linear(hidden_dim, action_dim)
        self.log_std_head = nn.Linear(hidden_dim, action_dim)
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.orthogonal_(m.weight, gain=np.sqrt(2))
            torch.nn.init.constant_(m.bias, 0)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        
        mean = torch.tanh(self.mean_head(x))  # é™åˆ¶åœ¨[-1, 1]
        log_std = torch.clamp(self.log_std_head(x), -20, 2)
        
        return mean, log_std
    
    def get_action_and_log_prob(self, state):
        mean, log_std = self.forward(state)
        std = torch.exp(log_std)
        
        normal = Normal(mean, std)
        action = normal.sample()
        log_prob = normal.log_prob(action).sum(dim=-1)
        
        return action, log_prob
    
    def get_log_prob(self, state, action):
        mean, log_std = self.forward(state)
        std = torch.exp(log_std)
        
        normal = Normal(mean, std)
        log_prob = normal.log_prob(action).sum(dim=-1)
        
        return log_prob


class CriticNetwork(nn.Module):
    """ä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰"""
    
    def __init__(self, state_dim: int, hidden_dim: int = 256):
        super().__init__()
        
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.value_head = nn.Linear(hidden_dim, 1)
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.orthogonal_(m.weight, gain=np.sqrt(2))
            torch.nn.init.constant_(m.bias, 0)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        value = self.value_head(x)
        
        return value


class PPOAgent:
    """PPOæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, config: TrainingConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        # ç½‘ç»œ
        self.actor = ActorNetwork(state_dim, action_dim).to(self.device)
        self.critic = CriticNetwork(state_dim).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.learning_rate)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.learning_rate)
        
        # ç»éªŒç¼“å†²åŒº
        self.buffer = {
            'states': [],
            'actions': [],
            'rewards': [],
            'values': [],
            'log_probs': [],
            'dones': []
        }
        
        # åˆ†é…GPUå†…å­˜
        self.gpu_memory = allocate_gpu_memory(GPUTaskType.REINFORCEMENT_LEARNING, "ppo_agent", 2048)
        
        # åˆ†é…CPUæ ¸å¿ƒ
        assign_cpu_cores(CPUTaskType.AI_TRAINING_LIGHT, [9, 10, 11, 12])
        
        logger.info("PPOæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def get_action(self, state: np.ndarray, training: bool = True):
        """è·å–åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            if training:
                action, log_prob = self.actor.get_action_and_log_prob(state_tensor)
                value = self.critic(state_tensor)
                
                return action.cpu().numpy()[0], log_prob.cpu().numpy()[0], value.cpu().numpy()[0]
            else:
                mean, _ = self.actor(state_tensor)
                return mean.cpu().numpy()[0], None, None
    
    def store_transition(self, state, action, reward, value, log_prob, done):
        """å­˜å‚¨è½¬æ¢"""
        self.buffer['states'].append(state)
        self.buffer['actions'].append(action)
        self.buffer['rewards'].append(reward)
        self.buffer['values'].append(value)
        self.buffer['log_probs'].append(log_prob)
        self.buffer['dones'].append(done)
    
    def update(self):
        """æ›´æ–°ç½‘ç»œ"""
        if len(self.buffer['states']) < self.config.batch_size:
            return {}
        
        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.FloatTensor(np.array(self.buffer['states'])).to(self.device)
        actions = torch.FloatTensor(np.array(self.buffer['actions'])).to(self.device)
        rewards = torch.FloatTensor(self.buffer['rewards']).to(self.device)
        old_values = torch.FloatTensor(self.buffer['values']).to(self.device)
        old_log_probs = torch.FloatTensor(self.buffer['log_probs']).to(self.device)
        dones = torch.BoolTensor(self.buffer['dones']).to(self.device)
        
        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        advantages, returns = self._compute_gae(rewards, old_values, dones)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # å¤šè½®æ›´æ–°
        total_policy_loss = 0
        total_value_loss = 0
        
        for _ in range(self.config.epochs_per_update):
            # éšæœºé‡‡æ ·æ‰¹æ¬¡
            indices = torch.randperm(len(states))[:self.config.batch_size]
            
            batch_states = states[indices]
            batch_actions = actions[indices]
            batch_advantages = advantages[indices]
            batch_returns = returns[indices]
            batch_old_log_probs = old_log_probs[indices]
            
            # è®¡ç®—æ–°çš„ç­–ç•¥æ¦‚ç‡
            new_log_probs = self.actor.get_log_prob(batch_states, batch_actions)
            ratio = torch.exp(new_log_probs - batch_old_log_probs)
            
            # PPOæŸå¤±
            surr1 = ratio * batch_advantages
            surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 
                               1 + self.config.clip_epsilon) * batch_advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # ä»·å€¼æŸå¤±
            new_values = self.critic(batch_states).squeeze()
            value_loss = F.mse_loss(new_values, batch_returns)
            
            # ç†µæŸå¤±
            _, log_std = self.actor(batch_states)
            entropy_loss = -torch.mean(log_std)
            
            # æ€»æŸå¤±
            actor_loss = policy_loss + self.config.entropy_coef * entropy_loss
            
            # æ›´æ–°ç½‘ç»œ
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.config.max_grad_norm)
            self.actor_optimizer.step()
            
            self.critic_optimizer.zero_grad()
            value_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.config.max_grad_norm)
            self.critic_optimizer.step()
            
            total_policy_loss += policy_loss.item()
            total_value_loss += value_loss.item()
            
            # KLæ•£åº¦æ£€æŸ¥
            with torch.no_grad():
                kl_div = torch.mean(batch_old_log_probs - new_log_probs)
                if kl_div > self.config.target_kl:
                    break
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.clear_buffer()
        
        return {
            'policy_loss': total_policy_loss / self.config.epochs_per_update,
            'value_loss': total_value_loss / self.config.epochs_per_update,
            'kl_divergence': kl_div.item()
        }
    
    def _compute_gae(self, rewards, values, dones):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡"""
        advantages = torch.zeros_like(rewards)
        gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + self.config.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.config.gamma * 0.95 * (1 - dones[t]) * gae
            advantages[t] = gae
        
        returns = advantages + values
        return advantages, returns
    
    def clear_buffer(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        for key in self.buffer:
            self.buffer[key].clear()
    
    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'config': self.config
        }, path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
    
    def load_model(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        logger.info(f"æ¨¡å‹å·²ä» {path} åŠ è½½")


class RLTrainer:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, env: TradingEnvironment, agent: PPOAgent, config: TrainingConfig):
        self.env = env
        self.agent = agent
        self.config = config
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = deque(maxlen=100)
        self.episode_lengths = deque(maxlen=100)
        self.training_metrics = []
        
        self.is_training = False
        
    async def train(self, num_episodes: int = 1000, save_interval: int = 100):
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        self.is_training = True
        logger.info(f"å¼€å§‹è®­ç»ƒï¼Œå…± {num_episodes} è½®")
        
        for episode in range(num_episodes):
            if not self.is_training:
                break
            
            # è¿è¡Œä¸€è½®
            metrics = await self._run_episode(training=True)
            
            # è®°å½•ç»Ÿè®¡
            self.episode_rewards.append(metrics.episode_reward)
            self.episode_lengths.append(metrics.episode_length)
            self.training_metrics.append(metrics)
            
            # æ›´æ–°æ™ºèƒ½ä½“
            if len(self.agent.buffer['states']) >= self.config.update_frequency:
                update_info = self.agent.update()
                metrics.policy_loss = update_info.get('policy_loss', 0)
                metrics.value_loss = update_info.get('value_loss', 0)
            
            # æ—¥å¿—è¾“å‡º
            if episode % 10 == 0:
                avg_reward = np.mean(list(self.episode_rewards))
                avg_length = np.mean(list(self.episode_lengths))
                
                logger.info(
                    f"Episode {episode}: "
                    f"Reward={metrics.episode_reward:.2f}, "
                    f"AvgReward={avg_reward:.2f}, "
                    f"Length={metrics.episode_length}, "
                    f"WinRate={metrics.win_rate:.2f}, "
                    f"Return={metrics.total_return:.4f}"
                )
            
            # ä¿å­˜æ¨¡å‹
            if episode % save_interval == 0 and episode > 0:
                model_path = f"models/ppo_episode_{episode}.pth"
                Path("models").mkdir(exist_ok=True)
                self.agent.save_model(model_path)
        
        logger.info("è®­ç»ƒå®Œæˆ")
    
    async def _run_episode(self, training: bool = True) -> ModelMetrics:
        """è¿è¡Œä¸€è½®"""
        state, _ = self.env.reset()
        episode_reward = 0
        episode_length = 0
        trades = []
        
        while True:
            # è·å–åŠ¨ä½œ
            if training:
                action, log_prob, value = self.agent.get_action(state, training=True)
            else:
                action, _, _ = self.agent.get_action(state, training=False)
                log_prob, value = None, None
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, truncated, info = self.env.step(action)
            
            # å­˜å‚¨è½¬æ¢
            if training and log_prob is not None:
                self.agent.store_transition(state, action, reward, value, log_prob, done or truncated)
            
            # è®°å½•äº¤æ˜“
            if abs(action[0] - self.env.position) > 0.01:
                trades.append({
                    'action': action[0],
                    'reward': reward,
                    'portfolio_value': info['portfolio_value']
                })
            
            episode_reward += reward
            episode_length += 1
            state = next_state
            
            if done or truncated:
                break
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = ModelMetrics()
        metrics.episode_reward = episode_reward
        metrics.episode_length = episode_length
        metrics.total_trades = len(trades)
        
        if trades:
            profitable_trades = sum(1 for t in trades if t['reward'] > 0)
            metrics.profitable_trades = profitable_trades
            metrics.win_rate = profitable_trades / len(trades)
        
        metrics.total_return = info.get('total_return', 0)
        
        # è®¡ç®—å¤æ™®æ¯”ç‡å’Œæœ€å¤§å›æ’¤
        if len(self.env.returns) > 1:
            returns_array = np.array(self.env.returns)
            metrics.volatility = np.std(returns_array)
            if metrics.volatility > 0:
                metrics.sharpe_ratio = np.mean(returns_array) / metrics.volatility * np.sqrt(252)
            
            # è®¡ç®—æœ€å¤§å›æ’¤
            cumulative = np.cumprod(1 + returns_array)
            running_max = np.maximum.accumulate(cumulative)
            drawdown = (cumulative - running_max) / running_max
            metrics.max_drawdown = np.min(drawdown)
        
        return metrics
    
    def stop_training(self):
        """åœæ­¢è®­ç»ƒ"""
        self.is_training = False
        logger.info("è®­ç»ƒå·²åœæ­¢")
    
    def get_training_stats(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡"""
        if not self.training_metrics:
            return {}
        
        recent_metrics = self.training_metrics[-10:]  # æœ€è¿‘10è½®
        
        return {
            'total_episodes': len(self.training_metrics),
            'avg_reward': np.mean([m.episode_reward for m in recent_metrics]),
            'avg_length': np.mean([m.episode_length for m in recent_metrics]),
            'avg_win_rate': np.mean([m.win_rate for m in recent_metrics]),
            'avg_return': np.mean([m.total_return for m in recent_metrics]),
            'avg_sharpe_ratio': np.mean([m.sharpe_ratio for m in recent_metrics]),
            'avg_max_drawdown': np.mean([m.max_drawdown for m in recent_metrics]),
            'best_episode_reward': max([m.episode_reward for m in self.training_metrics]),
            'best_return': max([m.total_return for m in self.training_metrics])
        }


# å…¨å±€è®­ç»ƒå™¨å®ä¾‹
rl_trainer = None


def create_trainer(data: np.ndarray, config: TrainingConfig) -> RLTrainer:
    """åˆ›å»ºè®­ç»ƒå™¨"""
    global rl_trainer
    
    # åˆ›å»ºç¯å¢ƒ
    env = TradingEnvironment(data)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    agent = PPOAgent(state_dim, action_dim, config)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    rl_trainer = RLTrainer(env, agent, config)
    
    return rl_trainer


async def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨å¼ºåŒ–å­¦ä¹ æ¨¡å‹æµ‹è¯•...")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n_steps = 1000
    n_features = 10
    
    # æ¨¡æ‹Ÿä»·æ ¼æ•°æ®
    prices = np.cumsum(np.random.randn(n_steps) * 0.01) + 100
    features = np.random.randn(n_steps, n_features - 1)
    data = np.column_stack([prices, features])
    
    # åˆ›å»ºé…ç½®
    config = TrainingConfig(
        model_type=ModelType.PPO,
        learning_rate=3e-4,
        batch_size=64,
        update_frequency=512
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = create_trainer(data, config)
    
    try:
        # å¼€å§‹è®­ç»ƒ
        await trainer.train(num_episodes=100, save_interval=50)
        
        # è·å–è®­ç»ƒç»Ÿè®¡
        stats = trainer.get_training_stats()
        logger.info(f"è®­ç»ƒç»Ÿè®¡: {stats}")
        
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·...")
        trainer.stop_training()
    finally:
        # æ¸…ç†GPUå†…å­˜
        if trainer.agent.gpu_memory:
            deallocate_gpu_memory("ppo_agent")


if __name__ == "__main__":
    asyncio.run(main())
