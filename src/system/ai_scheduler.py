#!/usr/bin/env python3
"""
ğŸ¦Š çŒç‹AIé‡åŒ–äº¤æ˜“ç³»ç»Ÿ - AIæ¨¡å‹è°ƒåº¦ä¸­å¿ƒ
8å¤§AIæ™ºèƒ½ä½“ç»Ÿä¸€åè°ƒè°ƒåº¦ï¼Œæ™ºèƒ½å†³ç­–èåˆ
ä¸“ä¸ºå²è¯—çº§AIé‡åŒ–äº¤æ˜“è®¾è®¡ï¼Œç”Ÿäº§çº§å®ç›˜äº¤æ˜“æ ‡å‡†
"""

import asyncio
import time
import threading
from typing import Dict, Any, List, Optional, Tuple, Callable
from datetime import datetime, timezone, timedelta
from dataclasses import dataclass, field
from enum import Enum
from loguru import logger
from concurrent.futures import ThreadPoolExecutor
import json
import numpy as np
import psutil
import GPUtil

class AIModelStatus(Enum):
    """AIæ¨¡å‹çŠ¶æ€"""
    INITIALIZING = "initializing"
    READY = "ready"
    TRAINING = "training"
    PREDICTING = "predicting"
    ERROR = "error"
    OFFLINE = "offline"

class AIModelPriority(Enum):
    """AIæ¨¡å‹ä¼˜å…ˆçº§"""
    LOW = 1
    NORMAL = 2
    HIGH = 3
    CRITICAL = 4

@dataclass
class AIModelInfo:
    """AIæ¨¡å‹ä¿¡æ¯"""
    model_id: str
    model_name: str
    model_type: str
    status: AIModelStatus
    priority: AIModelPriority
    cpu_cores: int
    gpu_memory: float  # GB
    last_prediction: Optional[Dict[str, Any]] = None
    last_update: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    performance_score: float = 0.0
    accuracy: float = 0.0
    latency_ms: float = 0.0
    error_count: int = 0
    prediction_count: int = 0
    is_active: bool = True

@dataclass
class SchedulingTask:
    """è°ƒåº¦ä»»åŠ¡"""
    task_id: str
    model_id: str
    task_type: str  # 'predict', 'train', 'update'
    priority: int
    data: Dict[str, Any]
    created_at: datetime
    scheduled_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    result: Optional[Dict[str, Any]] = None
    error_message: str = ""

class ResourceManager:
    """èµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.cpu_cores = psutil.cpu_count()
        self.total_memory = psutil.virtual_memory().total / (1024**3)  # GB
        self.gpu_info = self._get_gpu_info()
        
        # èµ„æºåˆ†é…
        self.allocated_cpu = {}  # model_id -> cores
        self.allocated_gpu = {}  # model_id -> memory_gb
        self.cpu_usage = {}     # core_id -> usage
        self.gpu_usage = {}     # gpu_id -> usage
        
        logger.info(f"ğŸ–¥ï¸ èµ„æºç®¡ç†å™¨åˆå§‹åŒ–: CPU {self.cpu_cores}æ ¸, å†…å­˜ {self.total_memory:.1f}GB")
        
    def _get_gpu_info(self) -> List[Dict[str, Any]]:
        """è·å–GPUä¿¡æ¯"""
        try:
            gpus = GPUtil.getGPUs()
            gpu_info = []
            for gpu in gpus:
                gpu_info.append({
                    'id': gpu.id,
                    'name': gpu.name,
                    'memory_total': gpu.memoryTotal,
                    'memory_free': gpu.memoryFree,
                    'memory_used': gpu.memoryUsed,
                    'temperature': gpu.temperature,
                    'load': gpu.load
                })
            return gpu_info
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def allocate_resources(self, model_id: str, cpu_cores: int, gpu_memory: float) -> bool:
        """åˆ†é…èµ„æº"""
        try:
            # æ£€æŸ¥CPUèµ„æº
            allocated_cores = sum(self.allocated_cpu.values())
            if allocated_cores + cpu_cores > self.cpu_cores:
                logger.warning(f"âš ï¸ CPUèµ„æºä¸è¶³: éœ€è¦{cpu_cores}æ ¸ï¼Œå¯ç”¨{self.cpu_cores - allocated_cores}æ ¸")
                return False
            
            # æ£€æŸ¥GPUèµ„æº
            if gpu_memory > 0 and self.gpu_info:
                total_allocated_gpu = sum(self.allocated_gpu.values())
                available_gpu = self.gpu_info[0]['memory_total'] / 1024  # è½¬æ¢ä¸ºGB
                
                if total_allocated_gpu + gpu_memory > available_gpu:
                    logger.warning(f"âš ï¸ GPUå†…å­˜ä¸è¶³: éœ€è¦{gpu_memory}GBï¼Œå¯ç”¨{available_gpu - total_allocated_gpu}GB")
                    return False
            
            # åˆ†é…èµ„æº
            self.allocated_cpu[model_id] = cpu_cores
            if gpu_memory > 0:
                self.allocated_gpu[model_id] = gpu_memory
            
            logger.info(f"âœ… èµ„æºåˆ†é…æˆåŠŸ: {model_id} CPU {cpu_cores}æ ¸, GPU {gpu_memory}GB")
            return True
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºåˆ†é…å¤±è´¥: {e}")
            return False
    
    def release_resources(self, model_id: str):
        """é‡Šæ”¾èµ„æº"""
        try:
            if model_id in self.allocated_cpu:
                del self.allocated_cpu[model_id]
            if model_id in self.allocated_gpu:
                del self.allocated_gpu[model_id]
            
            logger.info(f"ğŸ”„ èµ„æºå·²é‡Šæ”¾: {model_id}")
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºé‡Šæ”¾å¤±è´¥: {e}")
    
    def get_resource_usage(self) -> Dict[str, Any]:
        """è·å–èµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            
            # æ›´æ–°GPUä¿¡æ¯
            gpu_info = self._get_gpu_info()
            
            return {
                'cpu': {
                    'cores': self.cpu_cores,
                    'allocated': sum(self.allocated_cpu.values()),
                    'usage_percent': cpu_percent,
                    'available': self.cpu_cores - sum(self.allocated_cpu.values())
                },
                'memory': {
                    'total_gb': self.total_memory,
                    'used_gb': memory.used / (1024**3),
                    'available_gb': memory.available / (1024**3),
                    'usage_percent': memory.percent
                },
                'gpu': gpu_info
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–èµ„æºä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
            return {}

class AIScheduler:
    """ğŸ¦Š çŒç‹AI - AIæ¨¡å‹è°ƒåº¦ä¸­å¿ƒ"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.resource_manager = ResourceManager()
        
        # AIæ¨¡å‹æ³¨å†Œè¡¨
        self.ai_models = {}  # model_id -> AIModelInfo
        self.model_instances = {}  # model_id -> model_instance
        
        # ä»»åŠ¡é˜Ÿåˆ—
        self.task_queue = asyncio.Queue()
        self.completed_tasks = {}  # task_id -> SchedulingTask
        self.running_tasks = {}   # task_id -> SchedulingTask
        
        # è°ƒåº¦å™¨çŠ¶æ€
        self.is_running = False
        self.scheduler_tasks = []
        self.executor = ThreadPoolExecutor(max_workers=8)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'average_latency': 0.0,
            'active_models': 0,
            'total_predictions': 0
        }
        
        logger.info("ğŸ¦Š çŒç‹AIæ¨¡å‹è°ƒåº¦ä¸­å¿ƒåˆå§‹åŒ–å®Œæˆ")
    
    async def register_ai_model(self, model_id: str, model_name: str, model_type: str,
                               model_instance: Any, cpu_cores: int = 2, 
                               gpu_memory: float = 1.0, priority: AIModelPriority = AIModelPriority.NORMAL) -> bool:
        """æ³¨å†ŒAIæ¨¡å‹"""
        try:
            # åˆ†é…èµ„æº
            if not self.resource_manager.allocate_resources(model_id, cpu_cores, gpu_memory):
                return False
            
            # åˆ›å»ºæ¨¡å‹ä¿¡æ¯
            model_info = AIModelInfo(
                model_id=model_id,
                model_name=model_name,
                model_type=model_type,
                status=AIModelStatus.INITIALIZING,
                priority=priority,
                cpu_cores=cpu_cores,
                gpu_memory=gpu_memory
            )
            
            # æ³¨å†Œæ¨¡å‹
            self.ai_models[model_id] = model_info
            self.model_instances[model_id] = model_instance
            
            # åˆå§‹åŒ–æ¨¡å‹
            await self._initialize_model(model_id)
            
            self.stats['active_models'] = len([m for m in self.ai_models.values() if m.is_active])
            
            logger.success(f"âœ… AIæ¨¡å‹æ³¨å†ŒæˆåŠŸ: {model_name} ({model_id})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ AIæ¨¡å‹æ³¨å†Œå¤±è´¥ {model_id}: {e}")
            return False
    
    async def _initialize_model(self, model_id: str):
        """åˆå§‹åŒ–AIæ¨¡å‹"""
        try:
            model_info = self.ai_models[model_id]
            model_instance = self.model_instances[model_id]
            
            model_info.status = AIModelStatus.INITIALIZING
            
            # å¦‚æœæ¨¡å‹æœ‰åˆå§‹åŒ–æ–¹æ³•ï¼Œè°ƒç”¨å®ƒ
            if hasattr(model_instance, 'initialize'):
                await model_instance.initialize()
            
            model_info.status = AIModelStatus.READY
            model_info.last_update = datetime.now(timezone.utc)
            
            logger.info(f"âœ… AIæ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {model_id}")
            
        except Exception as e:
            logger.error(f"âŒ AIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥ {model_id}: {e}")
            self.ai_models[model_id].status = AIModelStatus.ERROR
            self.ai_models[model_id].error_count += 1
    
    async def schedule_prediction(self, model_id: str, data: Dict[str, Any], 
                                priority: int = 2) -> Optional[str]:
        """è°ƒåº¦é¢„æµ‹ä»»åŠ¡"""
        try:
            if model_id not in self.ai_models:
                logger.error(f"âŒ AIæ¨¡å‹ä¸å­˜åœ¨: {model_id}")
                return None
            
            # åˆ›å»ºä»»åŠ¡
            task = SchedulingTask(
                task_id=f"pred_{int(time.time() * 1000)}_{model_id}",
                model_id=model_id,
                task_type='predict',
                priority=priority,
                data=data,
                created_at=datetime.now(timezone.utc)
            )
            
            # æ·»åŠ åˆ°é˜Ÿåˆ—
            await self.task_queue.put(task)
            self.stats['total_tasks'] += 1
            
            logger.debug(f"ğŸ“‹ é¢„æµ‹ä»»åŠ¡å·²è°ƒåº¦: {task.task_id}")
            return task.task_id
            
        except Exception as e:
            logger.error(f"âŒ è°ƒåº¦é¢„æµ‹ä»»åŠ¡å¤±è´¥: {e}")
            return None
    
    async def schedule_training(self, model_id: str, data: Dict[str, Any],
                              priority: int = 1) -> Optional[str]:
        """è°ƒåº¦è®­ç»ƒä»»åŠ¡"""
        try:
            if model_id not in self.ai_models:
                logger.error(f"âŒ AIæ¨¡å‹ä¸å­˜åœ¨: {model_id}")
                return None
            
            # åˆ›å»ºä»»åŠ¡
            task = SchedulingTask(
                task_id=f"train_{int(time.time() * 1000)}_{model_id}",
                model_id=model_id,
                task_type='train',
                priority=priority,
                data=data,
                created_at=datetime.now(timezone.utc)
            )
            
            # æ·»åŠ åˆ°é˜Ÿåˆ—
            await self.task_queue.put(task)
            self.stats['total_tasks'] += 1
            
            logger.debug(f"ğŸ“‹ è®­ç»ƒä»»åŠ¡å·²è°ƒåº¦: {task.task_id}")
            return task.task_id
            
        except Exception as e:
            logger.error(f"âŒ è°ƒåº¦è®­ç»ƒä»»åŠ¡å¤±è´¥: {e}")
            return None
    
    async def get_ensemble_prediction(self, data: Dict[str, Any], 
                                    model_ids: List[str] = None) -> Dict[str, Any]:
        """è·å–é›†æˆé¢„æµ‹ç»“æœ"""
        try:
            if model_ids is None:
                # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
                model_ids = [mid for mid, info in self.ai_models.items() 
                           if info.status == AIModelStatus.READY and info.is_active]
            
            if not model_ids:
                logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„AIæ¨¡å‹è¿›è¡Œé›†æˆé¢„æµ‹")
                return {'prediction': 0.0, 'confidence': 0.0, 'models_used': []}
            
            # å¹¶è¡Œè°ƒåº¦é¢„æµ‹ä»»åŠ¡
            task_ids = []
            for model_id in model_ids:
                task_id = await self.schedule_prediction(model_id, data, priority=3)
                if task_id:
                    task_ids.append(task_id)
            
            if not task_ids:
                return {'prediction': 0.0, 'confidence': 0.0, 'models_used': []}
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            predictions = []
            confidences = []
            weights = []
            used_models = []
            
            timeout = 30  # 30ç§’è¶…æ—¶
            start_time = time.time()
            
            while len(predictions) < len(task_ids) and (time.time() - start_time) < timeout:
                for task_id in task_ids:
                    if task_id in self.completed_tasks:
                        task = self.completed_tasks[task_id]
                        if task.result and task_id not in [t['task_id'] for t in predictions]:
                            model_info = self.ai_models[task.model_id]
                            
                            predictions.append({
                                'task_id': task_id,
                                'model_id': task.model_id,
                                'prediction': task.result.get('prediction', 0.0),
                                'confidence': task.result.get('confidence', 0.0)
                            })
                            
                            confidences.append(task.result.get('confidence', 0.0))
                            weights.append(model_info.performance_score)
                            used_models.append(task.model_id)
                
                await asyncio.sleep(0.1)
            
            if not predictions:
                return {'prediction': 0.0, 'confidence': 0.0, 'models_used': []}
            
            # åŠ æƒå¹³å‡é›†æˆ
            pred_values = [p['prediction'] for p in predictions]
            conf_values = confidences
            
            # å¦‚æœæ²¡æœ‰æƒé‡ï¼Œä½¿ç”¨ç­‰æƒé‡
            if not any(weights):
                weights = [1.0] * len(predictions)
            
            # æ ‡å‡†åŒ–æƒé‡
            total_weight = sum(weights)
            if total_weight > 0:
                weights = [w / total_weight for w in weights]
            else:
                weights = [1.0 / len(predictions)] * len(predictions)
            
            # è®¡ç®—åŠ æƒé¢„æµ‹
            ensemble_prediction = sum(p * w for p, w in zip(pred_values, weights))
            ensemble_confidence = sum(c * w for c, w in zip(conf_values, weights))
            
            result = {
                'prediction': ensemble_prediction,
                'confidence': ensemble_confidence,
                'models_used': used_models,
                'individual_predictions': predictions,
                'weights': dict(zip(used_models, weights))
            }
            
            logger.info(f"ğŸ§  é›†æˆé¢„æµ‹å®Œæˆ: {len(used_models)}ä¸ªæ¨¡å‹, é¢„æµ‹å€¼: {ensemble_prediction:.4f}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆé¢„æµ‹å¤±è´¥: {e}")
            return {'prediction': 0.0, 'confidence': 0.0, 'models_used': []}
    
    async def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if self.is_running:
            return
        
        self.is_running = True
        
        # å¯åŠ¨è°ƒåº¦å™¨ä»»åŠ¡
        self.scheduler_tasks = [
            asyncio.create_task(self._task_scheduler()),
            asyncio.create_task(self._performance_monitor()),
            asyncio.create_task(self._health_checker())
        ]
        
        logger.success("ğŸš€ AIæ¨¡å‹è°ƒåº¦ä¸­å¿ƒå·²å¯åŠ¨")
    
    async def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.is_running = False
        
        # å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
        for task in self.scheduler_tasks:
            task.cancel()
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        await asyncio.gather(*self.scheduler_tasks, return_exceptions=True)
        
        # é‡Šæ”¾æ‰€æœ‰èµ„æº
        for model_id in list(self.ai_models.keys()):
            self.resource_manager.release_resources(model_id)
        
        self.executor.shutdown(wait=True)
        
        logger.info("ğŸ›‘ AIæ¨¡å‹è°ƒåº¦ä¸­å¿ƒå·²åœæ­¢")
    
    async def _task_scheduler(self):
        """ä»»åŠ¡è°ƒåº¦å™¨ä¸»å¾ªç¯"""
        while self.is_running:
            try:
                # è·å–ä»»åŠ¡ï¼ˆå¸¦è¶…æ—¶ï¼‰
                try:
                    task = await asyncio.wait_for(self.task_queue.get(), timeout=1.0)
                except asyncio.TimeoutError:
                    continue
                
                # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
                if task.model_id not in self.ai_models:
                    logger.error(f"âŒ ä»»åŠ¡æ¨¡å‹ä¸å­˜åœ¨: {task.model_id}")
                    continue
                
                model_info = self.ai_models[task.model_id]
                if model_info.status != AIModelStatus.READY:
                    logger.warning(f"âš ï¸ æ¨¡å‹çŠ¶æ€ä¸å¯ç”¨: {task.model_id} - {model_info.status}")
                    continue
                
                # æ‰§è¡Œä»»åŠ¡
                task.scheduled_at = datetime.now(timezone.utc)
                self.running_tasks[task.task_id] = task
                
                # å¼‚æ­¥æ‰§è¡Œä»»åŠ¡
                asyncio.create_task(self._execute_task(task))
                
            except Exception as e:
                logger.error(f"âŒ ä»»åŠ¡è°ƒåº¦å¼‚å¸¸: {e}")
                await asyncio.sleep(1)
    
    async def _execute_task(self, task: SchedulingTask):
        """æ‰§è¡Œä»»åŠ¡"""
        start_time = time.time()
        
        try:
            model_info = self.ai_models[task.model_id]
            model_instance = self.model_instances[task.model_id]
            
            # æ›´æ–°æ¨¡å‹çŠ¶æ€
            if task.task_type == 'predict':
                model_info.status = AIModelStatus.PREDICTING
            elif task.task_type == 'train':
                model_info.status = AIModelStatus.TRAINING
            
            # æ‰§è¡Œä»»åŠ¡
            result = None
            
            if task.task_type == 'predict':
                if hasattr(model_instance, 'predict'):
                    result = await model_instance.predict(task.data)
                elif hasattr(model_instance, 'get_prediction'):
                    result = await model_instance.get_prediction(task.data)
                else:
                    raise ValueError(f"æ¨¡å‹ {task.model_id} æ²¡æœ‰é¢„æµ‹æ–¹æ³•")
                    
            elif task.task_type == 'train':
                if hasattr(model_instance, 'train'):
                    result = await model_instance.train(task.data)
                elif hasattr(model_instance, 'update_model'):
                    result = await model_instance.update_model(task.data)
                else:
                    raise ValueError(f"æ¨¡å‹ {task.model_id} æ²¡æœ‰è®­ç»ƒæ–¹æ³•")
            
            # ä»»åŠ¡å®Œæˆ
            execution_time = (time.time() - start_time) * 1000  # æ¯«ç§’
            
            task.result = result
            task.completed_at = datetime.now(timezone.utc)
            
            # æ›´æ–°æ¨¡å‹ä¿¡æ¯
            model_info.status = AIModelStatus.READY
            model_info.last_prediction = result
            model_info.last_update = datetime.now(timezone.utc)
            model_info.latency_ms = execution_time
            model_info.prediction_count += 1
            
            # ç§»åŠ¨ä»»åŠ¡
            if task.task_id in self.running_tasks:
                del self.running_tasks[task.task_id]
            self.completed_tasks[task.task_id] = task
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['completed_tasks'] += 1
            self.stats['total_predictions'] += 1
            
            # æ›´æ–°å¹³å‡å»¶è¿Ÿ
            if self.stats['completed_tasks'] > 0:
                self.stats['average_latency'] = (
                    (self.stats['average_latency'] * (self.stats['completed_tasks'] - 1) + execution_time) /
                    self.stats['completed_tasks']
                )
            
            logger.debug(f"âœ… ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {task.task_id} ç”¨æ—¶ {execution_time:.2f}ms")
            
        except Exception as e:
            # ä»»åŠ¡å¤±è´¥
            execution_time = (time.time() - start_time) * 1000
            
            task.error_message = str(e)
            task.completed_at = datetime.now(timezone.utc)
            
            # æ›´æ–°æ¨¡å‹ä¿¡æ¯
            model_info = self.ai_models[task.model_id]
            model_info.status = AIModelStatus.ERROR
            model_info.error_count += 1
            model_info.latency_ms = execution_time
            
            # ç§»åŠ¨ä»»åŠ¡
            if task.task_id in self.running_tasks:
                del self.running_tasks[task.task_id]
            self.completed_tasks[task.task_id] = task
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['failed_tasks'] += 1
            
            logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task.task_id} - {e}")
    
    async def _performance_monitor(self):
        """æ€§èƒ½ç›‘æ§"""
        while self.is_running:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿç›‘æ§ä¸€æ¬¡
                
                # æ›´æ–°æ¨¡å‹æ€§èƒ½åˆ†æ•°
                for model_id, model_info in self.ai_models.items():
                    if model_info.prediction_count > 0:
                        # ç®€å•çš„æ€§èƒ½è¯„åˆ†ï¼šåŸºäºå‡†ç¡®ç‡å’Œå»¶è¿Ÿ
                        accuracy_score = model_info.accuracy
                        latency_score = max(0, 1 - model_info.latency_ms / 1000)  # å»¶è¿Ÿè¶Šä½åˆ†æ•°è¶Šé«˜
                        error_penalty = max(0, 1 - model_info.error_count / max(model_info.prediction_count, 1))
                        
                        model_info.performance_score = (accuracy_score * 0.5 + 
                                                       latency_score * 0.3 + 
                                                       error_penalty * 0.2)
                
                # æ¸…ç†æ—§çš„å®Œæˆä»»åŠ¡
                cutoff_time = datetime.now(timezone.utc) - timedelta(hours=1)
                old_tasks = [tid for tid, task in self.completed_tasks.items() 
                           if task.completed_at and task.completed_at < cutoff_time]
                
                for task_id in old_tasks:
                    del self.completed_tasks[task_id]
                
                logger.debug(f"ğŸ“Š æ€§èƒ½ç›‘æ§å®Œæˆï¼Œæ¸…ç†äº† {len(old_tasks)} ä¸ªæ—§ä»»åŠ¡")
                
            except Exception as e:
                logger.error(f"âŒ æ€§èƒ½ç›‘æ§å¼‚å¸¸: {e}")
    
    async def _health_checker(self):
        """å¥åº·æ£€æŸ¥"""
        while self.is_running:
            try:
                await asyncio.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
                
                # æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€
                for model_id, model_info in self.ai_models.items():
                    # å¦‚æœæ¨¡å‹é•¿æ—¶é—´æ²¡æœ‰æ›´æ–°ï¼Œæ ‡è®°ä¸ºç¦»çº¿
                    if model_info.last_update:
                        time_since_update = datetime.now(timezone.utc) - model_info.last_update
                        if time_since_update > timedelta(minutes=5):
                            if model_info.status != AIModelStatus.OFFLINE:
                                model_info.status = AIModelStatus.OFFLINE
                                logger.warning(f"âš ï¸ æ¨¡å‹ç¦»çº¿: {model_id}")
                    
                    # å¦‚æœé”™è¯¯ç‡è¿‡é«˜ï¼Œæš‚æ—¶ç¦ç”¨æ¨¡å‹
                    if model_info.prediction_count > 10:
                        error_rate = model_info.error_count / model_info.prediction_count
                        if error_rate > 0.5:  # é”™è¯¯ç‡è¶…è¿‡50%
                            model_info.is_active = False
                            logger.warning(f"âš ï¸ æ¨¡å‹é”™è¯¯ç‡è¿‡é«˜ï¼Œå·²ç¦ç”¨: {model_id}")
                
                # æ›´æ–°æ´»è·ƒæ¨¡å‹æ•°é‡
                self.stats['active_models'] = len([m for m in self.ai_models.values() if m.is_active])
                
            except Exception as e:
                logger.error(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
    
    def get_model_status(self, model_id: str = None) -> Dict[str, Any]:
        """è·å–æ¨¡å‹çŠ¶æ€"""
        if model_id:
            if model_id in self.ai_models:
                model_info = self.ai_models[model_id]
                return {
                    'model_id': model_info.model_id,
                    'model_name': model_info.model_name,
                    'status': model_info.status.value,
                    'performance_score': model_info.performance_score,
                    'accuracy': model_info.accuracy,
                    'latency_ms': model_info.latency_ms,
                    'prediction_count': model_info.prediction_count,
                    'error_count': model_info.error_count,
                    'is_active': model_info.is_active,
                    'last_update': model_info.last_update.isoformat()
                }
            else:
                return {}
        else:
            # è¿”å›æ‰€æœ‰æ¨¡å‹çŠ¶æ€
            return {
                mid: {
                    'model_id': info.model_id,
                    'model_name': info.model_name,
                    'status': info.status.value,
                    'performance_score': info.performance_score,
                    'accuracy': info.accuracy,
                    'latency_ms': info.latency_ms,
                    'prediction_count': info.prediction_count,
                    'error_count': info.error_count,
                    'is_active': info.is_active,
                    'last_update': info.last_update.isoformat()
                }
                for mid, info in self.ai_models.items()
            }
    
    def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡"""
        resource_usage = self.resource_manager.get_resource_usage()
        
        return {
            'scheduler_stats': self.stats,
            'resource_usage': resource_usage,
            'queue_size': self.task_queue.qsize(),
            'running_tasks': len(self.running_tasks),
            'completed_tasks': len(self.completed_tasks),
            'registered_models': len(self.ai_models),
            'active_models': self.stats['active_models']
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            return {
                'status': 'healthy' if self.is_running else 'stopped',
                'scheduler_running': self.is_running,
                'active_models': self.stats['active_models'],
                'queue_size': self.task_queue.qsize(),
                'average_latency': f"{self.stats['average_latency']:.2f}ms",
                'success_rate': (
                    (self.stats['completed_tasks'] / max(self.stats['total_tasks'], 1)) * 100
                    if self.stats['total_tasks'] > 0 else 0
                )
            }
        except Exception as e:
            return {'status': 'error', 'error': str(e)}

