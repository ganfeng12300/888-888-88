#!/usr/bin/env python3
"""
ğŸ“ˆ 888-888-88 AIæ€§èƒ½ç›‘æ§å™¨
Production-Grade AI Performance Monitoring System
"""

import asyncio
import json
import time
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from pathlib import Path
from collections import deque, defaultdict
import numpy as np
from loguru import logger

# å¯¼å…¥é”™è¯¯å¤„ç†ç³»ç»Ÿ
from src.core.error_handling_system import ai_operation, handle_errors

@dataclass
class ModelPerformanceMetrics:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    model_id: str
    timestamp: datetime
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    inference_time_ms: float
    memory_usage_mb: float
    prediction_count: int
    error_count: int
    confidence_avg: float
    confidence_std: float

@dataclass
class PredictionRecord:
    """é¢„æµ‹è®°å½•"""
    model_id: str
    timestamp: datetime
    features: Dict[str, Any]
    prediction: Any
    confidence: float
    actual_result: Optional[Any] = None
    processing_time_ms: float = 0.0
    error: Optional[str] = None

class AIPerformanceMonitor:
    """AIæ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, max_records: int = 10000):
        self.max_records = max_records
        self.prediction_records: deque = deque(maxlen=max_records)
        self.model_metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
        self.model_stats: Dict[str, Dict[str, Any]] = defaultdict(dict)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_predictions = 0
        self.total_errors = 0
        self.model_prediction_counts = defaultdict(int)
        self.model_error_counts = defaultdict(int)
        
        # åˆ›å»ºç›‘æ§æ—¥å¿—ç›®å½•
        self.log_dir = Path("logs/ai_performance")
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("ğŸ“ˆ AIæ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")
    
    @ai_operation
    async def record_prediction(self, model_id: str, features: Dict[str, Any], 
                               prediction: Any, confidence: float, 
                               processing_time_ms: float, actual_result: Optional[Any] = None,
                               error: Optional[str] = None):
        """è®°å½•é¢„æµ‹ç»“æœ"""
        try:
            record = PredictionRecord(
                model_id=model_id,
                timestamp=datetime.now(),
                features=features,
                prediction=prediction,
                confidence=confidence,
                actual_result=actual_result,
                processing_time_ms=processing_time_ms,
                error=error
            )
            
            self.prediction_records.append(record)
            
            # æ›´æ–°ç»Ÿè®¡
            self.total_predictions += 1
            self.model_prediction_counts[model_id] += 1
            
            if error:
                self.total_errors += 1
                self.model_error_counts[model_id] += 1
            
            # å¼‚æ­¥è®°å½•åˆ°æ–‡ä»¶
            asyncio.create_task(self._log_prediction_record(record))
            
        except Exception as e:
            logger.error(f"âŒ è®°å½•é¢„æµ‹ç»“æœå¤±è´¥: {e}")
    
    async def _log_prediction_record(self, record: PredictionRecord):
        """è®°å½•é¢„æµ‹åˆ°æ–‡ä»¶"""
        try:
            log_file = self.log_dir / f"predictions_{datetime.now().strftime('%Y%m%d')}.jsonl"
            
            record_data = {
                "model_id": record.model_id,
                "timestamp": record.timestamp.isoformat(),
                "prediction": str(record.prediction),
                "confidence": record.confidence,
                "processing_time_ms": record.processing_time_ms,
                "has_actual": record.actual_result is not None,
                "has_error": record.error is not None
            }
            
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(record_data, ensure_ascii=False) + '\n')
                
        except Exception as e:
            logger.error(f"âŒ è®°å½•é¢„æµ‹æ—¥å¿—å¤±è´¥: {e}")
    
    @ai_operation
    async def calculate_model_accuracy(self, model_id: str, hours: int = 24) -> float:
        """è®¡ç®—æ¨¡å‹å‡†ç¡®ç‡"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            # è·å–æœ‰å®é™…ç»“æœçš„é¢„æµ‹è®°å½•
            relevant_records = [
                record for record in self.prediction_records
                if (record.model_id == model_id and 
                    record.timestamp >= cutoff_time and
                    record.actual_result is not None and
                    record.error is None)
            ]
            
            if not relevant_records:
                return 0.0
            
            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ä¸šåŠ¡é€»è¾‘è°ƒæ•´ï¼‰
            correct_predictions = 0
            
            for record in relevant_records:
                # å¯¹äºæ•°å€¼é¢„æµ‹ï¼Œä½¿ç”¨é˜ˆå€¼åˆ¤æ–­
                if isinstance(record.prediction, (int, float)) and isinstance(record.actual_result, (int, float)):
                    # å¦‚æœé¢„æµ‹å€¼ä¸å®é™…å€¼çš„ç›¸å¯¹è¯¯å·®å°äº10%ï¼Œè®¤ä¸ºé¢„æµ‹æ­£ç¡®
                    relative_error = abs(record.prediction - record.actual_result) / max(abs(record.actual_result), 1e-6)
                    if relative_error < 0.1:
                        correct_predictions += 1
                # å¯¹äºåˆ†ç±»é¢„æµ‹ï¼Œç›´æ¥æ¯”è¾ƒ
                elif record.prediction == record.actual_result:
                    correct_predictions += 1
            
            accuracy = correct_predictions / len(relevant_records)
            
            # æ›´æ–°æ¨¡å‹ç»Ÿè®¡
            self.model_stats[model_id]['accuracy'] = accuracy
            self.model_stats[model_id]['accuracy_updated'] = datetime.now()
            
            return accuracy
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—æ¨¡å‹å‡†ç¡®ç‡å¤±è´¥ {model_id}: {e}")
            return 0.0
    
    @ai_operation
    async def get_average_accuracy(self, hours: int = 24) -> float:
        """è·å–å¹³å‡å‡†ç¡®ç‡"""
        try:
            model_ids = set(record.model_id for record in self.prediction_records)
            
            if not model_ids:
                return 0.0
            
            accuracies = []
            for model_id in model_ids:
                accuracy = await self.calculate_model_accuracy(model_id, hours)
                if accuracy > 0:
                    accuracies.append(accuracy)
            
            return sum(accuracies) / len(accuracies) if accuracies else 0.0
            
        except Exception as e:
            logger.error(f"âŒ è·å–å¹³å‡å‡†ç¡®ç‡å¤±è´¥: {e}")
            return 0.0
    
    @ai_operation
    async def get_average_inference_time(self, model_id: Optional[str] = None, hours: int = 24) -> float:
        """è·å–å¹³å‡æ¨ç†æ—¶é—´"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            relevant_records = [
                record for record in self.prediction_records
                if (record.timestamp >= cutoff_time and
                    record.error is None and
                    (model_id is None or record.model_id == model_id))
            ]
            
            if not relevant_records:
                return 0.0
            
            processing_times = [record.processing_time_ms for record in relevant_records]
            return sum(processing_times) / len(processing_times)
            
        except Exception as e:
            logger.error(f"âŒ è·å–å¹³å‡æ¨ç†æ—¶é—´å¤±è´¥: {e}")
            return 0.0
    
    @ai_operation
    async def get_predictions_count(self, model_id: Optional[str] = None, hours: int = 24) -> int:
        """è·å–é¢„æµ‹æ¬¡æ•°"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            relevant_records = [
                record for record in self.prediction_records
                if (record.timestamp >= cutoff_time and
                    (model_id is None or record.model_id == model_id))
            ]
            
            return len(relevant_records)
            
        except Exception as e:
            logger.error(f"âŒ è·å–é¢„æµ‹æ¬¡æ•°å¤±è´¥: {e}")
            return 0
    
    @ai_operation
    async def get_error_count(self, model_id: Optional[str] = None, hours: int = 24) -> int:
        """è·å–é”™è¯¯æ¬¡æ•°"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            error_records = [
                record for record in self.prediction_records
                if (record.timestamp >= cutoff_time and
                    record.error is not None and
                    (model_id is None or record.model_id == model_id))
            ]
            
            return len(error_records)
            
        except Exception as e:
            logger.error(f"âŒ è·å–é”™è¯¯æ¬¡æ•°å¤±è´¥: {e}")
            return 0
    
    @ai_operation
    async def get_model_performance_summary(self, model_id: str, hours: int = 24) -> Dict[str, Any]:
        """è·å–æ¨¡å‹æ€§èƒ½æ‘˜è¦"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            # è·å–ç›¸å…³è®°å½•
            model_records = [
                record for record in self.prediction_records
                if record.model_id == model_id and record.timestamp >= cutoff_time
            ]
            
            if not model_records:
                return {
                    "model_id": model_id,
                    "period_hours": hours,
                    "total_predictions": 0,
                    "error_count": 0,
                    "error_rate": 0.0,
                    "accuracy": 0.0,
                    "avg_inference_time_ms": 0.0,
                    "avg_confidence": 0.0,
                    "confidence_std": 0.0
                }
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            total_predictions = len(model_records)
            error_records = [r for r in model_records if r.error is not None]
            error_count = len(error_records)
            error_rate = error_count / total_predictions
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = await self.calculate_model_accuracy(model_id, hours)
            
            # è®¡ç®—æ¨ç†æ—¶é—´
            successful_records = [r for r in model_records if r.error is None]
            if successful_records:
                inference_times = [r.processing_time_ms for r in successful_records]
                avg_inference_time = sum(inference_times) / len(inference_times)
                
                confidences = [r.confidence for r in successful_records]
                avg_confidence = sum(confidences) / len(confidences)
                confidence_std = np.std(confidences) if len(confidences) > 1 else 0.0
            else:
                avg_inference_time = 0.0
                avg_confidence = 0.0
                confidence_std = 0.0
            
            return {
                "model_id": model_id,
                "period_hours": hours,
                "total_predictions": total_predictions,
                "error_count": error_count,
                "error_rate": error_rate,
                "accuracy": accuracy,
                "avg_inference_time_ms": avg_inference_time,
                "avg_confidence": avg_confidence,
                "confidence_std": confidence_std,
                "last_prediction": model_records[-1].timestamp.isoformat() if model_records else None
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ¨¡å‹æ€§èƒ½æ‘˜è¦å¤±è´¥ {model_id}: {e}")
            return {}
    
    @ai_operation
    async def get_all_models_performance(self, hours: int = 24) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½æ‘˜è¦"""
        try:
            model_ids = set(record.model_id for record in self.prediction_records)
            
            performance_data = {}
            
            for model_id in model_ids:
                performance_data[model_id] = await self.get_model_performance_summary(model_id, hours)
            
            return performance_data
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ‰€æœ‰æ¨¡å‹æ€§èƒ½å¤±è´¥: {e}")
            return {}
    
    @ai_operation
    async def get_performance_trends(self, model_id: str, days: int = 7) -> Dict[str, List[Any]]:
        """è·å–æ€§èƒ½è¶‹åŠ¿æ•°æ®"""
        try:
            end_time = datetime.now()
            start_time = end_time - timedelta(days=days)
            
            # æŒ‰å°æ—¶åˆ†ç»„ç»Ÿè®¡
            hourly_stats = defaultdict(lambda: {
                'predictions': 0,
                'errors': 0,
                'inference_times': [],
                'confidences': []
            })
            
            for record in self.prediction_records:
                if (record.model_id == model_id and 
                    start_time <= record.timestamp <= end_time):
                    
                    hour_key = record.timestamp.replace(minute=0, second=0, microsecond=0)
                    
                    hourly_stats[hour_key]['predictions'] += 1
                    
                    if record.error:
                        hourly_stats[hour_key]['errors'] += 1
                    else:
                        hourly_stats[hour_key]['inference_times'].append(record.processing_time_ms)
                        hourly_stats[hour_key]['confidences'].append(record.confidence)
            
            # æ„å»ºè¶‹åŠ¿æ•°æ®
            timestamps = []
            prediction_counts = []
            error_rates = []
            avg_inference_times = []
            avg_confidences = []
            
            # æŒ‰æ—¶é—´æ’åº
            sorted_hours = sorted(hourly_stats.keys())
            
            for hour in sorted_hours:
                stats = hourly_stats[hour]
                
                timestamps.append(hour.isoformat())
                prediction_counts.append(stats['predictions'])
                
                error_rate = stats['errors'] / max(1, stats['predictions'])
                error_rates.append(error_rate)
                
                avg_inference_time = (
                    sum(stats['inference_times']) / len(stats['inference_times'])
                    if stats['inference_times'] else 0.0
                )
                avg_inference_times.append(avg_inference_time)
                
                avg_confidence = (
                    sum(stats['confidences']) / len(stats['confidences'])
                    if stats['confidences'] else 0.0
                )
                avg_confidences.append(avg_confidence)
            
            return {
                "model_id": model_id,
                "period_days": days,
                "timestamps": timestamps,
                "prediction_counts": prediction_counts,
                "error_rates": error_rates,
                "avg_inference_times_ms": avg_inference_times,
                "avg_confidences": avg_confidences
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ€§èƒ½è¶‹åŠ¿å¤±è´¥ {model_id}: {e}")
            return {}
    
    @ai_operation
    async def detect_performance_anomalies(self, model_id: str, hours: int = 24) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ€§èƒ½å¼‚å¸¸"""
        try:
            anomalies = []
            
            # è·å–æ¨¡å‹æ€§èƒ½æ‘˜è¦
            performance = await self.get_model_performance_summary(model_id, hours)
            
            if not performance or performance['total_predictions'] == 0:
                return anomalies
            
            # æ£€æŸ¥é”™è¯¯ç‡å¼‚å¸¸
            if performance['error_rate'] > 0.1:  # é”™è¯¯ç‡è¶…è¿‡10%
                anomalies.append({
                    "type": "high_error_rate",
                    "severity": "high" if performance['error_rate'] > 0.2 else "medium",
                    "message": f"æ¨¡å‹ {model_id} é”™è¯¯ç‡è¿‡é«˜: {performance['error_rate']:.2%}",
                    "value": performance['error_rate'],
                    "threshold": 0.1
                })
            
            # æ£€æŸ¥æ¨ç†æ—¶é—´å¼‚å¸¸
            if performance['avg_inference_time_ms'] > 1000:  # æ¨ç†æ—¶é—´è¶…è¿‡1ç§’
                anomalies.append({
                    "type": "slow_inference",
                    "severity": "high" if performance['avg_inference_time_ms'] > 5000 else "medium",
                    "message": f"æ¨¡å‹ {model_id} æ¨ç†æ—¶é—´è¿‡é•¿: {performance['avg_inference_time_ms']:.1f}ms",
                    "value": performance['avg_inference_time_ms'],
                    "threshold": 1000
                })
            
            # æ£€æŸ¥å‡†ç¡®ç‡å¼‚å¸¸
            if performance['accuracy'] < 0.6:  # å‡†ç¡®ç‡ä½äº60%
                anomalies.append({
                    "type": "low_accuracy",
                    "severity": "high" if performance['accuracy'] < 0.4 else "medium",
                    "message": f"æ¨¡å‹ {model_id} å‡†ç¡®ç‡è¿‡ä½: {performance['accuracy']:.2%}",
                    "value": performance['accuracy'],
                    "threshold": 0.6
                })
            
            # æ£€æŸ¥ç½®ä¿¡åº¦å¼‚å¸¸
            if performance['avg_confidence'] < 0.5:  # å¹³å‡ç½®ä¿¡åº¦ä½äº50%
                anomalies.append({
                    "type": "low_confidence",
                    "severity": "medium",
                    "message": f"æ¨¡å‹ {model_id} å¹³å‡ç½®ä¿¡åº¦è¿‡ä½: {performance['avg_confidence']:.2%}",
                    "value": performance['avg_confidence'],
                    "threshold": 0.5
                })
            
            return anomalies
            
        except Exception as e:
            logger.error(f"âŒ æ£€æµ‹æ€§èƒ½å¼‚å¸¸å¤±è´¥ {model_id}: {e}")
            return []
    
    def get_monitor_stats(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_predictions": self.total_predictions,
            "total_errors": self.total_errors,
            "overall_error_rate": self.total_errors / max(1, self.total_predictions),
            "monitored_models": len(set(record.model_id for record in self.prediction_records)),
            "records_count": len(self.prediction_records),
            "max_records": self.max_records
        }
    
    async def cleanup_old_records(self, days: int = 30):
        """æ¸…ç†æ—§è®°å½•"""
        try:
            cutoff_time = datetime.now() - timedelta(days=days)
            
            # æ¸…ç†å†…å­˜ä¸­çš„è®°å½•
            original_count = len(self.prediction_records)
            self.prediction_records = deque(
                (record for record in self.prediction_records if record.timestamp >= cutoff_time),
                maxlen=self.max_records
            )
            
            cleaned_count = original_count - len(self.prediction_records)
            
            if cleaned_count > 0:
                logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} æ¡æ—§çš„é¢„æµ‹è®°å½•")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ—§è®°å½•å¤±è´¥: {e}")

# å…¨å±€AIæ€§èƒ½ç›‘æ§å™¨å®ä¾‹
ai_performance_monitor = AIPerformanceMonitor()

# å¯¼å‡ºä¸»è¦ç»„ä»¶
__all__ = [
    'AIPerformanceMonitor',
    'ModelPerformanceMetrics',
    'PredictionRecord',
    'ai_performance_monitor'
]
