#!/usr/bin/env python3
"""
ğŸ§  å…ƒå­¦ä¹ AIæŒ‡æŒ¥å®˜ - å†³ç­–åè°ƒæ ¸å¿ƒ
è´Ÿè´£åè°ƒ8å¤§AIæ¨¡å‹ï¼Œè¿›è¡Œå…ƒå­¦ä¹ å’Œå†³ç­–èåˆ
ä¸“ä¸ºç”Ÿäº§çº§å®ç›˜äº¤æ˜“è®¾è®¡ï¼Œæ”¯æŒGPUåŠ é€Ÿè®­ç»ƒ
"""

import asyncio
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timezone
import json
from dataclasses import dataclass
from loguru import logger
import threading
import time
from concurrent.futures import ThreadPoolExecutor
import psutil
import GPUtil

@dataclass
class AIModelStatus:
    """AIæ¨¡å‹çŠ¶æ€"""
    model_id: str
    model_name: str
    confidence: float
    prediction: float
    last_update: datetime
    performance_score: float
    training_progress: float
    gpu_usage: float
    memory_usage: float
    is_active: bool

@dataclass
class MetaDecision:
    """å…ƒå†³ç­–ç»“æœ"""
    final_signal: float  # -1åˆ°1ä¹‹é—´çš„æœ€ç»ˆä¿¡å·
    confidence: float    # 0åˆ°1ä¹‹é—´çš„ç½®ä¿¡åº¦
    contributing_models: List[str]  # å‚ä¸å†³ç­–çš„æ¨¡å‹
    model_weights: Dict[str, float]  # å„æ¨¡å‹æƒé‡
    risk_assessment: float  # é£é™©è¯„ä¼°
    timestamp: datetime
    reasoning: str  # å†³ç­–æ¨ç†

class MetaLearningNetwork(nn.Module):
    """å…ƒå­¦ä¹ ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_dim: int = 64, hidden_dim: int = 256, output_dim: int = 32):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        
        # æ³¨æ„åŠ›æœºåˆ¶å±‚
        self.attention = nn.MultiheadAttention(
            embed_dim=input_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        # ç‰¹å¾æå–ç½‘ç»œ
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # å†³ç­–èåˆç½‘ç»œ
        self.decision_fusion = nn.Sequential(
            nn.Linear(output_dim * 8, hidden_dim),  # 8ä¸ªAIæ¨¡å‹
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Tanh()  # è¾“å‡º-1åˆ°1ä¹‹é—´çš„ä¿¡å·
        )
        
        # ç½®ä¿¡åº¦ç½‘ç»œ
        self.confidence_network = nn.Sequential(
            nn.Linear(output_dim * 8, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()  # è¾“å‡º0åˆ°1ä¹‹é—´çš„ç½®ä¿¡åº¦
        )
        
    def forward(self, model_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        Args:
            model_features: [batch_size, num_models, feature_dim]
        Returns:
            decision: å†³ç­–ä¿¡å·
            confidence: ç½®ä¿¡åº¦
        """
        batch_size, num_models, feature_dim = model_features.shape
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attended_features, attention_weights = self.attention(
            model_features, model_features, model_features
        )
        
        # ç‰¹å¾æå–
        extracted_features = self.feature_extractor(attended_features)
        
        # å±•å¹³ç‰¹å¾
        flattened_features = extracted_features.view(batch_size, -1)
        
        # å†³ç­–èåˆ
        decision = self.decision_fusion(flattened_features)
        
        # ç½®ä¿¡åº¦è®¡ç®—
        confidence = self.confidence_network(flattened_features)
        
        return decision.squeeze(-1), confidence.squeeze(-1)

class MetaLearningCommander:
    """å…ƒå­¦ä¹ AIæŒ‡æŒ¥å®˜"""
    
    def __init__(self, device: str = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_statuses: Dict[str, AIModelStatus] = {}
        self.decision_history: List[MetaDecision] = []
        self.performance_metrics = {
            'total_decisions': 0,
            'correct_predictions': 0,
            'accuracy': 0.0,
            'avg_confidence': 0.0,
            'avg_return': 0.0,
            'sharpe_ratio': 0.0,
            'max_drawdown': 0.0
        }
        
        # åˆå§‹åŒ–å…ƒå­¦ä¹ ç½‘ç»œ
        self.meta_network = MetaLearningNetwork().to(self.device)
        self.optimizer = optim.AdamW(
            self.meta_network.parameters(),
            lr=0.001,
            weight_decay=0.01
        )
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=1000
        )
        
        # æ¨¡å‹æƒé‡ï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰
        self.model_weights = {
            'reinforcement_trader': 0.20,
            'time_series_prophet': 0.18,
            'ensemble_brain_trust': 0.15,
            'transfer_learning_adapter': 0.12,
            'expert_system_guardian': 0.10,
            'sentiment_scout': 0.10,
            'factor_mining_engine': 0.10,
            'meta_learning_commander': 0.05
        }
        
        # è®­ç»ƒå‚æ•°
        self.training_enabled = True
        self.batch_size = 32
        self.sequence_length = 100
        self.training_buffer = []
        self.max_buffer_size = 10000
        
        # æ€§èƒ½ç›‘æ§
        self.last_gpu_check = time.time()
        self.gpu_temperature = 0.0
        self.gpu_utilization = 0.0
        self.memory_usage = 0.0
        
        # çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        logger.info(f"ğŸ§  å…ƒå­¦ä¹ AIæŒ‡æŒ¥å®˜åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
        
    def register_ai_model(self, model_id: str, model_name: str) -> bool:
        """æ³¨å†ŒAIæ¨¡å‹"""
        try:
            self.model_statuses[model_id] = AIModelStatus(
                model_id=model_id,
                model_name=model_name,
                confidence=0.0,
                prediction=0.0,
                last_update=datetime.now(timezone.utc),
                performance_score=0.5,
                training_progress=0.0,
                gpu_usage=0.0,
                memory_usage=0.0,
                is_active=True
            )
            logger.info(f"âœ… AIæ¨¡å‹å·²æ³¨å†Œ: {model_name} ({model_id})")
            return True
        except Exception as e:
            logger.error(f"âŒ AIæ¨¡å‹æ³¨å†Œå¤±è´¥: {e}")
            return False
    
    def update_model_status(self, model_id: str, prediction: float, 
                          confidence: float, performance_score: float = None) -> bool:
        """æ›´æ–°AIæ¨¡å‹çŠ¶æ€"""
        try:
            if model_id not in self.model_statuses:
                logger.warning(f"âš ï¸ æœªçŸ¥çš„AIæ¨¡å‹ID: {model_id}")
                return False
            
            status = self.model_statuses[model_id]
            status.prediction = prediction
            status.confidence = confidence
            status.last_update = datetime.now(timezone.utc)
            
            if performance_score is not None:
                status.performance_score = performance_score
            
            # æ›´æ–°GPUå’Œå†…å­˜ä½¿ç”¨æƒ…å†µ
            self._update_hardware_metrics(model_id)
            
            return True
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ¨¡å‹çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def _update_hardware_metrics(self, model_id: str):
        """æ›´æ–°ç¡¬ä»¶æŒ‡æ ‡"""
        try:
            current_time = time.time()
            if current_time - self.last_gpu_check > 5:  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                # GPUç›‘æ§
                if torch.cuda.is_available():
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu = gpus[0]
                        self.gpu_temperature = gpu.temperature
                        self.gpu_utilization = gpu.load * 100
                        self.memory_usage = gpu.memoryUtil * 100
                
                # æ›´æ–°æ‰€æœ‰æ¨¡å‹çš„ç¡¬ä»¶æŒ‡æ ‡
                for status in self.model_statuses.values():
                    status.gpu_usage = self.gpu_utilization
                    status.memory_usage = self.memory_usage
                
                self.last_gpu_check = current_time
                
        except Exception as e:
            logger.warning(f"âš ï¸ ç¡¬ä»¶æŒ‡æ ‡æ›´æ–°å¤±è´¥: {e}")
    
    async def make_meta_decision(self, market_data: Dict[str, Any]) -> MetaDecision:
        """åˆ¶ä½œå…ƒå†³ç­–"""
        try:
            # æ”¶é›†æ‰€æœ‰æ´»è·ƒæ¨¡å‹çš„é¢„æµ‹
            active_models = {
                k: v for k, v in self.model_statuses.items() 
                if v.is_active and (datetime.now(timezone.utc) - v.last_update).seconds < 60
            }
            
            if len(active_models) < 3:
                logger.warning("âš ï¸ æ´»è·ƒæ¨¡å‹æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨ä¿å®ˆå†³ç­–")
                return self._make_conservative_decision()
            
            # å‡†å¤‡æ¨¡å‹ç‰¹å¾
            model_features = self._prepare_model_features(active_models, market_data)
            
            # GPUæ¨ç†
            with torch.no_grad():
                features_tensor = torch.FloatTensor(model_features).unsqueeze(0).to(self.device)
                decision_signal, confidence = self.meta_network(features_tensor)
                
                final_signal = float(decision_signal.cpu().numpy()[0])
                final_confidence = float(confidence.cpu().numpy()[0])
            
            # è®¡ç®—æ¨¡å‹æƒé‡
            model_weights = self._calculate_dynamic_weights(active_models)
            
            # é£é™©è¯„ä¼°
            risk_assessment = self._assess_risk(active_models, market_data)
            
            # åˆ›å»ºå…ƒå†³ç­–
            meta_decision = MetaDecision(
                final_signal=final_signal,
                confidence=final_confidence,
                contributing_models=list(active_models.keys()),
                model_weights=model_weights,
                risk_assessment=risk_assessment,
                timestamp=datetime.now(timezone.utc),
                reasoning=self._generate_reasoning(active_models, final_signal, final_confidence)
            )
            
            # è®°å½•å†³ç­–å†å²
            self.decision_history.append(meta_decision)
            if len(self.decision_history) > 1000:
                self.decision_history = self.decision_history[-1000:]
            
            # å¼‚æ­¥è®­ç»ƒ
            if self.training_enabled:
                asyncio.create_task(self._async_training_update(model_features, final_signal))
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self._update_performance_metrics()
            
            logger.info(f"ğŸ§  å…ƒå†³ç­–å®Œæˆ - ä¿¡å·: {final_signal:.4f}, ç½®ä¿¡åº¦: {final_confidence:.4f}")
            return meta_decision
            
        except Exception as e:
            logger.error(f"âŒ å…ƒå†³ç­–åˆ¶ä½œå¤±è´¥: {e}")
            return self._make_conservative_decision()
    
    def _prepare_model_features(self, active_models: Dict[str, AIModelStatus], 
                              market_data: Dict[str, Any]) -> np.ndarray:
        """å‡†å¤‡æ¨¡å‹ç‰¹å¾"""
        features = []
        
        for model_id, status in active_models.items():
            # åŸºç¡€ç‰¹å¾
            model_feature = [
                status.prediction,
                status.confidence,
                status.performance_score,
                status.training_progress,
                status.gpu_usage / 100.0,
                status.memory_usage / 100.0,
                float(status.is_active),
                (datetime.now(timezone.utc) - status.last_update).seconds / 60.0
            ]
            
            # å¸‚åœºç‰¹å¾
            if market_data:
                model_feature.extend([
                    market_data.get('price_change', 0.0),
                    market_data.get('volume_ratio', 1.0),
                    market_data.get('volatility', 0.0),
                    market_data.get('rsi', 50.0) / 100.0,
                    market_data.get('macd_signal', 0.0),
                    market_data.get('bb_position', 0.5),
                    market_data.get('sentiment_score', 0.0),
                    market_data.get('news_impact', 0.0)
                ])
            else:
                model_feature.extend([0.0] * 8)
            
            # å¡«å……åˆ°64ç»´
            while len(model_feature) < 64:
                model_feature.append(0.0)
            
            features.append(model_feature[:64])
        
        # ç¡®ä¿æœ‰8ä¸ªæ¨¡å‹çš„ç‰¹å¾ï¼ˆä¸è¶³çš„ç”¨é›¶å¡«å……ï¼‰
        while len(features) < 8:
            features.append([0.0] * 64)
        
        return np.array(features[:8])
    
    def _calculate_dynamic_weights(self, active_models: Dict[str, AIModelStatus]) -> Dict[str, float]:
        """è®¡ç®—åŠ¨æ€æƒé‡"""
        weights = {}
        total_score = sum(status.performance_score * status.confidence for status in active_models.values())
        
        if total_score > 0:
            for model_id, status in active_models.items():
                weights[model_id] = (status.performance_score * status.confidence) / total_score
        else:
            # å‡ç­‰æƒé‡
            weight = 1.0 / len(active_models)
            for model_id in active_models:
                weights[model_id] = weight
        
        return weights
    
    def _assess_risk(self, active_models: Dict[str, AIModelStatus], 
                    market_data: Dict[str, Any]) -> float:
        """è¯„ä¼°é£é™©"""
        try:
            risk_factors = []
            
            # æ¨¡å‹ä¸€è‡´æ€§é£é™©
            predictions = [status.prediction for status in active_models.values()]
            if predictions:
                prediction_std = np.std(predictions)
                risk_factors.append(prediction_std)
            
            # ç½®ä¿¡åº¦é£é™©
            confidences = [status.confidence for status in active_models.values()]
            if confidences:
                avg_confidence = np.mean(confidences)
                risk_factors.append(1.0 - avg_confidence)
            
            # å¸‚åœºæ³¢åŠ¨é£é™©
            if market_data:
                volatility = market_data.get('volatility', 0.0)
                risk_factors.append(volatility)
            
            # GPUæ¸©åº¦é£é™©
            if self.gpu_temperature > 80:
                risk_factors.append(0.5)
            elif self.gpu_temperature > 75:
                risk_factors.append(0.3)
            
            return min(np.mean(risk_factors) if risk_factors else 0.5, 1.0)
            
        except Exception as e:
            logger.warning(f"âš ï¸ é£é™©è¯„ä¼°å¤±è´¥: {e}")
            return 0.5
    
    def _generate_reasoning(self, active_models: Dict[str, AIModelStatus], 
                          signal: float, confidence: float) -> str:
        """ç”Ÿæˆå†³ç­–æ¨ç†"""
        try:
            reasoning_parts = []
            
            # ä¿¡å·å¼ºåº¦åˆ†æ
            if abs(signal) > 0.7:
                reasoning_parts.append(f"å¼ºçƒˆ{'ä¹°å…¥' if signal > 0 else 'å–å‡º'}ä¿¡å·({signal:.3f})")
            elif abs(signal) > 0.3:
                reasoning_parts.append(f"ä¸­ç­‰{'ä¹°å…¥' if signal > 0 else 'å–å‡º'}ä¿¡å·({signal:.3f})")
            else:
                reasoning_parts.append(f"å¼±{'ä¹°å…¥' if signal > 0 else 'å–å‡º'}ä¿¡å·({signal:.3f})")
            
            # ç½®ä¿¡åº¦åˆ†æ
            if confidence > 0.8:
                reasoning_parts.append(f"é«˜ç½®ä¿¡åº¦({confidence:.3f})")
            elif confidence > 0.6:
                reasoning_parts.append(f"ä¸­ç­‰ç½®ä¿¡åº¦({confidence:.3f})")
            else:
                reasoning_parts.append(f"ä½ç½®ä¿¡åº¦({confidence:.3f})")
            
            # æ¨¡å‹ä¸€è‡´æ€§
            predictions = [status.prediction for status in active_models.values()]
            if predictions:
                agreement = 1.0 - np.std(predictions)
                if agreement > 0.8:
                    reasoning_parts.append("æ¨¡å‹é«˜åº¦ä¸€è‡´")
                elif agreement > 0.6:
                    reasoning_parts.append("æ¨¡å‹åŸºæœ¬ä¸€è‡´")
                else:
                    reasoning_parts.append("æ¨¡å‹å­˜åœ¨åˆ†æ­§")
            
            # æ´»è·ƒæ¨¡å‹æ•°é‡
            reasoning_parts.append(f"{len(active_models)}ä¸ªæ¨¡å‹å‚ä¸å†³ç­–")
            
            return " | ".join(reasoning_parts)
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ¨ç†ç”Ÿæˆå¤±è´¥: {e}")
            return f"ä¿¡å·: {signal:.3f}, ç½®ä¿¡åº¦: {confidence:.3f}"
    
    def _make_conservative_decision(self) -> MetaDecision:
        """åˆ¶ä½œä¿å®ˆå†³ç­–"""
        return MetaDecision(
            final_signal=0.0,
            confidence=0.1,
            contributing_models=[],
            model_weights={},
            risk_assessment=0.9,
            timestamp=datetime.now(timezone.utc),
            reasoning="ä¿å®ˆå†³ç­– - æ¨¡å‹æ•°é‡ä¸è¶³æˆ–æ•°æ®å¼‚å¸¸"
        )
    
    async def _async_training_update(self, model_features: np.ndarray, target_signal: float):
        """å¼‚æ­¥è®­ç»ƒæ›´æ–°"""
        try:
            # æ·»åŠ åˆ°è®­ç»ƒç¼“å†²åŒº
            self.training_buffer.append({
                'features': model_features,
                'target': target_signal,
                'timestamp': time.time()
            })
            
            # é™åˆ¶ç¼“å†²åŒºå¤§å°
            if len(self.training_buffer) > self.max_buffer_size:
                self.training_buffer = self.training_buffer[-self.max_buffer_size:]
            
            # æ‰¹é‡è®­ç»ƒ
            if len(self.training_buffer) >= self.batch_size:
                await self._batch_training()
                
        except Exception as e:
            logger.warning(f"âš ï¸ å¼‚æ­¥è®­ç»ƒæ›´æ–°å¤±è´¥: {e}")
    
    async def _batch_training(self):
        """æ‰¹é‡è®­ç»ƒ"""
        try:
            if not self.training_enabled or len(self.training_buffer) < self.batch_size:
                return
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            batch_data = self.training_buffer[-self.batch_size:]
            features = torch.FloatTensor([item['features'] for item in batch_data]).to(self.device)
            targets = torch.FloatTensor([item['target'] for item in batch_data]).to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.meta_network.train()
            predictions, confidences = self.meta_network(features)
            
            # è®¡ç®—æŸå¤±
            prediction_loss = nn.MSELoss()(predictions, targets)
            confidence_loss = nn.BCELoss()(confidences, torch.abs(targets))
            total_loss = prediction_loss + 0.1 * confidence_loss
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.meta_network.parameters(), max_norm=1.0)
            self.optimizer.step()
            self.scheduler.step()
            
            self.meta_network.eval()
            
            logger.debug(f"ğŸ§  å…ƒå­¦ä¹ è®­ç»ƒå®Œæˆ - æŸå¤±: {total_loss.item():.6f}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ‰¹é‡è®­ç»ƒå¤±è´¥: {e}")
    
    def _update_performance_metrics(self):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        try:
            if len(self.decision_history) < 2:
                return
            
            recent_decisions = self.decision_history[-100:]  # æœ€è¿‘100ä¸ªå†³ç­–
            
            # åŸºç¡€ç»Ÿè®¡
            self.performance_metrics['total_decisions'] = len(self.decision_history)
            self.performance_metrics['avg_confidence'] = np.mean([d.confidence for d in recent_decisions])
            
            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆéœ€è¦å®é™…æ”¶ç›Šæ•°æ®ï¼‰
            # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„è®¡ç®—æ–¹å¼
            signals = [d.final_signal for d in recent_decisions]
            if signals:
                self.performance_metrics['avg_return'] = np.mean(signals)
                self.performance_metrics['sharpe_ratio'] = np.mean(signals) / (np.std(signals) + 1e-8)
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ€§èƒ½æŒ‡æ ‡æ›´æ–°å¤±è´¥: {e}")
    
    def get_status_report(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€æŠ¥å‘Š"""
        try:
            return {
                'commander_status': {
                    'device': self.device,
                    'training_enabled': self.training_enabled,
                    'gpu_temperature': self.gpu_temperature,
                    'gpu_utilization': self.gpu_utilization,
                    'memory_usage': self.memory_usage,
                    'buffer_size': len(self.training_buffer)
                },
                'registered_models': len(self.model_statuses),
                'active_models': sum(1 for s in self.model_statuses.values() if s.is_active),
                'model_statuses': {
                    k: {
                        'name': v.model_name,
                        'confidence': v.confidence,
                        'prediction': v.prediction,
                        'performance_score': v.performance_score,
                        'is_active': v.is_active,
                        'last_update': v.last_update.isoformat()
                    } for k, v in self.model_statuses.items()
                },
                'performance_metrics': self.performance_metrics,
                'recent_decisions': len(self.decision_history),
                'model_weights': self.model_weights
            }
        except Exception as e:
            logger.error(f"âŒ çŠ¶æ€æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def save_model(self, filepath: str) -> bool:
        """ä¿å­˜æ¨¡å‹"""
        try:
            torch.save({
                'model_state_dict': self.meta_network.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'performance_metrics': self.performance_metrics,
                'model_weights': self.model_weights
            }, filepath)
            logger.info(f"ğŸ’¾ å…ƒå­¦ä¹ æ¨¡å‹å·²ä¿å­˜: {filepath}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def load_model(self, filepath: str) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            checkpoint = torch.load(filepath, map_location=self.device)
            self.meta_network.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            self.performance_metrics = checkpoint.get('performance_metrics', self.performance_metrics)
            self.model_weights = checkpoint.get('model_weights', self.model_weights)
            logger.info(f"ğŸ“‚ å…ƒå­¦ä¹ æ¨¡å‹å·²åŠ è½½: {filepath}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

# å…¨å±€å®ä¾‹
meta_learning_commander = MetaLearningCommander()

def initialize_meta_learning_commander(device: str = None) -> MetaLearningCommander:
    """åˆå§‹åŒ–å…ƒå­¦ä¹ AIæŒ‡æŒ¥å®˜"""
    global meta_learning_commander
    if device:
        meta_learning_commander = MetaLearningCommander(device)
    return meta_learning_commander

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_meta_learning():
        commander = initialize_meta_learning_commander()
        
        # æ³¨å†Œæµ‹è¯•æ¨¡å‹
        commander.register_ai_model("test_model_1", "æµ‹è¯•æ¨¡å‹1")
        commander.register_ai_model("test_model_2", "æµ‹è¯•æ¨¡å‹2")
        
        # æ›´æ–°æ¨¡å‹çŠ¶æ€
        commander.update_model_status("test_model_1", 0.5, 0.8, 0.7)
        commander.update_model_status("test_model_2", -0.3, 0.6, 0.6)
        
        # åˆ¶ä½œå†³ç­–
        market_data = {
            'price_change': 0.02,
            'volume_ratio': 1.5,
            'volatility': 0.15,
            'rsi': 65.0,
            'macd_signal': 0.1
        }
        
        decision = await commander.make_meta_decision(market_data)
        print(f"å†³ç­–ç»“æœ: {decision}")
        
        # çŠ¶æ€æŠ¥å‘Š
        report = commander.get_status_report()
        print(f"çŠ¶æ€æŠ¥å‘Š: {json.dumps(report, indent=2, ensure_ascii=False)}")
    
    asyncio.run(test_meta_learning())

