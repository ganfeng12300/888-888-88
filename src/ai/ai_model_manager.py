#!/usr/bin/env python3
"""
ğŸ¤– 888-888-88 AIæ¨¡å‹ç®¡ç†å™¨
Production-Grade AI Model Management System
"""

import os
import sys
import asyncio
import json
import pickle
import hashlib
import threading
from typing import Dict, List, Any, Optional, Callable, Union, Type
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import numpy as np
import pandas as pd
from loguru import logger

# å¯¼å…¥é”™è¯¯å¤„ç†ç³»ç»Ÿ
from src.core.error_handling_system import (
    handle_errors, ai_operation, critical_section,
    ErrorCategory, ErrorSeverity, error_handler
)

class ModelType(Enum):
    """AIæ¨¡å‹ç±»å‹"""
    LSTM = "lstm"
    TRANSFORMER = "transformer"
    CNN = "cnn"
    RANDOM_FOREST = "random_forest"
    XGB = "xgboost"
    LIGHTGBM = "lightgbm"
    ENSEMBLE = "ensemble"
    REINFORCEMENT = "reinforcement"

class ModelStatus(Enum):
    """æ¨¡å‹çŠ¶æ€"""
    LOADING = "loading"
    READY = "ready"
    TRAINING = "training"
    UPDATING = "updating"
    ERROR = "error"
    DEPRECATED = "deprecated"

class ModelPriority(Enum):
    """æ¨¡å‹ä¼˜å…ˆçº§"""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4

@dataclass
class ModelMetadata:
    """æ¨¡å‹å…ƒæ•°æ®"""
    model_id: str
    name: str
    version: str
    model_type: ModelType
    status: ModelStatus
    priority: ModelPriority
    created_at: datetime
    updated_at: datetime
    file_path: str
    file_size: int
    checksum: str
    accuracy: float
    training_data_size: int
    features: List[str]
    target: str
    hyperparameters: Dict[str, Any]
    performance_metrics: Dict[str, float]
    dependencies: List[str]
    description: str

@dataclass
class PredictionRequest:
    """é¢„æµ‹è¯·æ±‚"""
    request_id: str
    model_id: str
    features: Dict[str, Any]
    timestamp: datetime
    priority: ModelPriority = ModelPriority.MEDIUM

@dataclass
class PredictionResult:
    """é¢„æµ‹ç»“æœ"""
    request_id: str
    model_id: str
    prediction: Any
    confidence: float
    processing_time_ms: float
    timestamp: datetime
    features_used: List[str]
    model_version: str

class BaseAIModel:
    """AIæ¨¡å‹åŸºç±»"""
    
    def __init__(self, model_id: str, metadata: ModelMetadata):
        self.model_id = model_id
        self.metadata = metadata
        self.model = None
        self.is_loaded = False
        self.last_used = datetime.now()
        self.prediction_count = 0
        self.error_count = 0
    
    @ai_operation
    async def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            logger.info(f"ğŸ¤– åŠ è½½æ¨¡å‹: {self.model_id}")
            self.metadata.status = ModelStatus.LOADING
            
            # éªŒè¯æ¨¡å‹æ–‡ä»¶
            if not Path(self.metadata.file_path).exists():
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.metadata.file_path}")
            
            # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
            if not self._verify_checksum():
                raise ValueError(f"æ¨¡å‹æ–‡ä»¶æ ¡éªŒå¤±è´¥: {self.model_id}")
            
            # åŠ è½½æ¨¡å‹ï¼ˆå­ç±»å®ç°ï¼‰
            await self._load_model_impl()
            
            self.is_loaded = True
            self.metadata.status = ModelStatus.READY
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_id}")
            
        except Exception as e:
            self.metadata.status = ModelStatus.ERROR
            self.error_count += 1
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ {self.model_id}: {e}")
            raise
    
    async def _load_model_impl(self):
        """åŠ è½½æ¨¡å‹å®ç°ï¼ˆå­ç±»é‡å†™ï¼‰"""
        raise NotImplementedError
    
    @ai_operation
    async def predict(self, features: Dict[str, Any]) -> PredictionResult:
        """é¢„æµ‹"""
        if not self.is_loaded:
            await self.load_model()
        
        start_time = datetime.now()
        
        try:
            # é¢„å¤„ç†ç‰¹å¾
            processed_features = await self._preprocess_features(features)
            
            # æ‰§è¡Œé¢„æµ‹
            prediction, confidence = await self._predict_impl(processed_features)
            
            # åå¤„ç†ç»“æœ
            final_prediction = await self._postprocess_prediction(prediction)
            
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            self.last_used = datetime.now()
            self.prediction_count += 1
            
            return PredictionResult(
                request_id=f"pred_{int(start_time.timestamp())}_{self.model_id}",
                model_id=self.model_id,
                prediction=final_prediction,
                confidence=confidence,
                processing_time_ms=processing_time,
                timestamp=start_time,
                features_used=list(processed_features.keys()),
                model_version=self.metadata.version
            )
            
        except Exception as e:
            self.error_count += 1
            logger.error(f"âŒ é¢„æµ‹å¤±è´¥ {self.model_id}: {e}")
            raise
    
    async def _preprocess_features(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """é¢„å¤„ç†ç‰¹å¾ï¼ˆå­ç±»å¯é‡å†™ï¼‰"""
        return features
    
    async def _predict_impl(self, features: Dict[str, Any]) -> tuple[Any, float]:
        """é¢„æµ‹å®ç°ï¼ˆå­ç±»é‡å†™ï¼‰"""
        raise NotImplementedError
    
    async def _postprocess_prediction(self, prediction: Any) -> Any:
        """åå¤„ç†é¢„æµ‹ç»“æœï¼ˆå­ç±»å¯é‡å†™ï¼‰"""
        return prediction
    
    def _verify_checksum(self) -> bool:
        """éªŒè¯æ–‡ä»¶æ ¡éªŒå’Œ"""
        try:
            with open(self.metadata.file_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
            return file_hash == self.metadata.checksum
        except Exception as e:
            logger.error(f"âŒ æ ¡éªŒå’ŒéªŒè¯å¤±è´¥: {e}")
            return False
    
    @ai_operation
    async def unload_model(self):
        """å¸è½½æ¨¡å‹"""
        try:
            self.model = None
            self.is_loaded = False
            self.metadata.status = ModelStatus.DEPRECATED
            logger.info(f"ğŸ—‘ï¸ æ¨¡å‹å·²å¸è½½: {self.model_id}")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹å¸è½½å¤±è´¥ {self.model_id}: {e}")
            raise
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "model_id": self.model_id,
            "status": self.metadata.status.value,
            "is_loaded": self.is_loaded,
            "prediction_count": self.prediction_count,
            "error_count": self.error_count,
            "last_used": self.last_used.isoformat(),
            "accuracy": self.metadata.accuracy,
            "error_rate": self.error_count / max(1, self.prediction_count)
        }

class LSTMModel(BaseAIModel):
    """LSTMæ¨¡å‹å®ç°"""
    
    async def _load_model_impl(self):
        """åŠ è½½LSTMæ¨¡å‹"""
        try:
            import tensorflow as tf
            self.model = tf.keras.models.load_model(self.metadata.file_path)
            logger.info(f"ğŸ“ˆ LSTMæ¨¡å‹åŠ è½½å®Œæˆ: {self.model_id}")
        except ImportError:
            logger.warning("âš ï¸ TensorFlowæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹ŸLSTMæ¨¡å‹")
            self.model = {"type": "lstm_mock", "weights": np.random.random((10, 10))}
    
    async def _predict_impl(self, features: Dict[str, Any]) -> tuple[Any, float]:
        """LSTMé¢„æµ‹å®ç°"""
        try:
            # è½¬æ¢ç‰¹å¾ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
            feature_array = np.array([list(features.values())]).reshape(1, -1, 1)
            
            if hasattr(self.model, 'predict'):
                # çœŸå®TensorFlowæ¨¡å‹
                prediction = self.model.predict(feature_array, verbose=0)
                confidence = float(np.max(prediction))
                return float(prediction[0][0]), confidence
            else:
                # æ¨¡æ‹Ÿé¢„æµ‹
                prediction = np.random.random()
                confidence = np.random.uniform(0.7, 0.95)
                return prediction, confidence
                
        except Exception as e:
            logger.error(f"âŒ LSTMé¢„æµ‹é”™è¯¯: {e}")
            raise

class TransformerModel(BaseAIModel):
    """Transformeræ¨¡å‹å®ç°"""
    
    async def _load_model_impl(self):
        """åŠ è½½Transformeræ¨¡å‹"""
        try:
            import torch
            self.model = torch.load(self.metadata.file_path, map_location='cpu')
            logger.info(f"ğŸ”„ Transformeræ¨¡å‹åŠ è½½å®Œæˆ: {self.model_id}")
        except ImportError:
            logger.warning("âš ï¸ PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹ŸTransformeræ¨¡å‹")
            self.model = {"type": "transformer_mock", "attention": np.random.random((8, 64, 64))}
    
    async def _predict_impl(self, features: Dict[str, Any]) -> tuple[Any, float]:
        """Transformeré¢„æµ‹å®ç°"""
        try:
            # æ¨¡æ‹ŸTransformeré¢„æµ‹
            prediction = np.random.uniform(-0.1, 0.1)  # ä»·æ ¼å˜åŒ–é¢„æµ‹
            confidence = np.random.uniform(0.6, 0.9)
            return prediction, confidence
        except Exception as e:
            logger.error(f"âŒ Transformeré¢„æµ‹é”™è¯¯: {e}")
            raise

class XGBoostModel(BaseAIModel):
    """XGBoostæ¨¡å‹å®ç°"""
    
    async def _load_model_impl(self):
        """åŠ è½½XGBoostæ¨¡å‹"""
        try:
            import xgboost as xgb
            self.model = xgb.Booster()
            self.model.load_model(self.metadata.file_path)
            logger.info(f"ğŸŒ³ XGBoostæ¨¡å‹åŠ è½½å®Œæˆ: {self.model_id}")
        except ImportError:
            logger.warning("âš ï¸ XGBoostæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹ŸXGBoostæ¨¡å‹")
            self.model = {"type": "xgb_mock", "trees": 100}
    
    async def _predict_impl(self, features: Dict[str, Any]) -> tuple[Any, float]:
        """XGBoosté¢„æµ‹å®ç°"""
        try:
            if hasattr(self.model, 'predict'):
                # çœŸå®XGBoostæ¨¡å‹
                import xgboost as xgb
                feature_array = np.array([list(features.values())])
                dtest = xgb.DMatrix(feature_array)
                prediction = self.model.predict(dtest)[0]
                confidence = min(0.95, abs(prediction) + 0.5)
                return float(prediction), confidence
            else:
                # æ¨¡æ‹Ÿé¢„æµ‹
                prediction = np.random.uniform(-0.05, 0.05)
                confidence = np.random.uniform(0.8, 0.95)
                return prediction, confidence
        except Exception as e:
            logger.error(f"âŒ XGBoosté¢„æµ‹é”™è¯¯: {e}")
            raise

class ModelFactory:
    """æ¨¡å‹å·¥å‚"""
    
    _model_classes = {
        ModelType.LSTM: LSTMModel,
        ModelType.TRANSFORMER: TransformerModel,
        ModelType.XGB: XGBoostModel,
        # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–æ¨¡å‹ç±»å‹
    }
    
    @classmethod
    def create_model(cls, metadata: ModelMetadata) -> BaseAIModel:
        """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
        model_class = cls._model_classes.get(metadata.model_type)
        if not model_class:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {metadata.model_type}")
        
        return model_class(metadata.model_id, metadata)

class AIModelManager:
    """AIæ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, models_dir: str = "models", max_loaded_models: int = 10):
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        self.max_loaded_models = max_loaded_models
        self.models: Dict[str, BaseAIModel] = {}
        self.model_metadata: Dict[str, ModelMetadata] = {}
        self.prediction_queue = asyncio.Queue()
        self.training_jobs: Dict[str, Dict[str, Any]] = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_predictions = 0
        self.total_errors = 0
        self.average_latency = 0.0
        
        # å¯åŠ¨åå°ä»»åŠ¡
        self._background_tasks = []
        
        logger.info(f"ğŸ¤– AIæ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹ç›®å½•: {self.models_dir}")
    
    @critical_section
    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨"""
        try:
            # æ‰«ææ¨¡å‹ç›®å½•
            await self._scan_models_directory()
            
            # å¯åŠ¨åå°ä»»åŠ¡
            self._background_tasks.append(
                asyncio.create_task(self._model_cleanup_task())
            )
            self._background_tasks.append(
                asyncio.create_task(self._prediction_processor())
            )
            
            logger.info(f"âœ… AIæ¨¡å‹ç®¡ç†å™¨å¯åŠ¨å®Œæˆ - å‘ç° {len(self.model_metadata)} ä¸ªæ¨¡å‹")
            
        except Exception as e:
            logger.error(f"âŒ AIæ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _scan_models_directory(self):
        """æ‰«ææ¨¡å‹ç›®å½•"""
        try:
            metadata_files = list(self.models_dir.glob("*.metadata.json"))
            
            for metadata_file in metadata_files:
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata_dict = json.load(f)
                    
                    # è½¬æ¢ä¸ºModelMetadataå¯¹è±¡
                    metadata_dict['created_at'] = datetime.fromisoformat(metadata_dict['created_at'])
                    metadata_dict['updated_at'] = datetime.fromisoformat(metadata_dict['updated_at'])
                    metadata_dict['model_type'] = ModelType(metadata_dict['model_type'])
                    metadata_dict['status'] = ModelStatus(metadata_dict['status'])
                    metadata_dict['priority'] = ModelPriority(metadata_dict['priority'])
                    
                    metadata = ModelMetadata(**metadata_dict)
                    self.model_metadata[metadata.model_id] = metadata
                    
                    logger.info(f"ğŸ“‹ å‘ç°æ¨¡å‹: {metadata.model_id} ({metadata.model_type.value})")
                    
                except Exception as e:
                    logger.error(f"âŒ è§£ææ¨¡å‹å…ƒæ•°æ®å¤±è´¥ {metadata_file}: {e}")
                    
        except Exception as e:
            logger.error(f"âŒ æ‰«ææ¨¡å‹ç›®å½•å¤±è´¥: {e}")
    
    @ai_operation
    async def load_model(self, model_id: str) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            if model_id in self.models and self.models[model_id].is_loaded:
                logger.info(f"â„¹ï¸ æ¨¡å‹å·²åŠ è½½: {model_id}")
                return True
            
            if model_id not in self.model_metadata:
                raise ValueError(f"æ¨¡å‹ä¸å­˜åœ¨: {model_id}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¸è½½å…¶ä»–æ¨¡å‹
            await self._manage_model_memory()
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            metadata = self.model_metadata[model_id]
            model = ModelFactory.create_model(metadata)
            
            # åŠ è½½æ¨¡å‹
            await model.load_model()
            
            self.models[model_id] = model
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_id}: {e}")
            return False
    
    async def _manage_model_memory(self):
        """ç®¡ç†æ¨¡å‹å†…å­˜"""
        loaded_models = [m for m in self.models.values() if m.is_loaded]
        
        if len(loaded_models) >= self.max_loaded_models:
            # æŒ‰æœ€åä½¿ç”¨æ—¶é—´æ’åºï¼Œå¸è½½æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹
            loaded_models.sort(key=lambda m: m.last_used)
            
            models_to_unload = loaded_models[:len(loaded_models) - self.max_loaded_models + 1]
            
            for model in models_to_unload:
                if model.metadata.priority != ModelPriority.CRITICAL:
                    await model.unload_model()
                    logger.info(f"ğŸ—‘ï¸ å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜: {model.model_id}")
    
    @ai_operation
    async def predict(self, model_id: str, features: Dict[str, Any], 
                     priority: ModelPriority = ModelPriority.MEDIUM) -> Optional[PredictionResult]:
        """æ‰§è¡Œé¢„æµ‹"""
        try:
            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            if not await self.load_model(model_id):
                raise ValueError(f"æ— æ³•åŠ è½½æ¨¡å‹: {model_id}")
            
            model = self.models[model_id]
            
            # åˆ›å»ºé¢„æµ‹è¯·æ±‚
            request = PredictionRequest(
                request_id=f"req_{int(datetime.now().timestamp())}_{model_id}",
                model_id=model_id,
                features=features,
                timestamp=datetime.now(),
                priority=priority
            )
            
            # æ‰§è¡Œé¢„æµ‹
            result = await model.predict(features)
            
            # æ›´æ–°ç»Ÿè®¡
            self.total_predictions += 1
            self.average_latency = (
                (self.average_latency * (self.total_predictions - 1) + result.processing_time_ms) 
                / self.total_predictions
            )
            
            return result
            
        except Exception as e:
            self.total_errors += 1
            logger.error(f"âŒ é¢„æµ‹å¤±è´¥ {model_id}: {e}")
            return None
    
    @ai_operation
    async def batch_predict(self, model_id: str, features_list: List[Dict[str, Any]]) -> List[Optional[PredictionResult]]:
        """æ‰¹é‡é¢„æµ‹"""
        results = []
        
        for features in features_list:
            result = await self.predict(model_id, features)
            results.append(result)
        
        return results
    
    def get_active_models(self) -> List[str]:
        """è·å–æ´»è·ƒæ¨¡å‹åˆ—è¡¨"""
        return [model_id for model_id, model in self.models.items() if model.is_loaded]
    
    def get_training_jobs(self) -> List[Dict[str, Any]]:
        """è·å–è®­ç»ƒä»»åŠ¡åˆ—è¡¨"""
        return list(self.training_jobs.values())
    
    def get_model_stats(self, model_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        if model_id in self.models:
            return self.models[model_id].get_stats()
        return None
    
    def get_manager_stats(self) -> Dict[str, Any]:
        """è·å–ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        loaded_count = len([m for m in self.models.values() if m.is_loaded])
        
        return {
            "total_models": len(self.model_metadata),
            "loaded_models": loaded_count,
            "total_predictions": self.total_predictions,
            "total_errors": self.total_errors,
            "error_rate": self.total_errors / max(1, self.total_predictions),
            "average_latency_ms": self.average_latency,
            "training_jobs": len(self.training_jobs)
        }
    
    async def _model_cleanup_task(self):
        """æ¨¡å‹æ¸…ç†ä»»åŠ¡"""
        while True:
            try:
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
                
                current_time = datetime.now()
                
                for model_id, model in list(self.models.items()):
                    # å¸è½½é•¿æ—¶é—´æœªä½¿ç”¨çš„ä½ä¼˜å…ˆçº§æ¨¡å‹
                    if (model.is_loaded and 
                        model.metadata.priority == ModelPriority.LOW and
                        (current_time - model.last_used).seconds > 1800):  # 30åˆ†é’Ÿ
                        
                        await model.unload_model()
                        logger.info(f"ğŸ§¹ è‡ªåŠ¨å¸è½½é•¿æ—¶é—´æœªä½¿ç”¨çš„æ¨¡å‹: {model_id}")
                
            except Exception as e:
                logger.error(f"âŒ æ¨¡å‹æ¸…ç†ä»»åŠ¡é”™è¯¯: {e}")
    
    async def _prediction_processor(self):
        """é¢„æµ‹å¤„ç†å™¨"""
        while True:
            try:
                # è¿™é‡Œå¯ä»¥å®ç°é¢„æµ‹é˜Ÿåˆ—å¤„ç†é€»è¾‘
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ é¢„æµ‹å¤„ç†å™¨é”™è¯¯: {e}")
    
    async def shutdown(self):
        """å…³é—­æ¨¡å‹ç®¡ç†å™¨"""
        try:
            # å–æ¶ˆåå°ä»»åŠ¡
            for task in self._background_tasks:
                task.cancel()
            
            # å¸è½½æ‰€æœ‰æ¨¡å‹
            for model in self.models.values():
                if model.is_loaded:
                    await model.unload_model()
            
            logger.info("ğŸ›‘ AIæ¨¡å‹ç®¡ç†å™¨å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"âŒ å…³é—­æ¨¡å‹ç®¡ç†å™¨å¤±è´¥: {e}")

# å…¨å±€AIæ¨¡å‹ç®¡ç†å™¨å®ä¾‹
ai_model_manager = AIModelManager()

# å¯¼å‡ºä¸»è¦ç»„ä»¶
__all__ = [
    'AIModelManager',
    'BaseAIModel',
    'ModelType',
    'ModelStatus',
    'ModelPriority',
    'ModelMetadata',
    'PredictionRequest',
    'PredictionResult',
    'LSTMModel',
    'TransformerModel',
    'XGBoostModel',
    'ai_model_manager'
]
