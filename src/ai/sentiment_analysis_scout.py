#!/usr/bin/env python3
"""
ğŸ•µï¸ æƒ…æ„Ÿåˆ†æä¾¦å¯Ÿå…µ - å¸‚åœºæƒ…ç»ªç›‘æ§
å¤šæºæƒ…æ„Ÿæ•°æ®åˆ†æå’Œå¸‚åœºæƒ…ç»ªé¢„æµ‹
ä¸“ä¸ºç”Ÿäº§çº§å®ç›˜äº¤æ˜“è®¾è®¡ï¼Œæ”¯æŒå®æ—¶æƒ…æ„Ÿç›‘æ§
"""

import asyncio
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, timezone, timedelta
import json
from dataclasses import dataclass
from loguru import logger
import threading
import time
from concurrent.futures import ThreadPoolExecutor
import pandas as pd
import re
import requests
from textblob import TextBlob
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import yfinance as yf
import tweepy
import feedparser
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import warnings
warnings.filterwarnings('ignore')

@dataclass
class SentimentData:
    """æƒ…æ„Ÿæ•°æ®"""
    source: str
    content: str
    sentiment_score: float  # -1åˆ°1ä¹‹é—´
    confidence: float      # 0åˆ°1ä¹‹é—´
    timestamp: datetime
    metadata: Dict[str, Any]

@dataclass
class MarketSentiment:
    """å¸‚åœºæƒ…æ„Ÿåˆ†æç»“æœ"""
    overall_sentiment: float     # ç»¼åˆæƒ…æ„Ÿåˆ†æ•°
    sentiment_strength: float    # æƒ…æ„Ÿå¼ºåº¦
    sentiment_trend: str         # 'bullish', 'bearish', 'neutral'
    confidence_level: float      # ç½®ä¿¡åº¦
    source_breakdown: Dict[str, float]  # å„æ¥æºæƒ…æ„Ÿåˆ†æ•°
    key_topics: List[str]        # å…³é”®è¯é¢˜
    sentiment_volatility: float  # æƒ…æ„Ÿæ³¢åŠ¨æ€§
    news_impact_score: float     # æ–°é—»å½±å“åˆ†æ•°
    social_buzz_level: float     # ç¤¾äº¤åª’ä½“çƒ­åº¦
    fear_greed_index: float      # ææƒ§è´ªå©ªæŒ‡æ•°
    timestamp: datetime

class SentimentTransformer(nn.Module):
    """æƒ…æ„Ÿåˆ†æTransformeræ¨¡å‹"""
    
    def __init__(self, vocab_size: int = 10000, embed_dim: int = 256, 
                 num_heads: int = 8, num_layers: int = 4):
        super().__init__()
        self.embed_dim = embed_dim
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_encoding = nn.Parameter(torch.randn(1000, embed_dim))
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=embed_dim * 4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(embed_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 3)  # è´Ÿé¢ã€ä¸­æ€§ã€æ­£é¢
        )
        
        # æƒ…æ„Ÿå¼ºåº¦é¢„æµ‹
        self.intensity_head = nn.Sequential(
            nn.Linear(embed_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        seq_len = x.size(1)
        
        # è¯åµŒå…¥å’Œä½ç½®ç¼–ç 
        embedded = self.embedding(x)
        embedded += self.pos_encoding[:seq_len, :].unsqueeze(0)
        
        # Transformerç¼–ç 
        encoded = self.transformer(embedded)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        pooled = torch.mean(encoded, dim=1)
        
        # åˆ†ç±»å’Œå¼ºåº¦é¢„æµ‹
        sentiment_logits = self.classifier(pooled)
        intensity = self.intensity_head(pooled)
        
        return sentiment_logits, intensity

class SentimentAnalysisScout:
    """æƒ…æ„Ÿåˆ†æä¾¦å¯Ÿå…µ"""
    
    def __init__(self, device: str = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.sentiment_model = SentimentTransformer().to(self.device)
        self.optimizer = optim.AdamW(self.sentiment_model.parameters(), lr=1e-4)
        
        # é¢„è®­ç»ƒæ¨¡å‹
        try:
            self.bert_analyzer = pipeline(
                "sentiment-analysis",
                model="nlptown/bert-base-multilingual-uncased-sentiment",
                device=0 if torch.cuda.is_available() else -1
            )
        except:
            logger.warning("âš ï¸ BERTæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
            self.bert_analyzer = None
        
        # NLTKæƒ…æ„Ÿåˆ†æå™¨
        try:
            nltk.download('vader_lexicon', quiet=True)
            self.vader_analyzer = SentimentIntensityAnalyzer()
        except:
            logger.warning("âš ï¸ VADERåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥")
            self.vader_analyzer = None
        
        # æ•°æ®æºé…ç½®
        self.data_sources = {
            'news': {
                'enabled': True,
                'weight': 0.3,
                'urls': [
                    'https://feeds.finance.yahoo.com/rss/2.0/headline',
                    'https://www.coindesk.com/arc/outboundfeeds/rss/',
                    'https://cointelegraph.com/rss'
                ]
            },
            'social': {
                'enabled': True,
                'weight': 0.2,
                'platforms': ['twitter', 'reddit']
            },
            'market_data': {
                'enabled': True,
                'weight': 0.3,
                'indicators': ['vix', 'put_call_ratio', 'margin_debt']
            },
            'technical': {
                'enabled': True,
                'weight': 0.2,
                'indicators': ['rsi', 'macd', 'bb_position']
            }
        }
        
        # æƒ…æ„Ÿå†å²æ•°æ®
        self.sentiment_history = []
        self.max_history = 1000
        
        # å…³é”®è¯åº“
        self.bullish_keywords = [
            'bull', 'bullish', 'moon', 'pump', 'rally', 'surge', 'breakout',
            'breakthrough', 'adoption', 'institutional', 'positive', 'optimistic',
            'growth', 'expansion', 'partnership', 'upgrade', 'innovation'
        ]
        
        self.bearish_keywords = [
            'bear', 'bearish', 'crash', 'dump', 'correction', 'decline',
            'selloff', 'panic', 'fear', 'uncertainty', 'regulation', 'ban',
            'hack', 'scam', 'bubble', 'overvalued', 'risk', 'concern'
        ]
        
        # ç¼“å­˜
        self.sentiment_cache = {}
        self.cache_duration = 300  # 5åˆ†é’Ÿç¼“å­˜
        
        # å®æ—¶çŠ¶æ€
        self.last_sentiment = None
        self.last_confidence = 0.0
        self.performance_score = 0.5
        
        logger.info("ğŸ•µï¸ æƒ…æ„Ÿåˆ†æä¾¦å¯Ÿå…µåˆå§‹åŒ–å®Œæˆ")
    
    async def analyze_market_sentiment(self, symbol: str = "BTC", 
                                     timeframe: str = "1h") -> MarketSentiment:
        """åˆ†æå¸‚åœºæƒ…æ„Ÿ"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"{symbol}_{timeframe}_{int(time.time() // self.cache_duration)}"
            if cache_key in self.sentiment_cache:
                return self.sentiment_cache[cache_key]
            
            # å¹¶è¡Œæ”¶é›†å„æºæ•°æ®
            tasks = []
            
            if self.data_sources['news']['enabled']:
                tasks.append(self._collect_news_sentiment(symbol))
            
            if self.data_sources['social']['enabled']:
                tasks.append(self._collect_social_sentiment(symbol))
            
            if self.data_sources['market_data']['enabled']:
                tasks.append(self._collect_market_sentiment(symbol))
            
            if self.data_sources['technical']['enabled']:
                tasks.append(self._collect_technical_sentiment(symbol))
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            sentiment_data = []
            for result in results:
                if not isinstance(result, Exception) and result:
                    sentiment_data.extend(result)
            
            if not sentiment_data:
                return self._create_neutral_sentiment()
            
            # è®¡ç®—ç»¼åˆæƒ…æ„Ÿ
            market_sentiment = self._calculate_market_sentiment(sentiment_data, symbol)
            
            # ç¼“å­˜ç»“æœ
            self.sentiment_cache[cache_key] = market_sentiment
            
            # æ¸…ç†æ—§ç¼“å­˜
            self._cleanup_cache()
            
            # æ›´æ–°å†å²
            self.sentiment_history.append({
                'timestamp': market_sentiment.timestamp,
                'sentiment': market_sentiment.overall_sentiment,
                'confidence': market_sentiment.confidence_level,
                'symbol': symbol
            })
            
            if len(self.sentiment_history) > self.max_history:
                self.sentiment_history = self.sentiment_history[-self.max_history:]
            
            # æ›´æ–°çŠ¶æ€
            self.last_sentiment = market_sentiment
            self.last_confidence = market_sentiment.confidence_level
            
            logger.info(f"ğŸ•µï¸ å¸‚åœºæƒ…æ„Ÿåˆ†æå®Œæˆ - {symbol}: {market_sentiment.overall_sentiment:.3f}")
            return market_sentiment
            
        except Exception as e:
            logger.error(f"âŒ å¸‚åœºæƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return self._create_neutral_sentiment()
    
    async def _collect_news_sentiment(self, symbol: str) -> List[SentimentData]:
        """æ”¶é›†æ–°é—»æƒ…æ„Ÿ"""
        try:
            sentiment_data = []
            
            for url in self.data_sources['news']['urls']:
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(url, timeout=10) as response:
                            if response.status == 200:
                                content = await response.text()
                                feed = feedparser.parse(content)
                                
                                for entry in feed.entries[:10]:  # æœ€æ–°10æ¡
                                    title = entry.get('title', '')
                                    summary = entry.get('summary', '')
                                    text = f"{title} {summary}"
                                    
                                    # æ£€æŸ¥æ˜¯å¦ä¸ç›®æ ‡èµ„äº§ç›¸å…³
                                    if self._is_relevant(text, symbol):
                                        sentiment_score = await self._analyze_text_sentiment(text)
                                        
                                        sentiment_data.append(SentimentData(
                                            source='news',
                                            content=text[:200],
                                            sentiment_score=sentiment_score,
                                            confidence=0.8,
                                            timestamp=datetime.now(timezone.utc),
                                            metadata={'url': url, 'title': title}
                                        ))
                                
                except Exception as e:
                    logger.warning(f"âš ï¸ æ–°é—»æºè·å–å¤±è´¥ {url}: {e}")
                    continue
            
            return sentiment_data
            
        except Exception as e:
            logger.error(f"âŒ æ–°é—»æƒ…æ„Ÿæ”¶é›†å¤±è´¥: {e}")
            return []
    
    async def _collect_social_sentiment(self, symbol: str) -> List[SentimentData]:
        """æ”¶é›†ç¤¾äº¤åª’ä½“æƒ…æ„Ÿ"""
        try:
            sentiment_data = []
            
            # æ¨¡æ‹Ÿç¤¾äº¤åª’ä½“æ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦APIå¯†é’¥ï¼‰
            social_texts = [
                f"{symbol} looking bullish today! ğŸš€",
                f"Bearish on {symbol}, expecting correction",
                f"{symbol} breaking resistance, moon time!",
                f"Worried about {symbol} regulation news",
                f"{symbol} adoption growing, very optimistic"
            ]
            
            for text in social_texts:
                sentiment_score = await self._analyze_text_sentiment(text)
                
                sentiment_data.append(SentimentData(
                    source='social',
                    content=text,
                    sentiment_score=sentiment_score,
                    confidence=0.6,
                    timestamp=datetime.now(timezone.utc),
                    metadata={'platform': 'twitter'}
                ))
            
            return sentiment_data
            
        except Exception as e:
            logger.error(f"âŒ ç¤¾äº¤åª’ä½“æƒ…æ„Ÿæ”¶é›†å¤±è´¥: {e}")
            return []
    
    async def _collect_market_sentiment(self, symbol: str) -> List[SentimentData]:
        """æ”¶é›†å¸‚åœºæ•°æ®æƒ…æ„Ÿ"""
        try:
            sentiment_data = []
            
            # VIXææ…ŒæŒ‡æ•°ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
            vix_value = np.random.uniform(15, 35)  # å®é™…åº”ä»APIè·å–
            vix_sentiment = -((vix_value - 20) / 20)  # VIXè¶Šé«˜è¶Šææ…Œ
            vix_sentiment = max(-1, min(1, vix_sentiment))
            
            sentiment_data.append(SentimentData(
                source='market_data',
                content=f"VIXææ…ŒæŒ‡æ•°: {vix_value:.2f}",
                sentiment_score=vix_sentiment,
                confidence=0.9,
                timestamp=datetime.now(timezone.utc),
                metadata={'indicator': 'vix', 'value': vix_value}
            ))
            
            # çœ‹è·Œçœ‹æ¶¨æ¯”ç‡ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
            put_call_ratio = np.random.uniform(0.5, 1.5)
            pc_sentiment = -((put_call_ratio - 1.0) / 0.5)  # æ¯”ç‡è¶Šé«˜è¶Šçœ‹è·Œ
            pc_sentiment = max(-1, min(1, pc_sentiment))
            
            sentiment_data.append(SentimentData(
                source='market_data',
                content=f"çœ‹è·Œçœ‹æ¶¨æ¯”ç‡: {put_call_ratio:.2f}",
                sentiment_score=pc_sentiment,
                confidence=0.8,
                timestamp=datetime.now(timezone.utc),
                metadata={'indicator': 'put_call_ratio', 'value': put_call_ratio}
            ))
            
            return sentiment_data
            
        except Exception as e:
            logger.error(f"âŒ å¸‚åœºæ•°æ®æƒ…æ„Ÿæ”¶é›†å¤±è´¥: {e}")
            return []
    
    async def _collect_technical_sentiment(self, symbol: str) -> List[SentimentData]:
        """æ”¶é›†æŠ€æœ¯æŒ‡æ ‡æƒ…æ„Ÿ"""
        try:
            sentiment_data = []
            
            # æ¨¡æ‹ŸæŠ€æœ¯æŒ‡æ ‡æ•°æ®
            rsi = np.random.uniform(30, 70)
            rsi_sentiment = 0.0
            if rsi > 70:
                rsi_sentiment = -0.5  # è¶…ä¹°
            elif rsi < 30:
                rsi_sentiment = 0.5   # è¶…å–
            else:
                rsi_sentiment = (50 - rsi) / 50  # ä¸­æ€§åŒºåŸŸ
            
            sentiment_data.append(SentimentData(
                source='technical',
                content=f"RSIæŒ‡æ ‡: {rsi:.2f}",
                sentiment_score=rsi_sentiment,
                confidence=0.7,
                timestamp=datetime.now(timezone.utc),
                metadata={'indicator': 'rsi', 'value': rsi}
            ))
            
            # MACDæƒ…æ„Ÿ
            macd = np.random.uniform(-0.1, 0.1)
            macd_sentiment = macd * 10  # æ”¾å¤§ä¿¡å·
            macd_sentiment = max(-1, min(1, macd_sentiment))
            
            sentiment_data.append(SentimentData(
                source='technical',
                content=f"MACDæŒ‡æ ‡: {macd:.4f}",
                sentiment_score=macd_sentiment,
                confidence=0.7,
                timestamp=datetime.now(timezone.utc),
                metadata={'indicator': 'macd', 'value': macd}
            ))
            
            return sentiment_data
            
        except Exception as e:
            logger.error(f"âŒ æŠ€æœ¯æŒ‡æ ‡æƒ…æ„Ÿæ”¶é›†å¤±è´¥: {e}")
            return []
    
    async def _analyze_text_sentiment(self, text: str) -> float:
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        try:
            sentiments = []
            
            # BERTåˆ†æ
            if self.bert_analyzer:
                try:
                    result = self.bert_analyzer(text[:512])  # BERTé™åˆ¶é•¿åº¦
                    if result and len(result) > 0:
                        label = result[0]['label'].lower()
                        score = result[0]['score']
                        
                        if 'positive' in label or '5' in label or '4' in label:
                            sentiments.append(score)
                        elif 'negative' in label or '1' in label or '2' in label:
                            sentiments.append(-score)
                        else:
                            sentiments.append(0.0)
                except Exception as e:
                    logger.debug(f"BERTåˆ†æå¤±è´¥: {e}")
            
            # VADERåˆ†æ
            if self.vader_analyzer:
                try:
                    scores = self.vader_analyzer.polarity_scores(text)
                    compound_score = scores['compound']
                    sentiments.append(compound_score)
                except Exception as e:
                    logger.debug(f"VADERåˆ†æå¤±è´¥: {e}")
            
            # TextBlobåˆ†æ
            try:
                blob = TextBlob(text)
                polarity = blob.sentiment.polarity
                sentiments.append(polarity)
            except Exception as e:
                logger.debug(f"TextBlobåˆ†æå¤±è´¥: {e}")
            
            # å…³é”®è¯åˆ†æ
            keyword_sentiment = self._analyze_keywords(text)
            if keyword_sentiment != 0:
                sentiments.append(keyword_sentiment)
            
            # è®¡ç®—å¹³å‡æƒ…æ„Ÿ
            if sentiments:
                return np.mean(sentiments)
            else:
                return 0.0
                
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return 0.0
    
    def _analyze_keywords(self, text: str) -> float:
        """å…³é”®è¯æƒ…æ„Ÿåˆ†æ"""
        try:
            text_lower = text.lower()
            bullish_count = sum(1 for word in self.bullish_keywords if word in text_lower)
            bearish_count = sum(1 for word in self.bearish_keywords if word in text_lower)
            
            total_keywords = bullish_count + bearish_count
            if total_keywords == 0:
                return 0.0
            
            sentiment = (bullish_count - bearish_count) / total_keywords
            return sentiment
            
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯åˆ†æå¤±è´¥: {e}")
            return 0.0
    
    def _is_relevant(self, text: str, symbol: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ç›®æ ‡èµ„äº§ç›¸å…³"""
        try:
            text_lower = text.lower()
            symbol_lower = symbol.lower()
            
            # ç›´æ¥åŒ¹é…
            if symbol_lower in text_lower:
                return True
            
            # å¸¸è§åˆ«ååŒ¹é…
            aliases = {
                'btc': ['bitcoin', 'btc'],
                'eth': ['ethereum', 'eth'],
                'ada': ['cardano', 'ada'],
                'dot': ['polkadot', 'dot']
            }
            
            if symbol_lower in aliases:
                for alias in aliases[symbol_lower]:
                    if alias in text_lower:
                        return True
            
            # é€šç”¨åŠ å¯†è´§å¸å…³é”®è¯
            crypto_keywords = ['crypto', 'cryptocurrency', 'blockchain', 'defi', 'nft']
            if any(keyword in text_lower for keyword in crypto_keywords):
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ ç›¸å…³æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _calculate_market_sentiment(self, sentiment_data: List[SentimentData], 
                                  symbol: str) -> MarketSentiment:
        """è®¡ç®—å¸‚åœºæƒ…æ„Ÿ"""
        try:
            if not sentiment_data:
                return self._create_neutral_sentiment()
            
            # æŒ‰æ¥æºåˆ†ç»„
            source_sentiments = {}
            for data in sentiment_data:
                if data.source not in source_sentiments:
                    source_sentiments[data.source] = []
                source_sentiments[data.source].append(data.sentiment_score)
            
            # è®¡ç®—å„æ¥æºå¹³å‡æƒ…æ„Ÿ
            source_breakdown = {}
            weighted_sentiments = []
            
            for source, sentiments in source_sentiments.items():
                avg_sentiment = np.mean(sentiments)
                source_breakdown[source] = avg_sentiment
                
                # åº”ç”¨æƒé‡
                weight = self.data_sources.get(source, {}).get('weight', 0.25)
                weighted_sentiments.append(avg_sentiment * weight)
            
            # ç»¼åˆæƒ…æ„Ÿåˆ†æ•°
            overall_sentiment = sum(weighted_sentiments)
            overall_sentiment = max(-1, min(1, overall_sentiment))
            
            # æƒ…æ„Ÿå¼ºåº¦
            sentiment_strength = abs(overall_sentiment)
            
            # æƒ…æ„Ÿè¶‹åŠ¿
            if overall_sentiment > 0.2:
                sentiment_trend = 'bullish'
            elif overall_sentiment < -0.2:
                sentiment_trend = 'bearish'
            else:
                sentiment_trend = 'neutral'
            
            # ç½®ä¿¡åº¦ï¼ˆåŸºäºæ•°æ®é‡å’Œä¸€è‡´æ€§ï¼‰
            confidence_level = min(len(sentiment_data) / 20.0, 1.0)  # æ•°æ®é‡å› å­
            sentiment_std = np.std([d.sentiment_score for d in sentiment_data])
            consistency_factor = max(0, 1 - sentiment_std)  # ä¸€è‡´æ€§å› å­
            confidence_level = (confidence_level + consistency_factor) / 2
            
            # å…³é”®è¯é¢˜æå–
            key_topics = self._extract_key_topics(sentiment_data)
            
            # æƒ…æ„Ÿæ³¢åŠ¨æ€§
            sentiment_volatility = sentiment_std
            
            # æ–°é—»å½±å“åˆ†æ•°
            news_data = [d for d in sentiment_data if d.source == 'news']
            news_impact_score = abs(np.mean([d.sentiment_score for d in news_data])) if news_data else 0.0
            
            # ç¤¾äº¤åª’ä½“çƒ­åº¦
            social_data = [d for d in sentiment_data if d.source == 'social']
            social_buzz_level = len(social_data) / 10.0  # æ ‡å‡†åŒ–åˆ°0-1
            social_buzz_level = min(social_buzz_level, 1.0)
            
            # ææƒ§è´ªå©ªæŒ‡æ•°
            fear_greed_index = self._calculate_fear_greed_index(sentiment_data)
            
            return MarketSentiment(
                overall_sentiment=overall_sentiment,
                sentiment_strength=sentiment_strength,
                sentiment_trend=sentiment_trend,
                confidence_level=confidence_level,
                source_breakdown=source_breakdown,
                key_topics=key_topics,
                sentiment_volatility=sentiment_volatility,
                news_impact_score=news_impact_score,
                social_buzz_level=social_buzz_level,
                fear_greed_index=fear_greed_index,
                timestamp=datetime.now(timezone.utc)
            )
            
        except Exception as e:
            logger.error(f"âŒ å¸‚åœºæƒ…æ„Ÿè®¡ç®—å¤±è´¥: {e}")
            return self._create_neutral_sentiment()
    
    def _extract_key_topics(self, sentiment_data: List[SentimentData]) -> List[str]:
        """æå–å…³é”®è¯é¢˜"""
        try:
            # ç®€å•çš„å…³é”®è¯é¢‘ç‡åˆ†æ
            all_text = ' '.join([d.content for d in sentiment_data])
            words = re.findall(r'\b\w+\b', all_text.lower())
            
            # è¿‡æ»¤åœç”¨è¯
            stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}
            words = [w for w in words if w not in stop_words and len(w) > 3]
            
            # è®¡ç®—è¯é¢‘
            word_freq = {}
            for word in words:
                word_freq[word] = word_freq.get(word, 0) + 1
            
            # è¿”å›å‰5ä¸ªé«˜é¢‘è¯
            top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
            return [word for word, freq in top_words]
            
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯é¢˜æå–å¤±è´¥: {e}")
            return []
    
    def _calculate_fear_greed_index(self, sentiment_data: List[SentimentData]) -> float:
        """è®¡ç®—ææƒ§è´ªå©ªæŒ‡æ•°"""
        try:
            # åŸºäºæƒ…æ„Ÿåˆ†æ•°è®¡ç®—ææƒ§è´ªå©ªæŒ‡æ•°
            sentiments = [d.sentiment_score for d in sentiment_data]
            if not sentiments:
                return 50.0  # ä¸­æ€§
            
            avg_sentiment = np.mean(sentiments)
            
            # è½¬æ¢åˆ°0-100èŒƒå›´ï¼Œ50ä¸ºä¸­æ€§
            fear_greed = 50 + (avg_sentiment * 50)
            return max(0, min(100, fear_greed))
            
        except Exception as e:
            logger.error(f"âŒ ææƒ§è´ªå©ªæŒ‡æ•°è®¡ç®—å¤±è´¥: {e}")
            return 50.0
    
    def _create_neutral_sentiment(self) -> MarketSentiment:
        """åˆ›å»ºä¸­æ€§æƒ…æ„Ÿç»“æœ"""
        return MarketSentiment(
            overall_sentiment=0.0,
            sentiment_strength=0.0,
            sentiment_trend='neutral',
            confidence_level=0.1,
            source_breakdown={},
            key_topics=[],
            sentiment_volatility=0.0,
            news_impact_score=0.0,
            social_buzz_level=0.0,
            fear_greed_index=50.0,
            timestamp=datetime.now(timezone.utc)
        )
    
    def _cleanup_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        try:
            current_time = time.time()
            expired_keys = []
            
            for key in self.sentiment_cache:
                # ä»keyä¸­æå–æ—¶é—´æˆ³
                parts = key.split('_')
                if len(parts) >= 3:
                    try:
                        cache_time = int(parts[-1]) * self.cache_duration
                        if current_time - cache_time > self.cache_duration:
                            expired_keys.append(key)
                    except ValueError:
                        expired_keys.append(key)
            
            for key in expired_keys:
                del self.sentiment_cache[key]
                
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€"""
        try:
            return {
                'model_id': 'sentiment_analysis_scout',
                'model_name': 'æƒ…æ„Ÿåˆ†æä¾¦å¯Ÿå…µ',
                'device': self.device,
                'data_sources': self.data_sources,
                'sentiment_history_length': len(self.sentiment_history),
                'cache_size': len(self.sentiment_cache),
                'last_sentiment': self.last_sentiment.overall_sentiment if self.last_sentiment else 0.0,
                'last_confidence': self.last_confidence,
                'performance_score': self.performance_score,
                'bullish_keywords_count': len(self.bullish_keywords),
                'bearish_keywords_count': len(self.bearish_keywords)
            }
        except Exception as e:
            logger.error(f"âŒ çŠ¶æ€è·å–å¤±è´¥: {e}")
            return {'error': str(e)}

# å…¨å±€å®ä¾‹
sentiment_analysis_scout = SentimentAnalysisScout()

def initialize_sentiment_analysis_scout(device: str = None) -> SentimentAnalysisScout:
    """åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æä¾¦å¯Ÿå…µ"""
    global sentiment_analysis_scout
    sentiment_analysis_scout = SentimentAnalysisScout(device)
    return sentiment_analysis_scout

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_sentiment_analysis():
        scout = initialize_sentiment_analysis_scout()
        
        # æµ‹è¯•å¸‚åœºæƒ…æ„Ÿåˆ†æ
        sentiment = await scout.analyze_market_sentiment("BTC")
        print(f"å¸‚åœºæƒ…æ„Ÿåˆ†æ: {sentiment}")
        
        # çŠ¶æ€æŠ¥å‘Š
        status = scout.get_status()
        print(f"çŠ¶æ€æŠ¥å‘Š: {json.dumps(status, indent=2, ensure_ascii=False)}")
    
    asyncio.run(test_sentiment_analysis())
