#!/usr/bin/env python3
"""
GPUå†…å­˜ä¼˜åŒ–å™¨ - ç”Ÿäº§çº§RTX3060 12GBæ˜¾å­˜æ™ºèƒ½ç®¡ç†
å®ç°æ˜¾å­˜åŠ¨æ€åˆ†é…ã€å†…å­˜æ± ç®¡ç†ã€æ¨¡å‹è°ƒåº¦ä¼˜åŒ–
"""
import os
import sys
import time
import threading
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Tuple
import json
import numpy as np
from loguru import logger
import torch
import torch.cuda
import GPUtil
import psutil
from concurrent.futures import ThreadPoolExecutor

class ProductionGPUMemoryOptimizer:
    """ç”Ÿäº§çº§GPUå†…å­˜ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.device_count = torch.cuda.device_count() if self.gpu_available else 0
        self.current_device = 0
        self.memory_pools = {}
        self.allocated_memory = {}
        self.memory_history = []
        self.optimization_stats = {
            'total_allocations': 0,
            'memory_saved': 0,
            'fragmentation_reduced': 0,
            'optimization_count': 0
        }
        self.is_monitoring = False
        self.monitor_thread = None
        self.memory_threshold = 0.85  # 85%å†…å­˜ä½¿ç”¨é˜ˆå€¼
        
        if self.gpu_available:
            self._initialize_gpu_monitoring()
            logger.info(f"ğŸ® GPUå†…å­˜ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ - æ£€æµ‹åˆ° {self.device_count} ä¸ªGPUè®¾å¤‡")
        else:
            logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°GPUè®¾å¤‡ï¼Œä½¿ç”¨CPUæ¨¡å¼")
    
    def _initialize_gpu_monitoring(self):
        """åˆå§‹åŒ–GPUç›‘æ§"""
        try:
            for i in range(self.device_count):
                torch.cuda.set_device(i)
                # æ¸…ç†GPUç¼“å­˜
                torch.cuda.empty_cache()
                
                # è·å–GPUä¿¡æ¯
                gpu_props = torch.cuda.get_device_properties(i)
                total_memory = gpu_props.total_memory
                
                self.memory_pools[i] = {
                    'total_memory': total_memory,
                    'allocated_memory': 0,
                    'cached_memory': 0,
                    'free_memory': total_memory,
                    'memory_blocks': [],
                    'last_cleanup': time.time()
                }
                
                logger.info(f"GPU {i}: {gpu_props.name} - {total_memory / 1024**3:.1f}GB æ€»å†…å­˜")
                
        except Exception as e:
            logger.error(f"GPUç›‘æ§åˆå§‹åŒ–é”™è¯¯: {e}")
    
    def start_memory_monitoring(self):
        """å¯åŠ¨å†…å­˜ç›‘æ§"""
        if not self.gpu_available:
            logger.warning("GPUä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜ç›‘æ§")
            return
        
        if self.is_monitoring:
            logger.warning("GPUå†…å­˜ç›‘æ§å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._memory_monitor_loop, daemon=True)
        self.monitor_thread.start()
        
        logger.info("ğŸ” GPUå†…å­˜ç›‘æ§å¯åŠ¨")
    
    def _memory_monitor_loop(self):
        """å†…å­˜ç›‘æ§ä¸»å¾ªç¯"""
        while self.is_monitoring:
            try:
                # æ›´æ–°å†…å­˜çŠ¶æ€
                self._update_memory_status()
                
                # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
                self._check_memory_pressure()
                
                # æ‰§è¡Œå†…å­˜ä¼˜åŒ–
                self._optimize_memory_usage()
                
                # è®°å½•å†…å­˜å†å²
                self._record_memory_history()
                
                time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"å†…å­˜ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(10)
    
    def _update_memory_status(self):
        """æ›´æ–°å†…å­˜çŠ¶æ€"""
        try:
            for device_id in range(self.device_count):
                torch.cuda.set_device(device_id)
                
                # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
                allocated = torch.cuda.memory_allocated(device_id)
                cached = torch.cuda.memory_reserved(device_id)
                total = torch.cuda.get_device_properties(device_id).total_memory
                
                self.memory_pools[device_id].update({
                    'allocated_memory': allocated,
                    'cached_memory': cached,
                    'free_memory': total - cached,
                    'utilization': cached / total
                })
                
        except Exception as e:
            logger.error(f"å†…å­˜çŠ¶æ€æ›´æ–°é”™è¯¯: {e}")
    
    def _check_memory_pressure(self):
        """æ£€æŸ¥å†…å­˜å‹åŠ›"""
        try:
            for device_id, pool in self.memory_pools.items():
                utilization = pool['utilization']
                
                if utilization > self.memory_threshold:
                    logger.warning(f"ğŸš¨ GPU {device_id} å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {utilization:.1%}")
                    self._trigger_memory_cleanup(device_id)
                    
        except Exception as e:
            logger.error(f"å†…å­˜å‹åŠ›æ£€æŸ¥é”™è¯¯: {e}")
    
    def _trigger_memory_cleanup(self, device_id: int):
        """è§¦å‘å†…å­˜æ¸…ç†"""
        try:
            torch.cuda.set_device(device_id)
            
            # æ¸…ç†æœªä½¿ç”¨çš„ç¼“å­˜
            torch.cuda.empty_cache()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            # æ›´æ–°æ¸…ç†æ—¶é—´
            self.memory_pools[device_id]['last_cleanup'] = time.time()
            
            logger.info(f"ğŸ§¹ GPU {device_id} å†…å­˜æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"å†…å­˜æ¸…ç†é”™è¯¯: {e}")
    
    def _optimize_memory_usage(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        try:
            for device_id in range(self.device_count):
                pool = self.memory_pools[device_id]
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¼˜åŒ–
                if pool['utilization'] > 0.7:  # 70%ä»¥ä¸Šå¼€å§‹ä¼˜åŒ–
                    self._defragment_memory(device_id)
                    self._optimize_memory_allocation(device_id)
                    
        except Exception as e:
            logger.error(f"å†…å­˜ä¼˜åŒ–é”™è¯¯: {e}")
    
    def _defragment_memory(self, device_id: int):
        """å†…å­˜ç¢ç‰‡æ•´ç†"""
        try:
            torch.cuda.set_device(device_id)
            
            # è®°å½•ä¼˜åŒ–å‰çŠ¶æ€
            before_fragmentation = self._calculate_fragmentation(device_id)
            
            # æ‰§è¡Œå†…å­˜æ•´ç†
            torch.cuda.empty_cache()
            
            # è®°å½•ä¼˜åŒ–åçŠ¶æ€
            after_fragmentation = self._calculate_fragmentation(device_id)
            
            fragmentation_reduced = before_fragmentation - after_fragmentation
            if fragmentation_reduced > 0:
                self.optimization_stats['fragmentation_reduced'] += fragmentation_reduced
                logger.info(f"ğŸ“Š GPU {device_id} å†…å­˜ç¢ç‰‡å‡å°‘: {fragmentation_reduced:.2%}")
                
        except Exception as e:
            logger.error(f"å†…å­˜ç¢ç‰‡æ•´ç†é”™è¯¯: {e}")
    
    def _calculate_fragmentation(self, device_id: int) -> float:
        """è®¡ç®—å†…å­˜ç¢ç‰‡ç‡"""
        try:
            pool = self.memory_pools[device_id]
            allocated = pool['allocated_memory']
            cached = pool['cached_memory']
            
            if cached == 0:
                return 0.0
            
            # ç®€å•çš„ç¢ç‰‡ç‡è®¡ç®—
            fragmentation = (cached - allocated) / cached
            return max(0.0, fragmentation)
            
        except Exception as e:
            logger.error(f"å†…å­˜ç¢ç‰‡è®¡ç®—é”™è¯¯: {e}")
            return 0.0
    
    def _optimize_memory_allocation(self, device_id: int):
        """ä¼˜åŒ–å†…å­˜åˆ†é…ç­–ç•¥"""
        try:
            pool = self.memory_pools[device_id]
            
            # åŸºäºä½¿ç”¨æ¨¡å¼è°ƒæ•´åˆ†é…ç­–ç•¥
            if pool['utilization'] > 0.8:
                # é«˜ä½¿ç”¨ç‡æ—¶é‡‡ç”¨ä¿å®ˆåˆ†é…
                torch.cuda.set_per_process_memory_fraction(0.9, device_id)
            elif pool['utilization'] < 0.3:
                # ä½ä½¿ç”¨ç‡æ—¶é‡Šæ”¾æ›´å¤šå†…å­˜
                torch.cuda.set_per_process_memory_fraction(0.5, device_id)
            else:
                # æ­£å¸¸ä½¿ç”¨ç‡
                torch.cuda.set_per_process_memory_fraction(0.8, device_id)
                
            self.optimization_stats['optimization_count'] += 1
            
        except Exception as e:
            logger.error(f"å†…å­˜åˆ†é…ä¼˜åŒ–é”™è¯¯: {e}")
    
    def _record_memory_history(self):
        """è®°å½•å†…å­˜å†å²"""
        try:
            timestamp = datetime.now(timezone.utc).isoformat()
            
            memory_snapshot = {
                'timestamp': timestamp,
                'devices': {}
            }
            
            for device_id, pool in self.memory_pools.items():
                memory_snapshot['devices'][device_id] = {
                    'allocated_mb': pool['allocated_memory'] / 1024**2,
                    'cached_mb': pool['cached_memory'] / 1024**2,
                    'free_mb': pool['free_memory'] / 1024**2,
                    'utilization': pool['utilization']
                }
            
            self.memory_history.append(memory_snapshot)
            
            # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
            if len(self.memory_history) > 1000:
                self.memory_history = self.memory_history[-500:]
                
        except Exception as e:
            logger.error(f"å†…å­˜å†å²è®°å½•é”™è¯¯: {e}")
    
    def allocate_model_memory(self, model_size_mb: float, device_id: Optional[int] = None) -> Optional[int]:
        """ä¸ºæ¨¡å‹åˆ†é…å†…å­˜"""
        try:
            if not self.gpu_available:
                logger.warning("GPUä¸å¯ç”¨ï¼Œæ— æ³•åˆ†é…GPUå†…å­˜")
                return None
            
            # é€‰æ‹©æœ€ä½³è®¾å¤‡
            if device_id is None:
                device_id = self._select_best_device(model_size_mb)
            
            if device_id is None:
                logger.error("æ²¡æœ‰è¶³å¤Ÿçš„GPUå†…å­˜åˆ†é…æ¨¡å‹")
                return None
            
            torch.cuda.set_device(device_id)
            
            # æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
            required_bytes = model_size_mb * 1024**2
            pool = self.memory_pools[device_id]
            
            if pool['free_memory'] < required_bytes:
                # å°è¯•æ¸…ç†å†…å­˜
                self._trigger_memory_cleanup(device_id)
                self._update_memory_status()
                
                if pool['free_memory'] < required_bytes:
                    logger.error(f"GPU {device_id} å†…å­˜ä¸è¶³ï¼Œéœ€è¦ {model_size_mb:.1f}MBï¼Œå¯ç”¨ {pool['free_memory']/1024**2:.1f}MB")
                    return None
            
            # è®°å½•åˆ†é…
            allocation_id = f"model_{int(time.time())}_{device_id}"
            self.allocated_memory[allocation_id] = {
                'device_id': device_id,
                'size_bytes': required_bytes,
                'allocated_time': time.time(),
                'model_type': 'ai_model'
            }
            
            self.optimization_stats['total_allocations'] += 1
            
            logger.info(f"âœ… æ¨¡å‹å†…å­˜åˆ†é…æˆåŠŸ - GPU {device_id}: {model_size_mb:.1f}MB")
            return device_id
            
        except Exception as e:
            logger.error(f"æ¨¡å‹å†…å­˜åˆ†é…é”™è¯¯: {e}")
            return None
    
    def _select_best_device(self, required_mb: float) -> Optional[int]:
        """é€‰æ‹©æœ€ä½³GPUè®¾å¤‡"""
        try:
            best_device = None
            best_score = -1
            
            required_bytes = required_mb * 1024**2
            
            for device_id, pool in self.memory_pools.items():
                # æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
                if pool['free_memory'] < required_bytes:
                    continue
                
                # è®¡ç®—è®¾å¤‡è¯„åˆ†ï¼ˆè€ƒè™‘å¯ç”¨å†…å­˜å’Œåˆ©ç”¨ç‡ï¼‰
                free_ratio = pool['free_memory'] / pool['total_memory']
                utilization_penalty = pool['utilization']
                
                score = free_ratio - utilization_penalty * 0.5
                
                if score > best_score:
                    best_score = score
                    best_device = device_id
            
            return best_device
            
        except Exception as e:
            logger.error(f"è®¾å¤‡é€‰æ‹©é”™è¯¯: {e}")
            return None
    
    def deallocate_model_memory(self, allocation_id: str):
        """é‡Šæ”¾æ¨¡å‹å†…å­˜"""
        try:
            if allocation_id not in self.allocated_memory:
                logger.warning(f"åˆ†é…IDä¸å­˜åœ¨: {allocation_id}")
                return
            
            allocation = self.allocated_memory[allocation_id]
            device_id = allocation['device_id']
            
            torch.cuda.set_device(device_id)
            torch.cuda.empty_cache()
            
            # ç§»é™¤åˆ†é…è®°å½•
            del self.allocated_memory[allocation_id]
            
            logger.info(f"ğŸ—‘ï¸ æ¨¡å‹å†…å­˜é‡Šæ”¾å®Œæˆ - GPU {device_id}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹å†…å­˜é‡Šæ”¾é”™è¯¯: {e}")
    
    def get_memory_status(self) -> Dict[str, Any]:
        """è·å–å†…å­˜çŠ¶æ€"""
        try:
            status = {
                'gpu_available': self.gpu_available,
                'device_count': self.device_count,
                'devices': {},
                'optimization_stats': self.optimization_stats.copy(),
                'allocated_models': len(self.allocated_memory)
            }
            
            if self.gpu_available:
                for device_id, pool in self.memory_pools.items():
                    # è·å–å®æ—¶GPUä¿¡æ¯
                    gpu_info = {}
                    try:
                        gpus = GPUtil.getGPUs()
                        if device_id < len(gpus):
                            gpu = gpus[device_id]
                            gpu_info = {
                                'temperature': gpu.temperature,
                                'load': gpu.load * 100,
                                'memory_used_mb': gpu.memoryUsed,
                                'memory_total_mb': gpu.memoryTotal
                            }
                    except:
                        pass
                    
                    status['devices'][device_id] = {
                        'total_memory_gb': pool['total_memory'] / 1024**3,
                        'allocated_memory_mb': pool['allocated_memory'] / 1024**2,
                        'cached_memory_mb': pool['cached_memory'] / 1024**2,
                        'free_memory_mb': pool['free_memory'] / 1024**2,
                        'utilization': pool['utilization'],
                        'fragmentation': self._calculate_fragmentation(device_id),
                        **gpu_info
                    }
            
            return status
            
        except Exception as e:
            logger.error(f"å†…å­˜çŠ¶æ€è·å–é”™è¯¯: {e}")
            return {'error': str(e)}
    
    def optimize_for_training(self, model_count: int = 6):
        """ä¸ºè®­ç»ƒä¼˜åŒ–å†…å­˜é…ç½®"""
        try:
            if not self.gpu_available:
                logger.warning("GPUä¸å¯ç”¨ï¼Œè·³è¿‡è®­ç»ƒä¼˜åŒ–")
                return
            
            logger.info(f"ğŸ¯ ä¸º {model_count} ä¸ªæ¨¡å‹ä¼˜åŒ–GPUå†…å­˜é…ç½®")
            
            for device_id in range(self.device_count):
                torch.cuda.set_device(device_id)
                
                # æ¸…ç†æ‰€æœ‰ç¼“å­˜
                torch.cuda.empty_cache()
                
                # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
                memory_fraction = min(0.95, 0.8 + (model_count * 0.02))
                torch.cuda.set_per_process_memory_fraction(memory_fraction, device_id)
                
                logger.info(f"GPU {device_id} å†…å­˜åˆ†é…æ¯”ä¾‹è®¾ç½®ä¸º: {memory_fraction:.1%}")
            
            # é™ä½å†…å­˜ç›‘æ§é˜ˆå€¼ä»¥æ›´ç§¯æåœ°ç®¡ç†å†…å­˜
            self.memory_threshold = 0.8
            
        except Exception as e:
            logger.error(f"è®­ç»ƒå†…å­˜ä¼˜åŒ–é”™è¯¯: {e}")
    
    def stop_memory_monitoring(self):
        """åœæ­¢å†…å­˜ç›‘æ§"""
        self.is_monitoring = False
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5)
        
        logger.info("ğŸ›‘ GPUå†…å­˜ç›‘æ§å·²åœæ­¢")

# å…¨å±€GPUå†…å­˜ä¼˜åŒ–å™¨å®ä¾‹
_gpu_memory_optimizer = None

def initialize_gpu_memory_optimizer() -> ProductionGPUMemoryOptimizer:
    """åˆå§‹åŒ–GPUå†…å­˜ä¼˜åŒ–å™¨"""
    global _gpu_memory_optimizer
    
    if _gpu_memory_optimizer is None:
        _gpu_memory_optimizer = ProductionGPUMemoryOptimizer()
        _gpu_memory_optimizer.start_memory_monitoring()
        logger.success("âœ… GPUå†…å­˜ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    return _gpu_memory_optimizer

def get_gpu_memory_optimizer() -> Optional[ProductionGPUMemoryOptimizer]:
    """è·å–GPUå†…å­˜ä¼˜åŒ–å™¨å®ä¾‹"""
    return _gpu_memory_optimizer

if __name__ == "__main__":
    # æµ‹è¯•GPUå†…å­˜ä¼˜åŒ–å™¨
    optimizer = initialize_gpu_memory_optimizer()
    
    # è¿è¡Œæµ‹è¯•
    for i in range(10):
        status = optimizer.get_memory_status()
        print(f"GPUçŠ¶æ€: {status}")
        time.sleep(2)
    
    optimizer.stop_memory_monitoring()
