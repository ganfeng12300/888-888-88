#!/usr/bin/env python3
"""
ğŸ§  ç¥ç»ç½‘ç»œå¼•æ“ - ç”Ÿäº§çº§AIç³»ç»Ÿ
Neural Network Engine - Production-Grade AI System

ç”Ÿäº§çº§ç‰¹æ€§ï¼š
- æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œæ¨ç†
- å¤šæ¨¡å‹é›†æˆå’Œç®¡ç†
- å®æ—¶é¢„æµ‹å’Œå†³ç­–
- æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶
- è‡ªåŠ¨åŒ–æ¨¡å‹ä¼˜åŒ–
"""

import numpy as np
import pandas as pd
import threading
import time
import pickle
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, asdict
from collections import deque
from pathlib import Path

# æ·±åº¦å­¦ä¹ æ¡†æ¶å¯¼å…¥
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    optim = None

try:
    import tensorflow as tf
    from tensorflow import keras
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False
    tf = None
    keras = None

try:
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import mean_squared_error, r2_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

from ..monitoring.unified_logging_system import UnifiedLoggingSystem, LogConfig, LogCategory

@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯æ•°æ®ç»“æ„"""
    model_id: str
    name: str
    type: str  # 'pytorch', 'tensorflow', 'sklearn'
    version: str
    created_at: datetime
    updated_at: datetime
    performance_metrics: Dict[str, float]
    parameters: Dict[str, Any]
    file_path: Optional[str] = None

@dataclass
class PredictionRequest:
    """é¢„æµ‹è¯·æ±‚æ•°æ®ç»“æ„"""
    request_id: str
    model_id: str
    input_data: Any
    timestamp: datetime
    priority: int = 1
    metadata: Dict[str, Any] = None

@dataclass
class PredictionResult:
    """é¢„æµ‹ç»“æœæ•°æ®ç»“æ„"""
    request_id: str
    model_id: str
    prediction: Any
    confidence: float
    processing_time: float
    timestamp: datetime
    error: Optional[str] = None

class TradingNeuralNetwork(nn.Module):
    """äº¤æ˜“ç¥ç»ç½‘ç»œæ¨¡å‹"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int, dropout_rate: float = 0.2):
        super(TradingNeuralNetwork, self).__init__()
        
        layers = []
        prev_size = input_size
        
        # æ„å»ºéšè—å±‚
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_size),
                nn.Dropout(dropout_rate)
            ])
            prev_size = hidden_size
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, output_size))
        
        self.network = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """åˆå§‹åŒ–æƒé‡"""
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, models_dir: str = "models"):
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        log_config = LogConfig(
            log_dir="logs",
            console_output=True,
            file_output=True,
            json_format=False
        )
        self.logger = UnifiedLoggingSystem(log_config) # "ModelManager")
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(exist_ok=True)
        
        self.loaded_models: Dict[str, Any] = {}
        self.model_info: Dict[str, ModelInfo] = {}
        self._lock = threading.Lock()
        
        # åŠ è½½å·²æœ‰æ¨¡å‹
        self._load_existing_models()
    
    def _load_existing_models(self):
        """åŠ è½½å·²æœ‰æ¨¡å‹"""
        try:
            info_file = self.models_dir / "models_info.json"
            if info_file.exists():
                with open(info_file, 'r', encoding='utf-8') as f:
                    models_data = json.load(f)
                
                for model_id, model_data in models_data.items():
                    model_info = ModelInfo(
                        model_id=model_data['model_id'],
                        name=model_data['name'],
                        type=model_data['type'],
                        version=model_data['version'],
                        created_at=datetime.fromisoformat(model_data['created_at']),
                        updated_at=datetime.fromisoformat(model_data['updated_at']),
                        performance_metrics=model_data['performance_metrics'],
                        parameters=model_data['parameters'],
                        file_path=model_data.get('file_path')
                    )
                    self.model_info[model_id] = model_info
                
                self.logger.info(f"åŠ è½½äº† {len(self.model_info)} ä¸ªæ¨¡å‹ä¿¡æ¯")
                
        except Exception as e:
            self.logger.error(f"åŠ è½½æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
    
    def save_model_info(self):
        """ä¿å­˜æ¨¡å‹ä¿¡æ¯"""
        try:
            info_file = self.models_dir / "models_info.json"
            models_data = {}
            
            for model_id, model_info in self.model_info.items():
                models_data[model_id] = {
                    'model_id': model_info.model_id,
                    'name': model_info.name,
                    'type': model_info.type,
                    'version': model_info.version,
                    'created_at': model_info.created_at.isoformat(),
                    'updated_at': model_info.updated_at.isoformat(),
                    'performance_metrics': model_info.performance_metrics,
                    'parameters': model_info.parameters,
                    'file_path': model_info.file_path
                }
            
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump(models_data, f, indent=2, ensure_ascii=False)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
    
    def register_model(self, model: Any, model_info: ModelInfo) -> bool:
        """æ³¨å†Œæ¨¡å‹"""
        try:
            with self._lock:
                # ä¿å­˜æ¨¡å‹æ–‡ä»¶
                model_file = self.models_dir / f"{model_info.model_id}.pkl"
                
                if model_info.type == 'pytorch' and TORCH_AVAILABLE:
                    torch.save(model.state_dict(), model_file)
                elif model_info.type == 'tensorflow' and TF_AVAILABLE:
                    model.save(str(model_file.with_suffix('.h5')))
                    model_file = model_file.with_suffix('.h5')
                elif model_info.type == 'sklearn' and SKLEARN_AVAILABLE:
                    with open(model_file, 'wb') as f:
                        pickle.dump(model, f)
                else:
                    # é€šç”¨pickleä¿å­˜
                    with open(model_file, 'wb') as f:
                        pickle.dump(model, f)
                
                model_info.file_path = str(model_file)
                model_info.updated_at = datetime.now()
                
                # æ³¨å†Œåˆ°å†…å­˜
                self.loaded_models[model_info.model_id] = model
                self.model_info[model_info.model_id] = model_info
                
                # ä¿å­˜æ¨¡å‹ä¿¡æ¯
                self.save_model_info()
                
                self.logger.info(f"æ¨¡å‹æ³¨å†ŒæˆåŠŸ: {model_info.model_id}")
                return True
                
        except Exception as e:
            self.logger.error(f"æ³¨å†Œæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def load_model(self, model_id: str) -> Optional[Any]:
        """åŠ è½½æ¨¡å‹"""
        try:
            if model_id in self.loaded_models:
                return self.loaded_models[model_id]
            
            if model_id not in self.model_info:
                self.logger.error(f"æ¨¡å‹ä¸å­˜åœ¨: {model_id}")
                return None
            
            model_info = self.model_info[model_id]
            model_file = Path(model_info.file_path)
            
            if not model_file.exists():
                self.logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file}")
                return None
            
            with self._lock:
                if model_info.type == 'pytorch' and TORCH_AVAILABLE:
                    # é‡æ–°åˆ›å»ºPyTorchæ¨¡å‹ç»“æ„
                    model = self._create_pytorch_model(model_info.parameters)
                    model.load_state_dict(torch.load(model_file))
                    model.eval()
                elif model_info.type == 'tensorflow' and TF_AVAILABLE:
                    model = tf.keras.models.load_model(str(model_file))
                else:
                    # é€šç”¨pickleåŠ è½½
                    with open(model_file, 'rb') as f:
                        model = pickle.load(f)
                
                self.loaded_models[model_id] = model
                self.logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id}")
                return model
                
        except Exception as e:
            self.logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def _create_pytorch_model(self, parameters: Dict[str, Any]) -> nn.Module:
        """åˆ›å»ºPyTorchæ¨¡å‹"""
        return TradingNeuralNetwork(
            input_size=parameters['input_size'],
            hidden_sizes=parameters['hidden_sizes'],
            output_size=parameters['output_size'],
            dropout_rate=parameters.get('dropout_rate', 0.2)
        )
    
    def unload_model(self, model_id: str):
        """å¸è½½æ¨¡å‹"""
        with self._lock:
            if model_id in self.loaded_models:
                del self.loaded_models[model_id]
                self.logger.info(f"æ¨¡å‹å·²å¸è½½: {model_id}")
    
    def get_model_info(self, model_id: str) -> Optional[ModelInfo]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.model_info.get(model_id)
    
    def list_models(self) -> List[ModelInfo]:
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        return list(self.model_info.values())

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_manager: ModelManager):
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        log_config = LogConfig(
            log_dir="logs",
            console_output=True,
            file_output=True,
            json_format=False
        )
        self.logger = UnifiedLoggingSystem(log_config) # "ModelTrainer")
        self.model_manager = model_manager
        self.training_history = deque(maxlen=100)
    
    def train_pytorch_model(self, 
                           train_data: np.ndarray, 
                           train_labels: np.ndarray,
                           model_config: Dict[str, Any],
                           training_config: Dict[str, Any]) -> Optional[str]:
        """è®­ç»ƒPyTorchæ¨¡å‹"""
        if not TORCH_AVAILABLE:
            self.logger.error("PyTorchä¸å¯ç”¨")
            return None
        
        try:
            # åˆ›å»ºæ¨¡å‹
            model = TradingNeuralNetwork(
                input_size=model_config['input_size'],
                hidden_sizes=model_config['hidden_sizes'],
                output_size=model_config['output_size'],
                dropout_rate=model_config.get('dropout_rate', 0.2)
            )
            
            # å‡†å¤‡æ•°æ®
            X_tensor = torch.FloatTensor(train_data)
            y_tensor = torch.FloatTensor(train_labels)
            dataset = TensorDataset(X_tensor, y_tensor)
            dataloader = DataLoader(dataset, batch_size=training_config.get('batch_size', 32), shuffle=True)
            
            # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            optimizer = optim.Adam(model.parameters(), lr=training_config.get('learning_rate', 0.001))
            criterion = nn.MSELoss()
            
            # è®­ç»ƒå¾ªç¯
            model.train()
            training_losses = []
            
            for epoch in range(training_config.get('epochs', 100)):
                epoch_loss = 0.0
                
                for batch_X, batch_y in dataloader:
                    optimizer.zero_grad()
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                
                avg_loss = epoch_loss / len(dataloader)
                training_losses.append(avg_loss)
                
                if epoch % 10 == 0:
                    self.logger.info(f"Epoch {epoch}, Loss: {avg_loss:.6f}")
            
            # è¯„ä¼°æ¨¡å‹
            model.eval()
            with torch.no_grad():
                predictions = model(X_tensor).numpy()
                mse = mean_squared_error(train_labels, predictions)
                r2 = r2_score(train_labels, predictions)
            
            # æ³¨å†Œæ¨¡å‹
            model_id = f"pytorch_model_{int(time.time())}"
            model_info = ModelInfo(
                model_id=model_id,
                name=f"Trading Neural Network {datetime.now().strftime('%Y%m%d_%H%M%S')}",
                type='pytorch',
                version='1.0',
                created_at=datetime.now(),
                updated_at=datetime.now(),
                performance_metrics={
                    'mse': float(mse),
                    'r2_score': float(r2),
                    'final_loss': training_losses[-1]
                },
                parameters=model_config
            )
            
            if self.model_manager.register_model(model, model_info):
                self.logger.info(f"PyTorchæ¨¡å‹è®­ç»ƒå®Œæˆ: {model_id}")
                return model_id
            
        except Exception as e:
            self.logger.error(f"PyTorchæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        
        return None
    
    def train_sklearn_model(self,
                           train_data: np.ndarray,
                           train_labels: np.ndarray,
                           model_type: str = 'random_forest',
                           model_config: Dict[str, Any] = None) -> Optional[str]:
        """è®­ç»ƒSklearnæ¨¡å‹"""
        if not SKLEARN_AVAILABLE:
            self.logger.error("Sklearnä¸å¯ç”¨")
            return None
        
        try:
            model_config = model_config or {}
            
            # åˆ›å»ºæ¨¡å‹
            if model_type == 'random_forest':
                model = RandomForestRegressor(
                    n_estimators=model_config.get('n_estimators', 100),
                    max_depth=model_config.get('max_depth', None),
                    random_state=42
                )
            elif model_type == 'gradient_boosting':
                model = GradientBoostingRegressor(
                    n_estimators=model_config.get('n_estimators', 100),
                    learning_rate=model_config.get('learning_rate', 0.1),
                    max_depth=model_config.get('max_depth', 3),
                    random_state=42
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(train_data, train_labels)
            
            # è¯„ä¼°æ¨¡å‹
            predictions = model.predict(train_data)
            mse = mean_squared_error(train_labels, predictions)
            r2 = r2_score(train_labels, predictions)
            
            # æ³¨å†Œæ¨¡å‹
            model_id = f"sklearn_{model_type}_{int(time.time())}"
            model_info = ModelInfo(
                model_id=model_id,
                name=f"Sklearn {model_type} {datetime.now().strftime('%Y%m%d_%H%M%S')}",
                type='sklearn',
                version='1.0',
                created_at=datetime.now(),
                updated_at=datetime.now(),
                performance_metrics={
                    'mse': float(mse),
                    'r2_score': float(r2)
                },
                parameters={
                    'model_type': model_type,
                    **model_config
                }
            )
            
            if self.model_manager.register_model(model, model_info):
                self.logger.info(f"Sklearnæ¨¡å‹è®­ç»ƒå®Œæˆ: {model_id}")
                return model_id
            
        except Exception as e:
            self.logger.error(f"Sklearnæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        
        return None

class PredictionEngine:
    """é¢„æµ‹å¼•æ“"""
    
    def __init__(self, model_manager: ModelManager):
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        log_config = LogConfig(
            log_dir="logs",
            console_output=True,
            file_output=True,
            json_format=False
        )
        self.logger = UnifiedLoggingSystem(log_config) # "PredictionEngine")
        self.model_manager = model_manager
        self.prediction_queue = deque()
        self.prediction_history = deque(maxlen=10000)
        self._running = False
        self._prediction_thread = None
        self._lock = threading.Lock()
    
    def start(self):
        """å¯åŠ¨é¢„æµ‹å¼•æ“"""
        if self._running:
            return
        
        self._running = True
        self._prediction_thread = threading.Thread(target=self._prediction_loop, daemon=True)
        self._prediction_thread.start()
        
        self.logger.info("é¢„æµ‹å¼•æ“å·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢é¢„æµ‹å¼•æ“"""
        self._running = False
        if self._prediction_thread:
            self._prediction_thread.join(timeout=5)
        
        self.logger.info("é¢„æµ‹å¼•æ“å·²åœæ­¢")
    
    def submit_prediction(self, model_id: str, input_data: Any, priority: int = 1, metadata: Dict[str, Any] = None) -> str:
        """æäº¤é¢„æµ‹è¯·æ±‚"""
        request = PredictionRequest(
            request_id=f"pred_{int(time.time() * 1000000)}",
            model_id=model_id,
            input_data=input_data,
            timestamp=datetime.now(),
            priority=priority,
            metadata=metadata or {}
        )
        
        with self._lock:
            # æŒ‰ä¼˜å…ˆçº§æ’å…¥é˜Ÿåˆ—
            inserted = False
            for i, existing_request in enumerate(self.prediction_queue):
                if request.priority > existing_request.priority:
                    self.prediction_queue.insert(i, request)
                    inserted = True
                    break
            
            if not inserted:
                self.prediction_queue.append(request)
        
        self.logger.info(f"é¢„æµ‹è¯·æ±‚å·²æäº¤: {request.request_id}")
        return request.request_id
    
    def get_prediction_result(self, request_id: str) -> Optional[PredictionResult]:
        """è·å–é¢„æµ‹ç»“æœ"""
        for result in self.prediction_history:
            if result.request_id == request_id:
                return result
        return None
    
    def _prediction_loop(self):
        """é¢„æµ‹ä¸»å¾ªç¯"""
        while self._running:
            try:
                with self._lock:
                    if not self.prediction_queue:
                        time.sleep(0.1)
                        continue
                    
                    request = self.prediction_queue.popleft()
                
                # æ‰§è¡Œé¢„æµ‹
                result = self._execute_prediction(request)
                self.prediction_history.append(result)
                
            except Exception as e:
                self.logger.error(f"é¢„æµ‹å¾ªç¯å¼‚å¸¸: {e}")
                time.sleep(1)
    
    def _execute_prediction(self, request: PredictionRequest) -> PredictionResult:
        """æ‰§è¡Œé¢„æµ‹"""
        start_time = time.time()
        
        try:
            # åŠ è½½æ¨¡å‹
            model = self.model_manager.load_model(request.model_id)
            if model is None:
                raise ValueError(f"æ— æ³•åŠ è½½æ¨¡å‹: {request.model_id}")
            
            # æ‰§è¡Œé¢„æµ‹
            model_info = self.model_manager.get_model_info(request.model_id)
            
            if model_info.type == 'pytorch' and TORCH_AVAILABLE:
                prediction, confidence = self._pytorch_predict(model, request.input_data)
            elif model_info.type == 'tensorflow' and TF_AVAILABLE:
                prediction, confidence = self._tensorflow_predict(model, request.input_data)
            elif model_info.type == 'sklearn' and SKLEARN_AVAILABLE:
                prediction, confidence = self._sklearn_predict(model, request.input_data)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_info.type}")
            
            processing_time = time.time() - start_time
            
            result = PredictionResult(
                request_id=request.request_id,
                model_id=request.model_id,
                prediction=prediction,
                confidence=confidence,
                processing_time=processing_time,
                timestamp=datetime.now()
            )
            
            self.logger.info(f"é¢„æµ‹å®Œæˆ: {request.request_id}, è€—æ—¶: {processing_time:.3f}ç§’")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            
            result = PredictionResult(
                request_id=request.request_id,
                model_id=request.model_id,
                prediction=None,
                confidence=0.0,
                processing_time=processing_time,
                timestamp=datetime.now(),
                error=str(e)
            )
            
            self.logger.error(f"é¢„æµ‹å¤±è´¥: {request.request_id}, é”™è¯¯: {e}")
            return result
    
    def _pytorch_predict(self, model: nn.Module, input_data: np.ndarray) -> Tuple[np.ndarray, float]:
        """PyTorchæ¨¡å‹é¢„æµ‹"""
        model.eval()
        with torch.no_grad():
            input_tensor = torch.FloatTensor(input_data)
            if len(input_tensor.shape) == 1:
                input_tensor = input_tensor.unsqueeze(0)
            
            output = model(input_tensor)
            prediction = output.numpy()
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆç®€å•çš„æ–¹å·®å€’æ•°ï¼‰
            confidence = 1.0 / (1.0 + np.var(prediction))
            
            return prediction.squeeze(), float(confidence)
    
    def _tensorflow_predict(self, model, input_data: np.ndarray) -> Tuple[np.ndarray, float]:
        """TensorFlowæ¨¡å‹é¢„æµ‹"""
        if len(input_data.shape) == 1:
            input_data = input_data.reshape(1, -1)
        
        prediction = model.predict(input_data, verbose=0)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = 1.0 / (1.0 + np.var(prediction))
        
        return prediction.squeeze(), float(confidence)
    
    def _sklearn_predict(self, model, input_data: np.ndarray) -> Tuple[np.ndarray, float]:
        """Sklearnæ¨¡å‹é¢„æµ‹"""
        if len(input_data.shape) == 1:
            input_data = input_data.reshape(1, -1)
        
        prediction = model.predict(input_data)
        
        # å¯¹äºæ”¯æŒçš„æ¨¡å‹ï¼Œè·å–é¢„æµ‹åŒºé—´
        confidence = 0.8  # é»˜è®¤ç½®ä¿¡åº¦
        
        if hasattr(model, 'predict_proba'):
            # åˆ†ç±»æ¨¡å‹
            probabilities = model.predict_proba(input_data)
            confidence = float(np.max(probabilities))
        elif hasattr(model, 'estimators_'):
            # é›†æˆæ¨¡å‹ï¼Œè®¡ç®—é¢„æµ‹æ–¹å·®
            predictions = np.array([estimator.predict(input_data) for estimator in model.estimators_])
            variance = np.var(predictions, axis=0)
            confidence = 1.0 / (1.0 + np.mean(variance))
        
        return prediction, float(confidence)

class NeuralNetworkEngine:
    """ç¥ç»ç½‘ç»œå¼•æ“ä¸»ç±»"""
    
    def __init__(self, models_dir: str = "models"):
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        log_config = LogConfig(
            log_dir="logs",
            console_output=True,
            file_output=True,
            json_format=False
        )
        self.logger = UnifiedLoggingSystem(log_config) # "NeuralNetworkEngine")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model_manager = ModelManager(models_dir)
        self.model_trainer = ModelTrainer(self.model_manager)
        self.prediction_engine = PredictionEngine(self.model_manager)
        
        # å¯åŠ¨é¢„æµ‹å¼•æ“
        self.prediction_engine.start()
        
        self.logger.info("ç¥ç»ç½‘ç»œå¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            'torch_available': TORCH_AVAILABLE,
            'tensorflow_available': TF_AVAILABLE,
            'sklearn_available': SKLEARN_AVAILABLE,
            'loaded_models': len(self.model_manager.loaded_models),
            'total_models': len(self.model_manager.model_info),
            'prediction_queue_size': len(self.prediction_engine.prediction_queue),
            'prediction_history_size': len(self.prediction_engine.prediction_history)
        }
    
    def train_model(self, 
                   train_data: np.ndarray,
                   train_labels: np.ndarray,
                   model_type: str = 'pytorch',
                   model_config: Dict[str, Any] = None,
                   training_config: Dict[str, Any] = None) -> Optional[str]:
        """è®­ç»ƒæ¨¡å‹"""
        if model_type == 'pytorch':
            return self.model_trainer.train_pytorch_model(train_data, train_labels, model_config or {}, training_config or {})
        elif model_type.startswith('sklearn_'):
            sklearn_type = model_type.replace('sklearn_', '')
            return self.model_trainer.train_sklearn_model(train_data, train_labels, sklearn_type, model_config or {})
        else:
            self.logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
            return None
    
    def predict(self, model_id: str, input_data: Any, priority: int = 1) -> str:
        """æäº¤é¢„æµ‹è¯·æ±‚"""
        return self.prediction_engine.submit_prediction(model_id, input_data, priority)
    
    def get_prediction(self, request_id: str) -> Optional[PredictionResult]:
        """è·å–é¢„æµ‹ç»“æœ"""
        return self.prediction_engine.get_prediction_result(request_id)
    
    def wait_for_prediction(self, request_id: str, timeout: float = 30.0) -> Optional[PredictionResult]:
        """ç­‰å¾…é¢„æµ‹ç»“æœ"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            result = self.get_prediction(request_id)
            if result:
                return result
            time.sleep(0.1)
        
        return None
    
    def list_models(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        return [asdict(model_info) for model_info in self.model_manager.list_models()]
    
    def get_model_info(self, model_id: str) -> Optional[Dict]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        model_info = self.model_manager.get_model_info(model_id)
        return asdict(model_info) if model_info else None
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            self.prediction_engine.stop()
            self.logger.info("ç¥ç»ç½‘ç»œå¼•æ“èµ„æºå·²æ¸…ç†")
        except Exception as e:
            self.logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºç¥ç»ç½‘ç»œå¼•æ“
    nn_engine = NeuralNetworkEngine()
    
    try:
        # è·å–ç³»ç»Ÿä¿¡æ¯
        system_info = nn_engine.get_system_info()
        print("AIç³»ç»Ÿä¿¡æ¯:", json.dumps(system_info, indent=2, ensure_ascii=False))
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        X_train = np.random.rand(1000, 10).astype(np.float32)
        y_train = np.sum(X_train, axis=1) + np.random.normal(0, 0.1, 1000)
        
        # è®­ç»ƒæ¨¡å‹
        if TORCH_AVAILABLE:
            model_config = {
                'input_size': 10,
                'hidden_sizes': [64, 32],
                'output_size': 1,
                'dropout_rate': 0.2
            }
            
            training_config = {
                'epochs': 50,
                'batch_size': 32,
                'learning_rate': 0.001
            }
            
            model_id = nn_engine.train_model(X_train, y_train, 'pytorch', model_config, training_config)
            if model_id:
                print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ: {model_id}")
                
                # æµ‹è¯•é¢„æµ‹
                test_data = np.random.rand(1, 10).astype(np.float32)
                request_id = nn_engine.predict(model_id, test_data)
                
                result = nn_engine.wait_for_prediction(request_id, timeout=10)
                if result and not result.error:
                    print(f"é¢„æµ‹ç»“æœ: {result.prediction}, ç½®ä¿¡åº¦: {result.confidence:.3f}")
                else:
                    print(f"é¢„æµ‹å¤±è´¥: {result.error if result else 'è¶…æ—¶'}")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
    
    finally:
        nn_engine.cleanup()
