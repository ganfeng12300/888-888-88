#!/usr/bin/env python3
"""
ğŸ§  888-888-88 AIæ¨¡å‹è®­ç»ƒå™¨
ç”Ÿäº§çº§æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–ç³»ç»Ÿ
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum
import matplotlib.pyplot as plt
import seaborn as sns
from loguru import logger
import warnings
warnings.filterwarnings('ignore')


class ModelType(Enum):
    """æ¨¡å‹ç±»å‹"""
    LSTM = "lstm"
    GRU = "gru"
    TRANSFORMER = "transformer"
    CNN_LSTM = "cnn_lstm"
    ATTENTION_LSTM = "attention_lstm"


class FeatureType(Enum):
    """ç‰¹å¾ç±»å‹"""
    PRICE = "price"
    VOLUME = "volume"
    TECHNICAL = "technical"
    SENTIMENT = "sentiment"
    MACRO = "macro"


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    model_type: ModelType = ModelType.LSTM
    sequence_length: int = 60
    prediction_horizon: int = 1
    batch_size: int = 32
    epochs: int = 100
    learning_rate: float = 0.001
    dropout_rate: float = 0.2
    hidden_size: int = 128
    num_layers: int = 2
    early_stopping_patience: int = 10
    validation_split: float = 0.2
    test_split: float = 0.1


@dataclass
class ModelMetrics:
    """æ¨¡å‹è¯„ä¼°æŒ‡æ ‡"""
    mse: float
    mae: float
    rmse: float
    r2: float
    directional_accuracy: float
    sharpe_ratio: float
    max_drawdown: float
    profit_factor: float


class LSTMModel(nn.Module):
    """LSTMé¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, 
                 output_size: int, dropout: float = 0.2):
        super(LSTMModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )
        
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        batch_size = x.size(0)
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        
        # LSTMå‰å‘ä¼ æ’­
        lstm_out, _ = self.lstm(x, (h0, c0))
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]
        
        # Dropoutå’Œå…¨è¿æ¥å±‚
        output = self.dropout(last_output)
        output = self.fc(output)
        
        return output


class TransformerModel(nn.Module):
    """Transformeré¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_size: int, d_model: int, nhead: int, 
                 num_layers: int, output_size: int, dropout: float = 0.1):
        super(TransformerModel, self).__init__()
        
        self.input_projection = nn.Linear(input_size, d_model)
        self.positional_encoding = self._generate_positional_encoding(1000, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dropout=dropout,
            batch_first=True
        )
        
        self.transformer = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=num_layers
        )
        
        self.output_projection = nn.Linear(d_model, output_size)
        self.dropout = nn.Dropout(dropout)
        
    def _generate_positional_encoding(self, max_len: int, d_model: int):
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe.unsqueeze(0)
    
    def forward(self, x):
        seq_len = x.size(1)
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        if seq_len <= self.positional_encoding.size(1):
            x += self.positional_encoding[:, :seq_len, :].to(x.device)
        
        # Transformerç¼–ç 
        transformer_out = self.transformer(x)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        last_output = transformer_out[:, -1, :]
        
        # è¾“å‡ºæŠ•å½±
        output = self.dropout(last_output)
        output = self.output_projection(output)
        
        return output


class ModelTrainer:
    """AIæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.scaler = StandardScaler()
        self.target_scaler = MinMaxScaler()
        
        # æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€
        self.model = None
        self.optimizer = None
        self.criterion = nn.MSELoss()
        self.training_history = []
        
        # ç‰¹å¾å·¥ç¨‹
        self.feature_columns = []
        self.feature_importance = {}
        
        logger.info(f"ğŸ§  AIæ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {self.device})")
    
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾å·¥ç¨‹"""
        logger.info("ğŸ”§ å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        
        # åŸºç¡€ä»·æ ¼ç‰¹å¾
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        df['price_change'] = df['close'] - df['open']
        df['price_range'] = df['high'] - df['low']
        df['body_size'] = abs(df['close'] - df['open'])
        df['upper_shadow'] = df['high'] - df[['open', 'close']].max(axis=1)
        df['lower_shadow'] = df[['open', 'close']].min(axis=1) - df['low']
        
        # ç§»åŠ¨å¹³å‡ç‰¹å¾
        for window in [5, 10, 20, 50]:
            df[f'sma_{window}'] = df['close'].rolling(window).mean()
            df[f'ema_{window}'] = df['close'].ewm(span=window).mean()
            df[f'price_to_sma_{window}'] = df['close'] / df[f'sma_{window}']
        
        # æ³¢åŠ¨ç‡ç‰¹å¾
        for window in [5, 10, 20]:
            df[f'volatility_{window}'] = df['returns'].rolling(window).std()
            df[f'realized_vol_{window}'] = df['returns'].rolling(window).std() * np.sqrt(252)
        
        # æŠ€æœ¯æŒ‡æ ‡
        df = self._add_technical_indicators(df)
        
        # æˆäº¤é‡ç‰¹å¾
        df['volume_sma'] = df['volume'].rolling(20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma']
        df['price_volume'] = df['close'] * df['volume']
        df['vwap'] = (df['price_volume'].rolling(20).sum() / 
                     df['volume'].rolling(20).sum())
        
        # æ—¶é—´ç‰¹å¾
        df['hour'] = df.index.hour
        df['day_of_week'] = df.index.dayofweek
        df['month'] = df.index.month
        df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)
        
        # æ»åç‰¹å¾
        for lag in [1, 2, 3, 5, 10]:
            df[f'returns_lag_{lag}'] = df['returns'].shift(lag)
            df[f'volume_lag_{lag}'] = df['volume'].shift(lag)
        
        # å‰ç»ç‰¹å¾ï¼ˆç›®æ ‡å˜é‡ï¼‰
        df['target'] = df['close'].shift(-self.config.prediction_horizon)
        df['target_returns'] = df['target'] / df['close'] - 1
        
        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        df = df.dropna()
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        self.feature_columns = [col for col in df.columns 
                              if col not in ['target', 'target_returns'] 
                              and not col.startswith('Unnamed')]
        
        logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œå…±ç”Ÿæˆ {len(self.feature_columns)} ä¸ªç‰¹å¾")
        return df
    
    def _add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ æŠ€æœ¯æŒ‡æ ‡"""
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD
        ema12 = df['close'].ewm(span=12).mean()
        ema26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema12 - ema26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']
        
        # å¸ƒæ—å¸¦
        df['bb_middle'] = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        
        # éšæœºæŒ‡æ ‡
        low_min = df['low'].rolling(window=14).min()
        high_max = df['high'].rolling(window=14).max()
        df['stoch_k'] = 100 * (df['close'] - low_min) / (high_max - low_min)
        df['stoch_d'] = df['stoch_k'].rolling(window=3).mean()
        
        # å¨å»‰æŒ‡æ ‡
        df['williams_r'] = -100 * (high_max - df['close']) / (high_max - low_min)
        
        # å•†å“é€šé“æŒ‡æ•°
        tp = (df['high'] + df['low'] + df['close']) / 3
        sma_tp = tp.rolling(20).mean()
        mad = tp.rolling(20).apply(lambda x: np.mean(np.abs(x - x.mean())))
        df['cci'] = (tp - sma_tp) / (0.015 * mad)
        
        return df
    
    def create_sequences(self, data: np.ndarray, targets: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """åˆ›å»ºæ—¶é—´åºåˆ—åºåˆ—"""
        X, y = [], []
        
        for i in range(self.config.sequence_length, len(data)):
            X.append(data[i-self.config.sequence_length:i])
            y.append(targets[i])
        
        return np.array(X), np.array(y)
    
    def prepare_data(self, df: pd.DataFrame) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # ç‰¹å¾å’Œç›®æ ‡
        features = df[self.feature_columns].values
        targets = df['target_returns'].values
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        features_scaled = self.scaler.fit_transform(features)
        targets_scaled = self.target_scaler.fit_transform(targets.reshape(-1, 1)).flatten()
        
        # åˆ›å»ºåºåˆ—
        X, y = self.create_sequences(features_scaled, targets_scaled)
        
        # æ—¶é—´åºåˆ—åˆ†å‰²
        train_size = int(len(X) * (1 - self.config.validation_split - self.config.test_split))
        val_size = int(len(X) * self.config.validation_split)
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:train_size + val_size]
        y_val = y[train_size:train_size + val_size]
        X_test = X[train_size + val_size:]
        y_test = y[train_size + val_size:]
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train = torch.FloatTensor(X_train)
        y_train = torch.FloatTensor(y_train)
        X_val = torch.FloatTensor(X_val)
        y_val = torch.FloatTensor(y_val)
        X_test = torch.FloatTensor(X_test)
        y_test = torch.FloatTensor(y_test)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)
        test_dataset = TensorDataset(X_test, y_test)
        
        train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.config.batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size, shuffle=False)
        
        logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ - è®­ç»ƒ: {len(X_train)}, éªŒè¯: {len(X_val)}, æµ‹è¯•: {len(X_test)}")
        
        return train_loader, val_loader, test_loader
    
    def create_model(self, input_size: int) -> nn.Module:
        """åˆ›å»ºæ¨¡å‹"""
        if self.config.model_type == ModelType.LSTM:
            model = LSTMModel(
                input_size=input_size,
                hidden_size=self.config.hidden_size,
                num_layers=self.config.num_layers,
                output_size=1,
                dropout=self.config.dropout_rate
            )
        elif self.config.model_type == ModelType.TRANSFORMER:
            model = TransformerModel(
                input_size=input_size,
                d_model=self.config.hidden_size,
                nhead=8,
                num_layers=self.config.num_layers,
                output_size=1,
                dropout=self.config.dropout_rate
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.config.model_type}")
        
        return model.to(self.device)
    
    def train_model(self, train_loader: DataLoader, val_loader: DataLoader) -> Dict[str, List[float]]:
        """è®­ç»ƒæ¨¡å‹"""
        logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        
        # åˆ›å»ºæ¨¡å‹
        input_size = train_loader.dataset[0][0].shape[-1]
        self.model = self.create_model(input_size)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.config.learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5, verbose=True
        )
        
        # è®­ç»ƒå†å²
        history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': []
        }
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.config.epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            
            for batch_X, batch_y in train_loader:
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)
                
                self.optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = self.criterion(outputs.squeeze(), batch_y)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                train_loss += loss.item()
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X = batch_X.to(self.device)
                    batch_y = batch_y.to(self.device)
                    
                    outputs = self.model(batch_X)
                    loss = self.criterion(outputs.squeeze(), batch_y)
                    val_loss += loss.item()
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(avg_val_loss)
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            history['train_loss'].append(avg_train_loss)
            history['val_loss'].append(avg_val_loss)
            history['learning_rate'].append(current_lr)
            
            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.model.state_dict(), 'best_model.pth')
            else:
                patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch {epoch+1}/{self.config.epochs} - "
                          f"Train Loss: {avg_train_loss:.6f}, "
                          f"Val Loss: {avg_val_loss:.6f}, "
                          f"LR: {current_lr:.6f}")
            
            # æ—©åœ
            if patience_counter >= self.config.early_stopping_patience:
                logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        self.model.load_state_dict(torch.load('best_model.pth'))
        
        self.training_history = history
        logger.info("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        return history
    
    def evaluate_model(self, test_loader: DataLoader) -> ModelMetrics:
        """è¯„ä¼°æ¨¡å‹"""
        logger.info("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        self.model.eval()
        predictions = []
        actuals = []
        
        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)
                
                outputs = self.model(batch_X)
                predictions.extend(outputs.squeeze().cpu().numpy())
                actuals.extend(batch_y.cpu().numpy())
        
        predictions = np.array(predictions)
        actuals = np.array(actuals)
        
        # åæ ‡å‡†åŒ–
        predictions = self.target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()
        actuals = self.target_scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()
        
        # è®¡ç®—æŒ‡æ ‡
        mse = mean_squared_error(actuals, predictions)
        mae = mean_absolute_error(actuals, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(actuals, predictions)
        
        # æ–¹å‘å‡†ç¡®ç‡
        actual_direction = np.sign(actuals)
        pred_direction = np.sign(predictions)
        directional_accuracy = np.mean(actual_direction == pred_direction)
        
        # äº¤æ˜“æŒ‡æ ‡
        returns = predictions  # é¢„æµ‹æ”¶ç›Šç‡
        sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252) if np.std(returns) > 0 else 0
        
        # æœ€å¤§å›æ’¤
        cumulative_returns = np.cumprod(1 + returns)
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdown = (cumulative_returns - running_max) / running_max
        max_drawdown = np.min(drawdown)
        
        # ç›ˆåˆ©å› å­
        positive_returns = returns[returns > 0]
        negative_returns = returns[returns < 0]
        profit_factor = (np.sum(positive_returns) / abs(np.sum(negative_returns)) 
                        if len(negative_returns) > 0 else np.inf)
        
        metrics = ModelMetrics(
            mse=mse,
            mae=mae,
            rmse=rmse,
            r2=r2,
            directional_accuracy=directional_accuracy,
            sharpe_ratio=sharpe_ratio,
            max_drawdown=max_drawdown,
            profit_factor=profit_factor
        )
        
        logger.info(f"âœ… æ¨¡å‹è¯„ä¼°å®Œæˆ - RÂ²: {r2:.4f}, æ–¹å‘å‡†ç¡®ç‡: {directional_accuracy:.4f}")
        
        return metrics
    
    def save_model(self, filepath: str, metadata: Dict[str, Any] = None) -> None:
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'model_state_dict': self.model.state_dict(),
            'config': self.config.__dict__,
            'scaler': self.scaler,
            'target_scaler': self.target_scaler,
            'feature_columns': self.feature_columns,
            'training_history': self.training_history,
            'metadata': metadata or {},
            'timestamp': datetime.now().isoformat()
        }
        
        torch.save(model_data, filepath)
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {filepath}")
    
    def load_model(self, filepath: str) -> Dict[str, Any]:
        """åŠ è½½æ¨¡å‹"""
        model_data = torch.load(filepath, map_location=self.device)
        
        # æ¢å¤é…ç½®
        config_dict = model_data['config']
        self.config = TrainingConfig(**config_dict)
        
        # æ¢å¤ç¼©æ”¾å™¨å’Œç‰¹å¾
        self.scaler = model_data['scaler']
        self.target_scaler = model_data['target_scaler']
        self.feature_columns = model_data['feature_columns']
        self.training_history = model_data.get('training_history', [])
        
        # é‡å»ºæ¨¡å‹
        input_size = len(self.feature_columns)
        self.model = self.create_model(input_size)
        self.model.load_state_dict(model_data['model_state_dict'])
        
        logger.info(f"ğŸ“‚ æ¨¡å‹å·²ä» {filepath} åŠ è½½")
        
        return model_data.get('metadata', {})
    
    def predict(self, data: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒæˆ–åŠ è½½")
        
        self.model.eval()
        
        # æ ‡å‡†åŒ–è¾“å…¥
        data_scaled = self.scaler.transform(data)
        
        # åˆ›å»ºåºåˆ—
        if len(data_scaled) < self.config.sequence_length:
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {self.config.sequence_length} ä¸ªæ ·æœ¬")
        
        # å–æœ€åä¸€ä¸ªåºåˆ—
        sequence = data_scaled[-self.config.sequence_length:].reshape(1, self.config.sequence_length, -1)
        sequence_tensor = torch.FloatTensor(sequence).to(self.device)
        
        with torch.no_grad():
            prediction = self.model(sequence_tensor)
            prediction = prediction.cpu().numpy()
        
        # åæ ‡å‡†åŒ–
        prediction = self.target_scaler.inverse_transform(prediction.reshape(-1, 1)).flatten()
        
        return prediction
    
    def plot_training_history(self, save_path: str = None) -> None:
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        if not self.training_history:
            logger.warning("âš ï¸ æ²¡æœ‰è®­ç»ƒå†å²æ•°æ®")
            return
        
        fig, axes = plt.subplots(2, 1, figsize=(12, 8))
        
        # æŸå¤±æ›²çº¿
        axes[0].plot(self.training_history['train_loss'], label='è®­ç»ƒæŸå¤±', color='blue')
        axes[0].plot(self.training_history['val_loss'], label='éªŒè¯æŸå¤±', color='red')
        axes[0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].legend()
        axes[0].grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿
        axes[1].plot(self.training_history['learning_rate'], label='å­¦ä¹ ç‡', color='green')
        axes[1].set_title('å­¦ä¹ ç‡å˜åŒ–')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Learning Rate')
        axes[1].legend()
        axes[1].grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"ğŸ“Š è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹è®­ç»ƒå™¨
    config = TrainingConfig(
        model_type=ModelType.LSTM,
        epochs=50,
        batch_size=32
    )
    
    trainer = ModelTrainer(config)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    dates = pd.date_range('2020-01-01', '2023-12-31', freq='H')
    np.random.seed(42)
    
    data = {
        'open': np.random.randn(len(dates)).cumsum() + 100,
        'high': np.random.randn(len(dates)).cumsum() + 102,
        'low': np.random.randn(len(dates)).cumsum() + 98,
        'close': np.random.randn(len(dates)).cumsum() + 100,
        'volume': np.random.exponential(1000, len(dates))
    }
    
    df = pd.DataFrame(data, index=dates)
    
    # ç‰¹å¾å·¥ç¨‹
    df = trainer.prepare_features(df)
    
    # å‡†å¤‡æ•°æ®
    train_loader, val_loader, test_loader = trainer.prepare_data(df)
    
    # è®­ç»ƒæ¨¡å‹
    history = trainer.train_model(train_loader, val_loader)
    
    # è¯„ä¼°æ¨¡å‹
    metrics = trainer.evaluate_model(test_loader)
    print(f"æ¨¡å‹è¯„ä¼°ç»“æœ: RÂ² = {metrics.r2:.4f}, æ–¹å‘å‡†ç¡®ç‡ = {metrics.directional_accuracy:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model('test_model.pth')
    
    logger.info("ğŸ‰ æ¨¡å‹è®­ç»ƒæµ‹è¯•å®Œæˆ")

