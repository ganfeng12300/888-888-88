#!/usr/bin/env python3
"""
GPUæ¨¡å‹è°ƒåº¦å™¨ - ç”Ÿäº§çº§å¤šAIæ¨¡å‹å¹¶è¡Œè®­ç»ƒè°ƒåº¦ç®¡ç†
å®ç°6ä¸ªAIæ¨¡å‹æ™ºèƒ½è°ƒåº¦ã€è´Ÿè½½å‡è¡¡ã€èµ„æºä¼˜åŒ–
"""
import asyncio
import os
import sys
import time
import threading
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Tuple, Callable
import json
import queue
import numpy as np
from loguru import logger
import torch
import torch.multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import psutil
import GPUtil

class ModelTask:
    """æ¨¡å‹ä»»åŠ¡ç±»"""
    
    def __init__(self, task_id: str, model_type: str, priority: int = 1, 
                 config: Dict = None, callback: Callable = None):
        self.task_id = task_id
        self.model_type = model_type
        self.priority = priority
        self.config = config or {}
        self.callback = callback
        self.status = 'pending'
        self.created_time = time.time()
        self.start_time = None
        self.end_time = None
        self.result = None
        self.error = None
        self.device_id = None
        self.memory_allocated = 0
        
    def __lt__(self, other):
        # ä¼˜å…ˆçº§é˜Ÿåˆ—æ’åºï¼ˆä¼˜å…ˆçº§é«˜çš„å…ˆæ‰§è¡Œï¼‰
        return self.priority > other.priority

class ProductionGPUModelScheduler:
    """ç”Ÿäº§çº§GPUæ¨¡å‹è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.device_count = torch.cuda.device_count() if self.gpu_available else 0
        self.cpu_cores = psutil.cpu_count()
        
        # ä»»åŠ¡é˜Ÿåˆ—
        self.task_queue = queue.PriorityQueue()
        self.running_tasks = {}
        self.completed_tasks = {}
        self.failed_tasks = {}
        
        # èµ„æºç®¡ç†
        self.device_status = {}
        self.cpu_slots = min(6, self.cpu_cores // 2)  # CPUè®­ç»ƒæ§½ä½
        self.gpu_slots = min(2, self.device_count) if self.gpu_available else 0  # GPUè®­ç»ƒæ§½ä½
        self.used_cpu_slots = 0
        self.used_gpu_slots = 0
        
        # è°ƒåº¦ç»Ÿè®¡
        self.scheduler_stats = {
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'average_execution_time': 0.0,
            'gpu_utilization': 0.0,
            'cpu_utilization': 0.0
        }
        
        # çº¿ç¨‹æ± 
        self.cpu_executor = ThreadPoolExecutor(max_workers=self.cpu_slots)
        self.gpu_executor = ThreadPoolExecutor(max_workers=self.gpu_slots)
        
        # è°ƒåº¦æ§åˆ¶
        self.is_running = False
        self.scheduler_thread = None
        self.monitor_thread = None
        
        # AIæ¨¡å‹ç±»å‹é…ç½®
        self.model_configs = {
            'reinforcement_learning': {
                'memory_requirement_mb': 2048,
                'preferred_device': 'gpu',
                'training_time_estimate': 300,
                'priority_base': 5
            },
            'deep_learning': {
                'memory_requirement_mb': 1536,
                'preferred_device': 'gpu',
                'training_time_estimate': 240,
                'priority_base': 4
            },
            'ensemble_learning': {
                'memory_requirement_mb': 512,
                'preferred_device': 'cpu',
                'training_time_estimate': 180,
                'priority_base': 3
            },
            'expert_system': {
                'memory_requirement_mb': 256,
                'preferred_device': 'cpu',
                'training_time_estimate': 120,
                'priority_base': 2
            },
            'meta_learning': {
                'memory_requirement_mb': 1024,
                'preferred_device': 'gpu',
                'training_time_estimate': 360,
                'priority_base': 6
            },
            'transfer_learning': {
                'memory_requirement_mb': 768,
                'preferred_device': 'gpu',
                'training_time_estimate': 200,
                'priority_base': 4
            }
        }
        
        self._initialize_device_status()
        logger.info(f"ğŸ¯ GPUæ¨¡å‹è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ - CPUæ§½ä½: {self.cpu_slots}, GPUæ§½ä½: {self.gpu_slots}")
    
    def _initialize_device_status(self):
        """åˆå§‹åŒ–è®¾å¤‡çŠ¶æ€"""
        try:
            # åˆå§‹åŒ–CPUçŠ¶æ€
            self.device_status['cpu'] = {
                'available_slots': self.cpu_slots,
                'used_slots': 0,
                'total_memory_gb': psutil.virtual_memory().total / 1024**3,
                'available_memory_gb': psutil.virtual_memory().available / 1024**3,
                'utilization': 0.0,
                'temperature': 0.0,
                'running_tasks': []
            }
            
            # åˆå§‹åŒ–GPUçŠ¶æ€
            if self.gpu_available:
                for device_id in range(self.device_count):
                    gpu_props = torch.cuda.get_device_properties(device_id)
                    self.device_status[f'gpu_{device_id}'] = {
                        'device_id': device_id,
                        'name': gpu_props.name,
                        'total_memory_gb': gpu_props.total_memory / 1024**3,
                        'available_memory_gb': gpu_props.total_memory / 1024**3,
                        'utilization': 0.0,
                        'temperature': 0.0,
                        'running_tasks': [],
                        'max_concurrent_tasks': 1  # RTX3060å»ºè®®å•ä»»åŠ¡
                    }
            
        except Exception as e:
            logger.error(f"è®¾å¤‡çŠ¶æ€åˆå§‹åŒ–é”™è¯¯: {e}")
    
    def start_scheduler(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if self.is_running:
            logger.warning("æ¨¡å‹è°ƒåº¦å™¨å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_running = True
        
        # å¯åŠ¨è°ƒåº¦çº¿ç¨‹
        self.scheduler_thread = threading.Thread(target=self._scheduler_loop, daemon=True)
        self.scheduler_thread.start()
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        
        logger.info("ğŸš€ GPUæ¨¡å‹è°ƒåº¦å™¨å¯åŠ¨")
    
    def _scheduler_loop(self):
        """è°ƒåº¦å™¨ä¸»å¾ªç¯"""
        while self.is_running:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰å¾…æ‰§è¡Œä»»åŠ¡
                if not self.task_queue.empty():
                    # å°è¯•è°ƒåº¦ä»»åŠ¡
                    self._schedule_next_task()
                
                # æ¸…ç†å®Œæˆçš„ä»»åŠ¡
                self._cleanup_completed_tasks()
                
                # æ›´æ–°è°ƒåº¦ç»Ÿè®¡
                self._update_scheduler_stats()
                
                time.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"è°ƒåº¦å™¨å¾ªç¯é”™è¯¯: {e}")
                time.sleep(5)
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.is_running:
            try:
                # æ›´æ–°è®¾å¤‡çŠ¶æ€
                self._update_device_status()
                
                # æ£€æŸ¥ä»»åŠ¡è¶…æ—¶
                self._check_task_timeouts()
                
                # è®°å½•ç›‘æ§ä¿¡æ¯
                self._log_scheduler_status()
                
                time.sleep(10)  # æ¯10ç§’ç›‘æ§ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(15)
    
    def _schedule_next_task(self):
        """è°ƒåº¦ä¸‹ä¸€ä¸ªä»»åŠ¡"""
        try:
            # è·å–ä¼˜å…ˆçº§æœ€é«˜çš„ä»»åŠ¡
            priority, task = self.task_queue.get_nowait()
            
            # é€‰æ‹©æœ€ä½³è®¾å¤‡
            device_type, device_id = self._select_best_device(task)
            
            if device_type is None:
                # æ²¡æœ‰å¯ç”¨è®¾å¤‡ï¼Œé‡æ–°æ”¾å›é˜Ÿåˆ—
                self.task_queue.put((priority, task))
                return
            
            # åˆ†é…èµ„æºå¹¶æ‰§è¡Œä»»åŠ¡
            self._execute_task(task, device_type, device_id)
            
        except queue.Empty:
            pass
        except Exception as e:
            logger.error(f"ä»»åŠ¡è°ƒåº¦é”™è¯¯: {e}")
    
    def _select_best_device(self, task: ModelTask) -> Tuple[Optional[str], Optional[int]]:
        """é€‰æ‹©æœ€ä½³è®¾å¤‡"""
        try:
            model_config = self.model_configs.get(task.model_type, {})
            preferred_device = model_config.get('preferred_device', 'cpu')
            memory_requirement = model_config.get('memory_requirement_mb', 512)
            
            # ä¼˜å…ˆé€‰æ‹©æ¨èè®¾å¤‡ç±»å‹
            if preferred_device == 'gpu' and self.gpu_available:
                # æŸ¥æ‰¾å¯ç”¨GPU
                for device_id in range(self.device_count):
                    device_key = f'gpu_{device_id}'
                    device_status = self.device_status[device_key]
                    
                    # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
                    if (len(device_status['running_tasks']) < device_status['max_concurrent_tasks'] and
                        device_status['available_memory_gb'] * 1024 > memory_requirement):
                        return 'gpu', device_id
            
            # æ£€æŸ¥CPUæ˜¯å¦å¯ç”¨
            cpu_status = self.device_status['cpu']
            if (cpu_status['used_slots'] < cpu_status['available_slots'] and
                cpu_status['available_memory_gb'] * 1024 > memory_requirement):
                return 'cpu', None
            
            return None, None
            
        except Exception as e:
            logger.error(f"è®¾å¤‡é€‰æ‹©é”™è¯¯: {e}")
            return None, None
    
    def _execute_task(self, task: ModelTask, device_type: str, device_id: Optional[int]):
        """æ‰§è¡Œä»»åŠ¡"""
        try:
            task.status = 'running'
            task.start_time = time.time()
            task.device_id = device_id
            
            # æ›´æ–°è®¾å¤‡çŠ¶æ€
            if device_type == 'gpu':
                device_key = f'gpu_{device_id}'
                self.device_status[device_key]['running_tasks'].append(task.task_id)
                self.used_gpu_slots += 1
            else:
                self.device_status['cpu']['used_slots'] += 1
                self.device_status['cpu']['running_tasks'].append(task.task_id)
                self.used_cpu_slots += 1
            
            # æ·»åŠ åˆ°è¿è¡Œä»»åŠ¡åˆ—è¡¨
            self.running_tasks[task.task_id] = task
            
            # é€‰æ‹©æ‰§è¡Œå™¨
            executor = self.gpu_executor if device_type == 'gpu' else self.cpu_executor
            
            # æäº¤ä»»åŠ¡æ‰§è¡Œ
            future = executor.submit(self._run_model_training, task, device_type, device_id)
            
            # è®¾ç½®å®Œæˆå›è°ƒ
            future.add_done_callback(lambda f: self._task_completed(task, f))
            
            logger.info(f"ğŸ¯ ä»»åŠ¡å¼€å§‹æ‰§è¡Œ - {task.task_id} ({task.model_type}) on {device_type}_{device_id or 'cpu'}")
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œé”™è¯¯: {e}")
            self._task_failed(task, str(e))
    
    def _run_model_training(self, task: ModelTask, device_type: str, device_id: Optional[int]) -> Any:
        """è¿è¡Œæ¨¡å‹è®­ç»ƒ"""
        try:
            model_config = self.model_configs.get(task.model_type, {})
            training_time = model_config.get('training_time_estimate', 120)
            
            # è®¾ç½®è®¾å¤‡
            if device_type == 'gpu' and device_id is not None:
                torch.cuda.set_device(device_id)
                device = torch.device(f'cuda:{device_id}')
            else:
                device = torch.device('cpu')
            
            logger.info(f"ğŸ§  å¼€å§‹è®­ç»ƒ {task.model_type} æ¨¡å‹ - è®¾å¤‡: {device}")
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼ˆå®é™…å®ç°ä¸­ä¼šè°ƒç”¨å…·ä½“çš„AIæ¨¡å‹è®­ç»ƒä»£ç ï¼‰
            result = self._simulate_model_training(task, device, training_time)
            
            logger.success(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ - {task.task_id}")
            return result
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è®­ç»ƒé”™è¯¯: {e}")
            raise e
    
    def _simulate_model_training(self, task: ModelTask, device: torch.device, training_time: int) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿæ¨¡å‹è®­ç»ƒï¼ˆå®é™…å®ç°ä¸­æ›¿æ¢ä¸ºçœŸå®è®­ç»ƒä»£ç ï¼‰"""
        try:
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
            start_time = time.time()
            
            # åˆ†é˜¶æ®µæ¨¡æ‹Ÿè®­ç»ƒ
            phases = ['æ•°æ®åŠ è½½', 'æ¨¡å‹åˆå§‹åŒ–', 'è®­ç»ƒå¾ªç¯', 'éªŒè¯è¯„ä¼°', 'æ¨¡å‹ä¿å­˜']
            phase_time = training_time / len(phases)
            
            results = {
                'model_type': task.model_type,
                'training_phases': [],
                'final_metrics': {},
                'device_used': str(device)
            }
            
            for i, phase in enumerate(phases):
                phase_start = time.time()
                
                # æ¨¡æ‹Ÿè¯¥é˜¶æ®µçš„å·¥ä½œ
                time.sleep(min(phase_time, 30))  # æœ€å¤š30ç§’ä¸€ä¸ªé˜¶æ®µ
                
                phase_end = time.time()
                phase_result = {
                    'phase': phase,
                    'duration': phase_end - phase_start,
                    'progress': (i + 1) / len(phases)
                }
                results['training_phases'].append(phase_result)
                
                logger.info(f"ğŸ“Š {task.task_id} - {phase} å®Œæˆ ({(i+1)/len(phases)*100:.1f}%)")
            
            # æ¨¡æ‹Ÿæœ€ç»ˆæŒ‡æ ‡
            results['final_metrics'] = {
                'accuracy': np.random.uniform(0.7, 0.95),
                'loss': np.random.uniform(0.05, 0.3),
                'training_time': time.time() - start_time,
                'convergence_epoch': np.random.randint(10, 100)
            }
            
            return results
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è®­ç»ƒæ¨¡æ‹Ÿé”™è¯¯: {e}")
            raise e
    
    def _task_completed(self, task: ModelTask, future):
        """ä»»åŠ¡å®Œæˆå›è°ƒ"""
        try:
            task.end_time = time.time()
            
            if future.exception():
                # ä»»åŠ¡å¤±è´¥
                task.status = 'failed'
                task.error = str(future.exception())
                self.failed_tasks[task.task_id] = task
                logger.error(f"âŒ ä»»åŠ¡å¤±è´¥ - {task.task_id}: {task.error}")
            else:
                # ä»»åŠ¡æˆåŠŸ
                task.status = 'completed'
                task.result = future.result()
                self.completed_tasks[task.task_id] = task
                logger.success(f"âœ… ä»»åŠ¡å®Œæˆ - {task.task_id} (è€—æ—¶: {task.end_time - task.start_time:.1f}s)")
                
                # æ‰§è¡Œå›è°ƒ
                if task.callback:
                    try:
                        task.callback(task)
                    except Exception as e:
                        logger.error(f"ä»»åŠ¡å›è°ƒé”™è¯¯: {e}")
            
            # é‡Šæ”¾èµ„æº
            self._release_task_resources(task)
            
            # ä»è¿è¡Œä»»åŠ¡ä¸­ç§»é™¤
            if task.task_id in self.running_tasks:
                del self.running_tasks[task.task_id]
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡å®Œæˆå¤„ç†é”™è¯¯: {e}")
    
    def _task_failed(self, task: ModelTask, error_msg: str):
        """ä»»åŠ¡å¤±è´¥å¤„ç†"""
        task.status = 'failed'
        task.error = error_msg
        task.end_time = time.time()
        self.failed_tasks[task.task_id] = task
        
        # é‡Šæ”¾èµ„æº
        self._release_task_resources(task)
        
        # ä»è¿è¡Œä»»åŠ¡ä¸­ç§»é™¤
        if task.task_id in self.running_tasks:
            del self.running_tasks[task.task_id]
        
        logger.error(f"âŒ ä»»åŠ¡å¤±è´¥ - {task.task_id}: {error_msg}")
    
    def _release_task_resources(self, task: ModelTask):
        """é‡Šæ”¾ä»»åŠ¡èµ„æº"""
        try:
            if task.device_id is not None:
                # GPUä»»åŠ¡
                device_key = f'gpu_{task.device_id}'
                if task.task_id in self.device_status[device_key]['running_tasks']:
                    self.device_status[device_key]['running_tasks'].remove(task.task_id)
                self.used_gpu_slots = max(0, self.used_gpu_slots - 1)
            else:
                # CPUä»»åŠ¡
                if task.task_id in self.device_status['cpu']['running_tasks']:
                    self.device_status['cpu']['running_tasks'].remove(task.task_id)
                self.device_status['cpu']['used_slots'] = max(0, self.device_status['cpu']['used_slots'] - 1)
                self.used_cpu_slots = max(0, self.used_cpu_slots - 1)
            
        except Exception as e:
            logger.error(f"èµ„æºé‡Šæ”¾é”™è¯¯: {e}")
    
    def _update_device_status(self):
        """æ›´æ–°è®¾å¤‡çŠ¶æ€"""
        try:
            # æ›´æ–°CPUçŠ¶æ€
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            
            self.device_status['cpu'].update({
                'utilization': cpu_percent,
                'available_memory_gb': memory.available / 1024**3
            })
            
            # æ›´æ–°GPUçŠ¶æ€
            if self.gpu_available:
                try:
                    gpus = GPUtil.getGPUs()
                    for device_id in range(min(len(gpus), self.device_count)):
                        gpu = gpus[device_id]
                        device_key = f'gpu_{device_id}'
                        
                        self.device_status[device_key].update({
                            'utilization': gpu.load * 100,
                            'temperature': gpu.temperature,
                            'available_memory_gb': (gpu.memoryTotal - gpu.memoryUsed) / 1024
                        })
                except:
                    pass
            
        except Exception as e:
            logger.error(f"è®¾å¤‡çŠ¶æ€æ›´æ–°é”™è¯¯: {e}")
    
    def _check_task_timeouts(self):
        """æ£€æŸ¥ä»»åŠ¡è¶…æ—¶"""
        try:
            current_time = time.time()
            timeout_threshold = 1800  # 30åˆ†é’Ÿè¶…æ—¶
            
            for task_id, task in list(self.running_tasks.items()):
                if task.start_time and (current_time - task.start_time) > timeout_threshold:
                    logger.warning(f"â° ä»»åŠ¡è¶…æ—¶ - {task_id}")
                    self._task_failed(task, "ä»»åŠ¡æ‰§è¡Œè¶…æ—¶")
                    
        except Exception as e:
            logger.error(f"è¶…æ—¶æ£€æŸ¥é”™è¯¯: {e}")
    
    def _cleanup_completed_tasks(self):
        """æ¸…ç†å®Œæˆçš„ä»»åŠ¡"""
        try:
            # ä¿æŒå®Œæˆä»»åŠ¡å†å²åœ¨åˆç†èŒƒå›´å†…
            if len(self.completed_tasks) > 100:
                # ç§»é™¤æœ€æ—§çš„ä»»åŠ¡
                oldest_tasks = sorted(self.completed_tasks.items(), 
                                    key=lambda x: x[1].end_time or 0)
                for task_id, _ in oldest_tasks[:50]:
                    del self.completed_tasks[task_id]
            
            # æ¸…ç†å¤±è´¥ä»»åŠ¡
            if len(self.failed_tasks) > 50:
                oldest_failed = sorted(self.failed_tasks.items(), 
                                     key=lambda x: x[1].end_time or 0)
                for task_id, _ in oldest_failed[:25]:
                    del self.failed_tasks[task_id]
                    
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ¸…ç†é”™è¯¯: {e}")
    
    def _update_scheduler_stats(self):
        """æ›´æ–°è°ƒåº¦ç»Ÿè®¡"""
        try:
            total_completed = len(self.completed_tasks)
            total_failed = len(self.failed_tasks)
            
            self.scheduler_stats.update({
                'total_tasks': total_completed + total_failed + len(self.running_tasks),
                'completed_tasks': total_completed,
                'failed_tasks': total_failed,
                'running_tasks': len(self.running_tasks),
                'pending_tasks': self.task_queue.qsize()
            })
            
            # è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´
            if self.completed_tasks:
                total_time = sum((task.end_time - task.start_time) 
                               for task in self.completed_tasks.values() 
                               if task.start_time and task.end_time)
                self.scheduler_stats['average_execution_time'] = total_time / len(self.completed_tasks)
            
            # è®¡ç®—èµ„æºåˆ©ç”¨ç‡
            if self.gpu_available:
                gpu_utilization = sum(self.device_status[f'gpu_{i}']['utilization'] 
                                    for i in range(self.device_count)) / self.device_count
                self.scheduler_stats['gpu_utilization'] = gpu_utilization
            
            self.scheduler_stats['cpu_utilization'] = self.device_status['cpu']['utilization']
            
        except Exception as e:
            logger.error(f"ç»Ÿè®¡æ›´æ–°é”™è¯¯: {e}")
    
    def _log_scheduler_status(self):
        """è®°å½•è°ƒåº¦å™¨çŠ¶æ€"""
        try:
            if int(time.time()) % 60 == 0:  # æ¯åˆ†é’Ÿè®°å½•ä¸€æ¬¡
                stats = self.scheduler_stats
                logger.info(f"ğŸ¯ è°ƒåº¦å™¨çŠ¶æ€ - è¿è¡Œ: {stats['running_tasks']}, "
                          f"å®Œæˆ: {stats['completed_tasks']}, "
                          f"å¤±è´¥: {stats['failed_tasks']}, "
                          f"ç­‰å¾…: {stats['pending_tasks']}")
                
        except Exception as e:
            logger.error(f"çŠ¶æ€è®°å½•é”™è¯¯: {e}")
    
    def submit_task(self, model_type: str, priority: int = 1, 
                   config: Dict = None, callback: Callable = None) -> str:
        """æäº¤è®­ç»ƒä»»åŠ¡"""
        try:
            task_id = f"{model_type}_{int(time.time())}_{np.random.randint(1000, 9999)}"
            task = ModelTask(task_id, model_type, priority, config, callback)
            
            # æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—
            self.task_queue.put((priority, task))
            
            logger.info(f"ğŸ“ ä»»åŠ¡æäº¤æˆåŠŸ - {task_id} ({model_type}), ä¼˜å…ˆçº§: {priority}")
            return task_id
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡æäº¤é”™è¯¯: {e}")
            return None
    
    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        try:
            # æ£€æŸ¥è¿è¡Œä¸­ä»»åŠ¡
            if task_id in self.running_tasks:
                task = self.running_tasks[task_id]
                return {
                    'task_id': task.task_id,
                    'status': task.status,
                    'model_type': task.model_type,
                    'device_id': task.device_id,
                    'start_time': task.start_time,
                    'running_time': time.time() - task.start_time if task.start_time else 0
                }
            
            # æ£€æŸ¥å®Œæˆä»»åŠ¡
            if task_id in self.completed_tasks:
                task = self.completed_tasks[task_id]
                return {
                    'task_id': task.task_id,
                    'status': task.status,
                    'model_type': task.model_type,
                    'result': task.result,
                    'execution_time': task.end_time - task.start_time if task.start_time and task.end_time else 0
                }
            
            # æ£€æŸ¥å¤±è´¥ä»»åŠ¡
            if task_id in self.failed_tasks:
                task = self.failed_tasks[task_id]
                return {
                    'task_id': task.task_id,
                    'status': task.status,
                    'model_type': task.model_type,
                    'error': task.error,
                    'execution_time': task.end_time - task.start_time if task.start_time and task.end_time else 0
                }
            
            return None
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢é”™è¯¯: {e}")
            return None
    
    def get_scheduler_status(self) -> Dict[str, Any]:
        """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
        return {
            'scheduler_stats': self.scheduler_stats.copy(),
            'device_status': self.device_status.copy(),
            'resource_usage': {
                'cpu_slots_used': self.used_cpu_slots,
                'cpu_slots_total': self.cpu_slots,
                'gpu_slots_used': self.used_gpu_slots,
                'gpu_slots_total': self.gpu_slots
            },
            'queue_status': {
                'pending_tasks': self.task_queue.qsize(),
                'running_tasks': len(self.running_tasks),
                'completed_tasks': len(self.completed_tasks),
                'failed_tasks': len(self.failed_tasks)
            }
        }
    
    def stop_scheduler(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.is_running = False
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.scheduler_thread and self.scheduler_thread.is_alive():
            self.scheduler_thread.join(timeout=10)
        
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=10)
        
        # å…³é—­çº¿ç¨‹æ± 
        self.cpu_executor.shutdown(wait=True)
        self.gpu_executor.shutdown(wait=True)
        
        logger.info("ğŸ›‘ GPUæ¨¡å‹è°ƒåº¦å™¨å·²åœæ­¢")

# å…¨å±€æ¨¡å‹è°ƒåº¦å™¨å®ä¾‹
_gpu_model_scheduler = None

def initialize_gpu_model_scheduler() -> ProductionGPUModelScheduler:
    """åˆå§‹åŒ–GPUæ¨¡å‹è°ƒåº¦å™¨"""
    global _gpu_model_scheduler
    
    if _gpu_model_scheduler is None:
        _gpu_model_scheduler = ProductionGPUModelScheduler()
        _gpu_model_scheduler.start_scheduler()
        logger.success("âœ… GPUæ¨¡å‹è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
    
    return _gpu_model_scheduler

def get_gpu_model_scheduler() -> Optional[ProductionGPUModelScheduler]:
    """è·å–GPUæ¨¡å‹è°ƒåº¦å™¨å®ä¾‹"""
    return _gpu_model_scheduler

if __name__ == "__main__":
    # æµ‹è¯•GPUæ¨¡å‹è°ƒåº¦å™¨
    scheduler = initialize_gpu_model_scheduler()
    
    # æäº¤æµ‹è¯•ä»»åŠ¡
    task_ids = []
    for model_type in ['reinforcement_learning', 'deep_learning', 'ensemble_learning']:
        task_id = scheduler.submit_task(model_type, priority=5)
        task_ids.append(task_id)
    
    # ç›‘æ§ä»»åŠ¡çŠ¶æ€
    for i in range(30):
        status = scheduler.get_scheduler_status()
        print(f"è°ƒåº¦å™¨çŠ¶æ€: {status['queue_status']}")
        time.sleep(10)
    
    scheduler.stop_scheduler()
