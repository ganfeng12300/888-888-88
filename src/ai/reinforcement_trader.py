"""
ğŸ¦Š çŒç‹AIé‡åŒ–äº¤æ˜“ç³»ç»Ÿï¼ˆå²è¯—çº§ï¼‰- å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜
GPUåŠ é€Ÿçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ äº¤æ˜“æ™ºèƒ½ä½“ï¼Œå®ç°PPOç®—æ³•

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å®æ—¶å¸‚åœºç¯å¢ƒå»ºæ¨¡
2. Actor-CriticåŒç½‘ç»œæ¶æ„  
3. GPUåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†
4. ç»éªŒå›æ”¾å’Œæ¢ç´¢ç­–ç•¥
5. åŠ¨æ€å¥–åŠ±å‡½æ•°ä¼˜åŒ–
"""

import time
import threading
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
from loguru import logger
import json
from pathlib import Path
from collections import deque
import random


@dataclass
class TradingAction:
    """äº¤æ˜“åŠ¨ä½œ"""
    action_type: str  # "buy", "sell", "hold"
    position_size: float  # ä»“ä½å¤§å° 0-1
    confidence: float  # ç½®ä¿¡åº¦ 0-1
    reasoning: str  # å†³ç­–ç†ç”±
    timestamp: datetime


class ActorNetwork(nn.Module):
    """Actorç½‘ç»œ - ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int = 50, hidden_dim: int = 256):
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )
        
        # åŠ¨ä½œç±»å‹è¾“å‡º (3ä¸ªåŠ¨ä½œ: hold, buy, sell)
        self.action_head = nn.Linear(hidden_dim // 2, 3)
        
        # ä»“ä½å¤§å°è¾“å‡º
        self.position_head = nn.Linear(hidden_dim // 2, 1)
        
    def forward(self, state):
        features = self.network(state)
        
        # åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        action_logits = self.action_head(features)
        action_probs = F.softmax(action_logits, dim=-1)
        
        # ä»“ä½å¤§å° (0-1)
        position_size = torch.sigmoid(self.position_head(features))
        
        return action_probs, position_size


class CriticNetwork(nn.Module):
    """Criticç½‘ç»œ - ä»·å€¼ç½‘ç»œ"""
    
    def __init__(self, state_dim: int = 50, hidden_dim: int = 256):
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
    def forward(self, state):
        return self.network(state)


class ReinforcementTrader:
    """å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜"""
    
    def __init__(self, data_dir: str = "data/rl_trader"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾å¤‡é…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸ® å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ç½‘ç»œæ¶æ„
        self.state_dim = 50
        self.actor = ActorNetwork(self.state_dim).to(self.device)
        self.critic = CriticNetwork(self.state_dim).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)
        
        # PPOå‚æ•°
        self.clip_epsilon = 0.2
        self.gamma = 0.99
        self.gae_lambda = 0.95
        self.ppo_epochs = 10
        self.batch_size = 64
        
        # ç»éªŒç¼“å†²åŒº
        self.memory = deque(maxlen=10000)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_steps = 0
        self.total_rewards = 0
        self.episode_rewards = deque(maxlen=100)
        
        # æ¢ç´¢å‚æ•°
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        
        # çº¿ç¨‹é”
        self.lock = threading.RLock()
        
        # åŠ è½½æ¨¡å‹
        self._load_models()
        
        logger.info("ğŸ¤– å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜åˆå§‹åŒ–å®Œæˆ")
        
    def get_trading_action(self, market_state: Dict[str, float]) -> TradingAction:
        """è·å–äº¤æ˜“åŠ¨ä½œ"""
        with self.lock:
            # å‡†å¤‡çŠ¶æ€
            state = self._prepare_state(market_state)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            # ç½‘ç»œæ¨ç†
            with torch.no_grad():
                self.actor.eval()
                action_probs, position_size = self.actor(state_tensor)
                
                # é€‰æ‹©åŠ¨ä½œ
                if np.random.random() < self.epsilon:
                    # æ¢ç´¢ï¼šéšæœºé€‰æ‹©
                    action_idx = np.random.randint(0, 3)
                    position_val = np.random.random()
                else:
                    # åˆ©ç”¨ï¼šæ ¹æ®ç­–ç•¥é€‰æ‹©
                    action_idx = torch.multinomial(action_probs, 1).item()
                    position_val = position_size.item()
                
                # è®¡ç®—ç½®ä¿¡åº¦
                confidence = float(action_probs[0, action_idx].item())
            
            # è½¬æ¢ä¸ºäº¤æ˜“åŠ¨ä½œ
            action_types = ["hold", "buy", "sell"]
            action = TradingAction(
                action_type=action_types[action_idx],
                position_size=position_val,
                confidence=confidence,
                reasoning=f"RLç­–ç•¥: {action_types[action_idx]} ä»“ä½={position_val:.3f}",
                timestamp=datetime.now()
            )
            
            logger.debug(f"ğŸ¯ RLäº¤æ˜“åŠ¨ä½œ: {action.action_type} ä»“ä½={action.position_size:.3f} ç½®ä¿¡åº¦={action.confidence:.3f}")
            
            return action
    
    def update_experience(self, state: Dict, action: TradingAction, reward: float, next_state: Dict, done: bool):
        """æ›´æ–°ç»éªŒ"""
        with self.lock:
            # å­˜å‚¨ç»éªŒ
            experience = {
                "state": self._prepare_state(state),
                "action": self._action_to_vector(action),
                "reward": reward,
                "next_state": self._prepare_state(next_state),
                "done": done,
                "timestamp": datetime.now()
            }
            
            self.memory.append(experience)
            self.total_steps += 1
            self.total_rewards += reward
            
            # è§¦å‘è®­ç»ƒ
            if len(self.memory) >= self.batch_size and self.total_steps % 100 == 0:
                self._train_model()
            
            # è¡°å‡æ¢ç´¢ç‡
            if self.epsilon > self.epsilon_min:
                self.epsilon *= self.epsilon_decay
                
            logger.debug(f"ğŸ“Š ç»éªŒæ›´æ–°: å¥–åŠ±={reward:.4f} æ¢ç´¢ç‡={self.epsilon:.3f}")
    
    def _prepare_state(self, market_state: Dict[str, float]) -> np.array:
        """å‡†å¤‡çŠ¶æ€å‘é‡"""
        state = np.zeros(self.state_dim)
        
        # åŸºç¡€å¸‚åœºæ•°æ® (0-4)
        state[0] = market_state.get("price", 50000) / 100000  # æ ‡å‡†åŒ–ä»·æ ¼
        state[1] = market_state.get("volume", 0) / 1000000    # æ ‡å‡†åŒ–æˆäº¤é‡
        state[2] = market_state.get("volatility", 0)          # æ³¢åŠ¨ç‡
        state[3] = market_state.get("bid_ask_spread", 0)      # ä¹°å–ä»·å·®
        state[4] = market_state.get("market_depth", 0)        # å¸‚åœºæ·±åº¦
        
        # æŠ€æœ¯æŒ‡æ ‡ (5-34)
        indicators = [
            "rsi", "macd", "bb_upper", "bb_lower", "sma_5", "sma_20",
            "ema_12", "ema_26", "atr", "obv", "stoch_k", "stoch_d",
            "williams_r", "cci", "momentum", "roc", "adx", "aroon_up",
            "aroon_down", "mfi", "bop", "vwap", "pivot_point", "support_1",
            "resistance_1", "fibonacci_38", "fibonacci_62", "ichimoku_tenkan",
            "ichimoku_kijun", "parabolic_sar"
        ]
        
        for i, indicator in enumerate(indicators):
            if i + 5 < self.state_dim:
                value = market_state.get(indicator, 0)
                # ç®€å•æ ‡å‡†åŒ–
                if abs(value) > 1000:
                    value = value / 1000
                elif abs(value) > 100:
                    value = value / 100
                elif abs(value) > 10:
                    value = value / 10
                state[i + 5] = value
        
        # æ—¶é—´ç‰¹å¾ (35-39)
        now = datetime.now()
        state[35] = now.hour / 24.0
        state[36] = now.minute / 60.0
        state[37] = now.weekday() / 7.0
        state[38] = now.day / 31.0
        state[39] = now.month / 12.0
        
        # ç»„åˆçŠ¶æ€ (40-49)
        state[40] = market_state.get("portfolio_value", 100000) / 100000
        state[41] = market_state.get("position", 0)
        state[42] = market_state.get("unrealized_pnl", 0) / 10000
        state[43] = market_state.get("cash_balance", 100000) / 100000
        state[44] = market_state.get("total_trades", 0) / 1000
        state[45] = market_state.get("win_rate", 0.5)
        state[46] = market_state.get("sharpe_ratio", 0)
        state[47] = market_state.get("max_drawdown", 0)
        state[48] = market_state.get("profit_factor", 1.0)
        state[49] = market_state.get("avg_trade_return", 0)
        
        return state.astype(np.float32)
    
    def _action_to_vector(self, action: TradingAction) -> np.array:
        """å°†åŠ¨ä½œè½¬æ¢ä¸ºå‘é‡"""
        action_types = {"hold": 0, "buy": 1, "sell": 2}
        return np.array([
            action_types.get(action.action_type, 0),
            action.position_size
        ], dtype=np.float32)
    
    def _train_model(self):
        """è®­ç»ƒPPOæ¨¡å‹"""
        if len(self.memory) < self.batch_size:
            return
            
        try:
            self.actor.train()
            self.critic.train()
            
            # é‡‡æ ·æ‰¹æ¬¡æ•°æ®
            batch = random.sample(list(self.memory), self.batch_size)
            
            states = torch.FloatTensor([exp["state"] for exp in batch]).to(self.device)
            actions = torch.LongTensor([exp["action"][0] for exp in batch]).to(self.device)
            position_sizes = torch.FloatTensor([exp["action"][1] for exp in batch]).to(self.device)
            rewards = torch.FloatTensor([exp["reward"] for exp in batch]).to(self.device)
            next_states = torch.FloatTensor([exp["next_state"] for exp in batch]).to(self.device)
            dones = torch.BoolTensor([exp["done"] for exp in batch]).to(self.device)
            
            # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
            with torch.no_grad():
                next_values = self.critic(next_states).squeeze()
                current_values = self.critic(states).squeeze()
                
                # TDç›®æ ‡
                td_targets = rewards + self.gamma * next_values * (~dones)
                advantages = td_targets - current_values
                
                # æ ‡å‡†åŒ–ä¼˜åŠ¿
                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            # è·å–æ—§ç­–ç•¥æ¦‚ç‡
            with torch.no_grad():
                old_action_probs, _ = self.actor(states)
                old_probs = old_action_probs.gather(1, actions.unsqueeze(1)).squeeze()
            
            # PPOè®­ç»ƒå¾ªç¯
            for _ in range(self.ppo_epochs):
                # å½“å‰ç­–ç•¥
                action_probs, pred_position_sizes = self.actor(states)
                current_probs = action_probs.gather(1, actions.unsqueeze(1)).squeeze()
                
                # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
                ratio = current_probs / (old_probs + 1e-8)
                
                # PPOæŸå¤±
                surr1 = ratio * advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
                actor_loss = -torch.min(surr1, surr2).mean()
                
                # ä»“ä½å¤§å°æŸå¤±
                position_loss = F.mse_loss(pred_position_sizes.squeeze(), position_sizes)
                
                # æ€»ActoræŸå¤±
                total_actor_loss = actor_loss + position_loss * 0.1
                
                # CriticæŸå¤±
                values = self.critic(states).squeeze()
                critic_loss = F.mse_loss(values, td_targets)
                
                # åå‘ä¼ æ’­
                self.actor_optimizer.zero_grad()
                total_actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
                self.actor_optimizer.step()
                
                self.critic_optimizer.zero_grad()
                critic_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
                self.critic_optimizer.step()
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if self.total_steps % 1000 == 0:
                self._save_models()
                
            logger.debug(f"ğŸ“ PPOè®­ç»ƒå®Œæˆ: ActoræŸå¤±={total_actor_loss.item():.4f} CriticæŸå¤±={critic_loss.item():.4f}")
            
        except Exception as e:
            logger.error(f"âŒ PPOè®­ç»ƒå¤±è´¥: {e}")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        with self.lock:
            avg_reward = np.mean(self.episode_rewards) if self.episode_rewards else 0
            
            return {
                "agent_type": "reinforcement_trader",
                "total_steps": self.total_steps,
                "total_rewards": self.total_rewards,
                "avg_episode_reward": avg_reward,
                "epsilon": self.epsilon,
                "memory_size": len(self.memory),
                "device": str(self.device),
                "model_parameters": {
                    "actor_params": sum(p.numel() for p in self.actor.parameters()),
                    "critic_params": sum(p.numel() for p in self.critic.parameters())
                }
            }
    
    def _save_models(self):
        """ä¿å­˜æ¨¡å‹"""
        try:
            torch.save({
                "actor_state_dict": self.actor.state_dict(),
                "critic_state_dict": self.critic.state_dict(),
                "actor_optimizer": self.actor_optimizer.state_dict(),
                "critic_optimizer": self.critic_optimizer.state_dict(),
                "total_steps": self.total_steps,
                "epsilon": self.epsilon,
                "total_rewards": self.total_rewards
            }, self.data_dir / "rl_trader_model.pth")
            
            logger.debug("ğŸ’¾ RLæ¨¡å‹ä¿å­˜å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜RLæ¨¡å‹å¤±è´¥: {e}")
    
    def _load_models(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            model_path = self.data_dir / "rl_trader_model.pth"
            if model_path.exists():
                checkpoint = torch.load(model_path, map_location=self.device)
                
                self.actor.load_state_dict(checkpoint["actor_state_dict"])
                self.critic.load_state_dict(checkpoint["critic_state_dict"])
                self.actor_optimizer.load_state_dict(checkpoint["actor_optimizer"])
                self.critic_optimizer.load_state_dict(checkpoint["critic_optimizer"])
                self.total_steps = checkpoint.get("total_steps", 0)
                self.epsilon = checkpoint.get("epsilon", 1.0)
                self.total_rewards = checkpoint.get("total_rewards", 0)
                
                logger.info("ğŸ“¥ RLæ¨¡å‹åŠ è½½å®Œæˆ")
                
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½RLæ¨¡å‹å¤±è´¥: {e}")


# å…¨å±€å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜å®ä¾‹
rl_trader = ReinforcementTrader()


if __name__ == "__main__":
    # æµ‹è¯•å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜
    logger.info("ğŸ§ª æµ‹è¯•å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜...")
    
    # æ¨¡æ‹Ÿå¸‚åœºçŠ¶æ€
    market_state = {
        "price": 50000,
        "volume": 1000000,
        "volatility": 0.02,
        "rsi": 65,
        "macd": 100,
        "portfolio_value": 100000,
        "position": 0,
        "unrealized_pnl": 0
    }
    
    # è·å–äº¤æ˜“åŠ¨ä½œ
    action = rl_trader.get_trading_action(market_state)
    logger.info(f"ğŸ¯ äº¤æ˜“åŠ¨ä½œ: {action}")
    
    # æ¨¡æ‹Ÿç»éªŒæ›´æ–°
    rl_trader.update_experience(
        state=market_state,
        action=action,
        reward=0.01,
        next_state=market_state,
        done=False
    )
    
    # è·å–æ€§èƒ½æŠ¥å‘Š
    report = rl_trader.get_performance_report()
    logger.info(f"ğŸ“Š æ€§èƒ½æŠ¥å‘Š: {report}")
