#!/usr/bin/env python3
"""
ğŸ¯ å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜ - GPUåŠ é€Ÿè®­ç»ƒ
ä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ è¿›è¡Œå®ç›˜äº¤æ˜“å†³ç­–
ä¸“ä¸ºç”Ÿäº§çº§å®ç›˜äº¤æ˜“è®¾è®¡ï¼Œæ”¯æŒPPOã€A3Cã€SACç­‰ç®—æ³•
"""

import asyncio
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Normal, Categorical
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, timezone
import json
from dataclasses import dataclass
from loguru import logger
import threading
import time
from concurrent.futures import ThreadPoolExecutor
import gym
from gym import spaces
import collections
import random
import pickle
import os

@dataclass
class TradingState:
    """äº¤æ˜“çŠ¶æ€"""
    price: float
    volume: float
    rsi: float
    macd: float
    bb_upper: float
    bb_lower: float
    bb_middle: float
    atr: float
    volume_sma: float
    price_change: float
    volatility: float
    sentiment: float
    news_impact: float
    time_of_day: float
    day_of_week: float
    position: float  # å½“å‰ä»“ä½ -1åˆ°1
    unrealized_pnl: float
    account_balance: float
    drawdown: float
    trades_today: int
    win_rate: float

@dataclass
class TradingAction:
    """äº¤æ˜“åŠ¨ä½œ"""
    action_type: str  # 'buy', 'sell', 'hold'
    position_size: float  # 0åˆ°1ä¹‹é—´çš„ä»“ä½å¤§å°
    confidence: float  # åŠ¨ä½œç½®ä¿¡åº¦
    stop_loss: float  # æ­¢æŸä»·æ ¼
    take_profit: float  # æ­¢ç›ˆä»·æ ¼

@dataclass
class Experience:
    """ç»éªŒå›æ”¾æ•°æ®"""
    state: np.ndarray
    action: int
    reward: float
    next_state: np.ndarray
    done: bool
    log_prob: float
    value: float

class TradingEnvironment:
    """äº¤æ˜“ç¯å¢ƒ"""
    
    def __init__(self, initial_balance: float = 100000.0):
        self.initial_balance = initial_balance
        self.current_balance = initial_balance
        self.position = 0.0  # å½“å‰ä»“ä½
        self.entry_price = 0.0
        self.max_position = 1.0  # æœ€å¤§ä»“ä½
        self.transaction_cost = 0.001  # äº¤æ˜“æˆæœ¬
        self.max_drawdown = 0.0
        self.peak_balance = initial_balance
        self.trades_count = 0
        self.winning_trades = 0
        self.current_step = 0
        self.max_steps = 1000
        
        # çŠ¶æ€ç©ºé—´ï¼š21ç»´
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(21,), dtype=np.float32
        )
        
        # åŠ¨ä½œç©ºé—´ï¼š3ä¸ªç¦»æ•£åŠ¨ä½œ (ä¹°å…¥ã€å–å‡ºã€æŒæœ‰)
        self.action_space = spaces.Discrete(3)
        
        self.state_history = collections.deque(maxlen=100)
        self.reward_history = collections.deque(maxlen=100)
        
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        self.current_balance = self.initial_balance
        self.position = 0.0
        self.entry_price = 0.0
        self.max_drawdown = 0.0
        self.peak_balance = self.initial_balance
        self.trades_count = 0
        self.winning_trades = 0
        self.current_step = 0
        
        # è¿”å›åˆå§‹çŠ¶æ€
        return self._get_current_state()
    
    def step(self, action: int, market_data: Dict[str, float]) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡ŒåŠ¨ä½œ"""
        self.current_step += 1
        
        # è§£æåŠ¨ä½œ
        action_type = ['buy', 'sell', 'hold'][action]
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(action_type, market_data)
        
        # æ‰§è¡Œäº¤æ˜“
        self._execute_trade(action_type, market_data)
        
        # æ›´æ–°çŠ¶æ€
        next_state = self._get_current_state(market_data)
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = self._is_done()
        
        # ä¿¡æ¯
        info = {
            'balance': self.current_balance,
            'position': self.position,
            'drawdown': self.max_drawdown,
            'trades': self.trades_count,
            'win_rate': self.winning_trades / max(self.trades_count, 1)
        }
        
        return next_state, reward, done, info
    
    def _get_current_state(self, market_data: Dict[str, float] = None) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€"""
        if market_data is None:
            market_data = {}
        
        state = np.array([
            market_data.get('price', 0.0) / 10000.0,  # æ ‡å‡†åŒ–ä»·æ ¼
            market_data.get('volume', 0.0) / 1000000.0,  # æ ‡å‡†åŒ–æˆäº¤é‡
            market_data.get('rsi', 50.0) / 100.0,  # RSI
            market_data.get('macd', 0.0),  # MACD
            market_data.get('bb_position', 0.5),  # å¸ƒæ—å¸¦ä½ç½®
            market_data.get('atr', 0.0) / 100.0,  # ATR
            market_data.get('price_change', 0.0),  # ä»·æ ¼å˜åŒ–
            market_data.get('volatility', 0.0),  # æ³¢åŠ¨ç‡
            market_data.get('sentiment', 0.0),  # æƒ…æ„Ÿåˆ†æ•°
            market_data.get('news_impact', 0.0),  # æ–°é—»å½±å“
            market_data.get('time_of_day', 0.5),  # æ—¶é—´
            market_data.get('day_of_week', 0.5),  # æ˜ŸæœŸ
            self.position,  # å½“å‰ä»“ä½
            (self.current_balance - self.initial_balance) / self.initial_balance,  # æ”¶ç›Šç‡
            self.max_drawdown,  # æœ€å¤§å›æ’¤
            self.trades_count / 100.0,  # äº¤æ˜“æ¬¡æ•°
            self.winning_trades / max(self.trades_count, 1),  # èƒœç‡
            min(self.current_step / self.max_steps, 1.0),  # è¿›åº¦
            market_data.get('support_level', 0.0) / 10000.0,  # æ”¯æ’‘ä½
            market_data.get('resistance_level', 0.0) / 10000.0,  # é˜»åŠ›ä½
            market_data.get('trend_strength', 0.0)  # è¶‹åŠ¿å¼ºåº¦
        ], dtype=np.float32)
        
        return state
    
    def _calculate_reward(self, action_type: str, market_data: Dict[str, float]) -> float:
        """è®¡ç®—å¥–åŠ±"""
        reward = 0.0
        
        # åŸºç¡€æ”¶ç›Šå¥–åŠ±
        if self.position != 0:
            price_change = market_data.get('price_change', 0.0)
            position_reward = self.position * price_change * 100  # æ”¾å¤§å¥–åŠ±
            reward += position_reward
        
        # äº¤æ˜“æˆæœ¬æƒ©ç½š
        if action_type in ['buy', 'sell'] and action_type != 'hold':
            reward -= self.transaction_cost * 10  # äº¤æ˜“æˆæœ¬æƒ©ç½š
        
        # é£é™©è°ƒæ•´å¥–åŠ±
        if self.max_drawdown > 0.1:  # å›æ’¤è¶…è¿‡10%
            reward -= 5.0
        elif self.max_drawdown > 0.05:  # å›æ’¤è¶…è¿‡5%
            reward -= 2.0
        
        # èƒœç‡å¥–åŠ±
        if self.trades_count > 10:
            win_rate = self.winning_trades / self.trades_count
            if win_rate > 0.6:
                reward += 2.0
            elif win_rate < 0.4:
                reward -= 1.0
        
        # æŒä»“æ—¶é—´å¥–åŠ±ï¼ˆé¿å…è¿‡åº¦äº¤æ˜“ï¼‰
        if action_type == 'hold' and abs(self.position) > 0.1:
            reward += 0.1
        
        return reward
    
    def _execute_trade(self, action_type: str, market_data: Dict[str, float]):
        """æ‰§è¡Œäº¤æ˜“"""
        current_price = market_data.get('price', 0.0)
        
        if action_type == 'buy' and self.position < self.max_position:
            # ä¹°å…¥
            trade_size = min(0.2, self.max_position - self.position)  # æ¯æ¬¡æœ€å¤šä¹°å…¥20%
            self.position += trade_size
            self.entry_price = current_price
            self.trades_count += 1
            
        elif action_type == 'sell' and self.position > -self.max_position:
            # å–å‡º
            if self.position > 0:
                # å¹³å¤šä»“
                pnl = (current_price - self.entry_price) * self.position
                self.current_balance += pnl * self.initial_balance
                if pnl > 0:
                    self.winning_trades += 1
            
            trade_size = min(0.2, self.position + self.max_position)  # æ¯æ¬¡æœ€å¤šå–å‡º20%
            self.position -= trade_size
            self.entry_price = current_price
            self.trades_count += 1
        
        # æ›´æ–°æœ€å¤§å›æ’¤
        self.peak_balance = max(self.peak_balance, self.current_balance)
        current_drawdown = (self.peak_balance - self.current_balance) / self.peak_balance
        self.max_drawdown = max(self.max_drawdown, current_drawdown)
    
    def _is_done(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦ç»“æŸ"""
        return (self.current_step >= self.max_steps or 
                self.max_drawdown > 0.2 or  # å›æ’¤è¶…è¿‡20%
                self.current_balance < self.initial_balance * 0.5)  # äºæŸè¶…è¿‡50%

class PPONetwork(nn.Module):
    """PPOç½‘ç»œ"""
    
    def __init__(self, state_dim: int = 21, action_dim: int = 3, hidden_dim: int = 256):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )
        
        # ç­–ç•¥ç½‘ç»œï¼ˆActorï¼‰
        self.policy_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Softmax(dim=-1)
        )
        
        # ä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        features = self.feature_extractor(state)
        policy = self.policy_head(features)
        value = self.value_head(features)
        return policy, value
    
    def get_action(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """è·å–åŠ¨ä½œ"""
        policy, value = self.forward(state)
        dist = Categorical(policy)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action, log_prob, value

class ReinforcementTrader:
    """å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜"""
    
    def __init__(self, device: str = None, model_path: str = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_path = model_path or "models/reinforcement_trader.pth"
        
        # åˆå§‹åŒ–ç¯å¢ƒå’Œç½‘ç»œ
        self.env = TradingEnvironment()
        self.network = PPONetwork().to(self.device)
        self.optimizer = optim.AdamW(self.network.parameters(), lr=3e-4, weight_decay=0.01)
        
        # PPOå‚æ•°
        self.gamma = 0.99  # æŠ˜æ‰£å› å­
        self.lambda_gae = 0.95  # GAEå‚æ•°
        self.clip_epsilon = 0.2  # PPOè£å‰ªå‚æ•°
        self.entropy_coef = 0.01  # ç†µç³»æ•°
        self.value_coef = 0.5  # ä»·å€¼æŸå¤±ç³»æ•°
        self.max_grad_norm = 0.5  # æ¢¯åº¦è£å‰ª
        
        # è®­ç»ƒå‚æ•°
        self.batch_size = 64
        self.update_epochs = 10
        self.buffer_size = 2048
        self.experience_buffer = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.training_stats = {
            'episodes': 0,
            'total_reward': 0.0,
            'avg_reward': 0.0,
            'win_rate': 0.0,
            'max_drawdown': 0.0,
            'sharpe_ratio': 0.0,
            'total_trades': 0,
            'profitable_trades': 0
        }
        
        # å®æ—¶äº¤æ˜“çŠ¶æ€
        self.current_position = 0.0
        self.last_action = 'hold'
        self.last_confidence = 0.0
        self.performance_score = 0.5
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if os.path.exists(self.model_path):
            self.load_model(self.model_path)
        
        logger.info(f"ğŸ¯ å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
    
    async def get_trading_signal(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–äº¤æ˜“ä¿¡å·"""
        try:
            # å‡†å¤‡çŠ¶æ€
            state = self.env._get_current_state(market_data)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            # è·å–åŠ¨ä½œ
            with torch.no_grad():
                action, log_prob, value = self.network.get_action(state_tensor)
                policy, _ = self.network(state_tensor)
                
                action_idx = int(action.cpu().numpy()[0])
                confidence = float(torch.max(policy).cpu().numpy())
                
            # è½¬æ¢ä¸ºäº¤æ˜“ä¿¡å·
            action_map = {0: 'buy', 1: 'sell', 2: 'hold'}
            signal_map = {'buy': 1.0, 'sell': -1.0, 'hold': 0.0}
            
            action_type = action_map[action_idx]
            signal_strength = signal_map[action_type]
            
            # è°ƒæ•´ä¿¡å·å¼ºåº¦åŸºäºç½®ä¿¡åº¦
            if action_type != 'hold':
                signal_strength *= confidence
            
            # æ›´æ–°çŠ¶æ€
            self.last_action = action_type
            self.last_confidence = confidence
            
            return {
                'signal': signal_strength,
                'confidence': confidence,
                'action': action_type,
                'position_size': min(confidence * 0.1, 0.05),  # æœ€å¤§5%ä»“ä½
                'reasoning': f"RLå†³ç­–: {action_type} (ç½®ä¿¡åº¦: {confidence:.3f})",
                'model_id': 'reinforcement_trader',
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ å¼ºåŒ–å­¦ä¹ ä¿¡å·ç”Ÿæˆå¤±è´¥: {e}")
            return {
                'signal': 0.0,
                'confidence': 0.1,
                'action': 'hold',
                'position_size': 0.0,
                'reasoning': f"é”™è¯¯: {str(e)}",
                'model_id': 'reinforcement_trader',
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
    
    async def train_episode(self, market_data_sequence: List[Dict[str, Any]]) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªå›åˆ"""
        try:
            # é‡ç½®ç¯å¢ƒ
            state = self.env.reset()
            episode_reward = 0.0
            episode_experiences = []
            
            for step, market_data in enumerate(market_data_sequence):
                # è·å–åŠ¨ä½œ
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                action, log_prob, value = self.network.get_action(state_tensor)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = self.env.step(int(action.cpu().numpy()[0]), market_data)
                
                # å­˜å‚¨ç»éªŒ
                experience = Experience(
                    state=state,
                    action=int(action.cpu().numpy()[0]),
                    reward=reward,
                    next_state=next_state,
                    done=done,
                    log_prob=float(log_prob.cpu().numpy()[0]),
                    value=float(value.cpu().numpy()[0])
                )
                episode_experiences.append(experience)
                
                episode_reward += reward
                state = next_state
                
                if done:
                    break
            
            # æ·»åŠ åˆ°ç»éªŒç¼“å†²åŒº
            self.experience_buffer.extend(episode_experiences)
            
            # é™åˆ¶ç¼“å†²åŒºå¤§å°
            if len(self.experience_buffer) > self.buffer_size:
                self.experience_buffer = self.experience_buffer[-self.buffer_size:]
            
            # æ›´æ–°ç½‘ç»œ
            if len(self.experience_buffer) >= self.batch_size:
                await self._update_network()
            
            # æ›´æ–°ç»Ÿè®¡
            self.training_stats['episodes'] += 1
            self.training_stats['total_reward'] += episode_reward
            self.training_stats['avg_reward'] = self.training_stats['total_reward'] / self.training_stats['episodes']
            
            # æ›´æ–°æ€§èƒ½åˆ†æ•°
            self.performance_score = min(max(self.training_stats['avg_reward'] / 100.0 + 0.5, 0.0), 1.0)
            
            return {
                'episode_reward': episode_reward,
                'episode_length': len(episode_experiences),
                'final_balance': info.get('balance', 0.0),
                'win_rate': info.get('win_rate', 0.0),
                'max_drawdown': info.get('drawdown', 0.0)
            }
            
        except Exception as e:
            logger.error(f"âŒ å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤±è´¥: {e}")
            return {'episode_reward': 0.0, 'episode_length': 0, 'error': str(e)}
    
    async def _update_network(self):
        """æ›´æ–°ç½‘ç»œ"""
        try:
            if len(self.experience_buffer) < self.batch_size:
                return
            
            # é‡‡æ ·æ‰¹æ¬¡
            batch = random.sample(self.experience_buffer, self.batch_size)
            
            # å‡†å¤‡æ•°æ®
            states = torch.FloatTensor([exp.state for exp in batch]).to(self.device)
            actions = torch.LongTensor([exp.action for exp in batch]).to(self.device)
            rewards = torch.FloatTensor([exp.reward for exp in batch]).to(self.device)
            next_states = torch.FloatTensor([exp.next_state for exp in batch]).to(self.device)
            dones = torch.BoolTensor([exp.done for exp in batch]).to(self.device)
            old_log_probs = torch.FloatTensor([exp.log_prob for exp in batch]).to(self.device)
            old_values = torch.FloatTensor([exp.value for exp in batch]).to(self.device)
            
            # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
            with torch.no_grad():
                _, next_values = self.network(next_states)
                next_values = next_values.squeeze(-1)
                
                # GAEè®¡ç®—
                advantages = torch.zeros_like(rewards)
                returns = torch.zeros_like(rewards)
                
                gae = 0
                for t in reversed(range(len(batch))):
                    if t == len(batch) - 1:
                        next_value = next_values[t] if not dones[t] else 0
                    else:
                        next_value = old_values[t + 1]
                    
                    delta = rewards[t] + self.gamma * next_value - old_values[t]
                    gae = delta + self.gamma * self.lambda_gae * gae
                    advantages[t] = gae
                    returns[t] = advantages[t] + old_values[t]
                
                # æ ‡å‡†åŒ–ä¼˜åŠ¿
                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            # PPOæ›´æ–°
            for _ in range(self.update_epochs):
                # å‰å‘ä¼ æ’­
                policies, values = self.network(states)
                values = values.squeeze(-1)
                
                # è®¡ç®—ç­–ç•¥æŸå¤±
                dist = Categorical(policies)
                new_log_probs = dist.log_prob(actions)
                entropy = dist.entropy().mean()
                
                ratio = torch.exp(new_log_probs - old_log_probs)
                surr1 = ratio * advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # è®¡ç®—ä»·å€¼æŸå¤±
                value_loss = F.mse_loss(values, returns)
                
                # æ€»æŸå¤±
                total_loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)
                self.optimizer.step()
            
            logger.debug(f"ğŸ¯ RLç½‘ç»œæ›´æ–°å®Œæˆ - ç­–ç•¥æŸå¤±: {policy_loss.item():.6f}, ä»·å€¼æŸå¤±: {value_loss.item():.6f}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ RLç½‘ç»œæ›´æ–°å¤±è´¥: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€"""
        return {
            'model_id': 'reinforcement_trader',
            'model_name': 'å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜',
            'device': self.device,
            'current_position': self.current_position,
            'last_action': self.last_action,
            'last_confidence': self.last_confidence,
            'performance_score': self.performance_score,
            'training_stats': self.training_stats,
            'buffer_size': len(self.experience_buffer),
            'network_parameters': sum(p.numel() for p in self.network.parameters()),
            'is_training': len(self.experience_buffer) >= self.batch_size
        }
    
    def save_model(self, filepath: str = None) -> bool:
        """ä¿å­˜æ¨¡å‹"""
        try:
            filepath = filepath or self.model_path
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            torch.save({
                'network_state_dict': self.network.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'training_stats': self.training_stats,
                'performance_score': self.performance_score,
                'hyperparameters': {
                    'gamma': self.gamma,
                    'lambda_gae': self.lambda_gae,
                    'clip_epsilon': self.clip_epsilon,
                    'entropy_coef': self.entropy_coef,
                    'value_coef': self.value_coef
                }
            }, filepath)
            
            logger.info(f"ğŸ’¾ å¼ºåŒ–å­¦ä¹ æ¨¡å‹å·²ä¿å­˜: {filepath}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def load_model(self, filepath: str = None) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            filepath = filepath or self.model_path
            if not os.path.exists(filepath):
                logger.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                return False
            
            checkpoint = torch.load(filepath, map_location=self.device)
            self.network.load_state_dict(checkpoint['network_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.training_stats = checkpoint.get('training_stats', self.training_stats)
            self.performance_score = checkpoint.get('performance_score', 0.5)
            
            # åŠ è½½è¶…å‚æ•°
            hyperparams = checkpoint.get('hyperparameters', {})
            self.gamma = hyperparams.get('gamma', self.gamma)
            self.lambda_gae = hyperparams.get('lambda_gae', self.lambda_gae)
            self.clip_epsilon = hyperparams.get('clip_epsilon', self.clip_epsilon)
            self.entropy_coef = hyperparams.get('entropy_coef', self.entropy_coef)
            self.value_coef = hyperparams.get('value_coef', self.value_coef)
            
            logger.info(f"ğŸ“‚ å¼ºåŒ–å­¦ä¹ æ¨¡å‹å·²åŠ è½½: {filepath}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

# å…¨å±€å®ä¾‹
reinforcement_trader = ReinforcementTrader()

def initialize_reinforcement_trader(device: str = None, model_path: str = None) -> ReinforcementTrader:
    """åˆå§‹åŒ–å¼ºåŒ–å­¦ä¹ äº¤æ˜“å‘˜"""
    global reinforcement_trader
    reinforcement_trader = ReinforcementTrader(device, model_path)
    return reinforcement_trader

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_reinforcement_trader():
        trader = initialize_reinforcement_trader()
        
        # æµ‹è¯•äº¤æ˜“ä¿¡å·
        market_data = {
            'price': 50000.0,
            'volume': 1000000.0,
            'rsi': 65.0,
            'macd': 0.1,
            'bb_position': 0.7,
            'price_change': 0.02,
            'volatility': 0.15,
            'sentiment': 0.3
        }
        
        signal = await trader.get_trading_signal(market_data)
        print(f"äº¤æ˜“ä¿¡å·: {signal}")
        
        # çŠ¶æ€æŠ¥å‘Š
        status = trader.get_status()
        print(f"çŠ¶æ€æŠ¥å‘Š: {json.dumps(status, indent=2, ensure_ascii=False)}")
    
    asyncio.run(test_reinforcement_trader())

