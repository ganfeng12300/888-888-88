#!/usr/bin/env python3
"""
ğŸ§  é›†æˆå­¦ä¹ æ™ºå›Šå›¢ - å¤šæ¨¡å‹æŠ•ç¥¨å†³ç­–
ä½¿ç”¨å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œé›†æˆé¢„æµ‹
ä¸“ä¸ºç”Ÿäº§çº§å®ç›˜äº¤æ˜“è®¾è®¡ï¼Œæ”¯æŒåŠ¨æ€æƒé‡è°ƒæ•´
"""

import asyncio
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, timezone
import json
from dataclasses import dataclass
from loguru import logger
import threading
import time
from concurrent.futures import ThreadPoolExecutor
import pandas as pd
from sklearn.ensemble import (
    RandomForestRegressor, GradientBoostingRegressor, 
    ExtraTreesRegressor, AdaBoostRegressor
)
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import joblib
import os

@dataclass
class ModelPrediction:
    """å•ä¸ªæ¨¡å‹é¢„æµ‹ç»“æœ"""
    model_name: str
    prediction: float
    confidence: float
    feature_importance: Dict[str, float]
    training_score: float
    validation_score: float
    prediction_time: float

@dataclass
class EnsemblePrediction:
    """é›†æˆé¢„æµ‹ç»“æœ"""
    final_prediction: float
    weighted_prediction: float
    voting_prediction: float
    confidence_score: float
    model_predictions: List[ModelPrediction]
    model_weights: Dict[str, float]
    consensus_level: float
    uncertainty_score: float
    timestamp: datetime

class BaseMLModel:
    """æœºå™¨å­¦ä¹ æ¨¡å‹åŸºç±»"""
    
    def __init__(self, name: str, model, scaler=None):
        self.name = name
        self.model = model
        self.scaler = scaler or StandardScaler()
        self.is_trained = False
        self.training_score = 0.0
        self.validation_score = 0.0
        self.feature_names = []
        self.feature_importance = {}
        
    def fit(self, X: np.ndarray, y: np.ndarray, feature_names: List[str] = None):
        """è®­ç»ƒæ¨¡å‹"""
        try:
            # æ ‡å‡†åŒ–ç‰¹å¾
            X_scaled = self.scaler.fit_transform(X)
            
            # è®­ç»ƒæ¨¡å‹
            self.model.fit(X_scaled, y)
            self.is_trained = True
            
            # è®¡ç®—è®­ç»ƒåˆ†æ•°
            train_pred = self.model.predict(X_scaled)
            self.training_score = r2_score(y, train_pred)
            
            # äº¤å‰éªŒè¯åˆ†æ•°
            cv_scores = cross_val_score(self.model, X_scaled, y, cv=5, scoring='r2')
            self.validation_score = np.mean(cv_scores)
            
            # ç‰¹å¾é‡è¦æ€§
            if feature_names:
                self.feature_names = feature_names
                if hasattr(self.model, 'feature_importances_'):
                    self.feature_importance = dict(zip(
                        feature_names, self.model.feature_importances_
                    ))
                elif hasattr(self.model, 'coef_'):
                    self.feature_importance = dict(zip(
                        feature_names, np.abs(self.model.coef_)
                    ))
            
            logger.debug(f"âœ… {self.name} è®­ç»ƒå®Œæˆ - RÂ²: {self.validation_score:.4f}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {self.name} è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def predict(self, X: np.ndarray) -> Tuple[float, float]:
        """é¢„æµ‹"""
        try:
            if not self.is_trained:
                return 0.0, 0.0
            
            start_time = time.time()
            X_scaled = self.scaler.transform(X.reshape(1, -1))
            prediction = float(self.model.predict(X_scaled)[0])
            prediction_time = time.time() - start_time
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºéªŒè¯åˆ†æ•°ï¼‰
            confidence = max(0.0, min(1.0, self.validation_score))
            
            return prediction, confidence, prediction_time
            
        except Exception as e:
            logger.error(f"âŒ {self.name} é¢„æµ‹å¤±è´¥: {e}")
            return 0.0, 0.0, 0.0

class EnsembleBrainTrust:
    """é›†æˆå­¦ä¹ æ™ºå›Šå›¢"""
    
    def __init__(self, device: str = None, model_dir: str = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_dir = model_dir or "models/ensemble"
        os.makedirs(self.model_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹é›†åˆ
        self.models = self._initialize_models()
        
        # åŠ¨æ€æƒé‡
        self.model_weights = {name: 1.0 / len(self.models) for name in self.models.keys()}
        self.weight_decay = 0.95  # æƒé‡è¡°å‡å› å­
        self.min_weight = 0.01   # æœ€å°æƒé‡
        
        # æ€§èƒ½è¿½è¸ª
        self.performance_history = {name: [] for name in self.models.keys()}
        self.prediction_history = []
        self.max_history = 1000
        
        # è®­ç»ƒå‚æ•°
        self.retrain_interval = 100  # é‡è®­ç»ƒé—´éš”
        self.prediction_count = 0
        self.last_retrain = 0
        
        # ç‰¹å¾å·¥ç¨‹
        self.feature_names = [
            'price', 'volume', 'rsi', 'macd', 'bb_position', 'atr',
            'ema_12', 'ema_26', 'sma_50', 'volume_sma', 'price_change',
            'volatility', 'sentiment', 'news_impact', 'time_of_day',
            'day_of_week', 'support_level', 'resistance_level', 'trend_strength',
            'momentum', 'stoch_k', 'stoch_d', 'williams_r', 'cci',
            'roc', 'trix', 'dmi_plus', 'dmi_minus', 'adx'
        ]
        
        # æ•°æ®ç¼“å­˜
        self.training_data = {'X': [], 'y': []}
        self.max_training_samples = 5000
        
        # å®æ—¶çŠ¶æ€
        self.last_prediction = None
        self.last_confidence = 0.0
        self.performance_score = 0.5
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.load_models()
        
        logger.info(f"ğŸ§  é›†æˆå­¦ä¹ æ™ºå›Šå›¢åˆå§‹åŒ–å®Œæˆ - {len(self.models)}ä¸ªæ¨¡å‹")
    
    def _initialize_models(self) -> Dict[str, BaseMLModel]:
        """åˆå§‹åŒ–æ¨¡å‹é›†åˆ"""
        models = {}
        
        # éšæœºæ£®æ—
        models['random_forest'] = BaseMLModel(
            'RandomForest',
            RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            ),
            RobustScaler()
        )
        
        # æ¢¯åº¦æå‡
        models['gradient_boosting'] = BaseMLModel(
            'GradientBoosting',
            GradientBoostingRegressor(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                subsample=0.8,
                random_state=42
            ),
            StandardScaler()
        )
        
        # XGBoost
        models['xgboost'] = BaseMLModel(
            'XGBoost',
            xgb.XGBRegressor(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1
            ),
            StandardScaler()
        )
        
        # LightGBM
        models['lightgbm'] = BaseMLModel(
            'LightGBM',
            lgb.LGBMRegressor(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
                verbose=-1
            ),
            StandardScaler()
        )
        
        # CatBoost
        models['catboost'] = BaseMLModel(
            'CatBoost',
            cb.CatBoostRegressor(
                iterations=200,
                learning_rate=0.1,
                depth=8,
                subsample=0.8,
                random_state=42,
                verbose=False
            ),
            StandardScaler()
        )
        
        # æ”¯æŒå‘é‡å›å½’
        models['svr'] = BaseMLModel(
            'SVR',
            SVR(
                kernel='rbf',
                C=1.0,
                gamma='scale',
                epsilon=0.01
            ),
            StandardScaler()
        )
        
        # ç¥ç»ç½‘ç»œ
        models['mlp'] = BaseMLModel(
            'MLP',
            MLPRegressor(
                hidden_layer_sizes=(128, 64, 32),
                activation='relu',
                solver='adam',
                alpha=0.001,
                learning_rate='adaptive',
                max_iter=500,
                random_state=42
            ),
            StandardScaler()
        )
        
        # çº¿æ€§æ¨¡å‹é›†åˆ
        models['ridge'] = BaseMLModel(
            'Ridge',
            Ridge(alpha=1.0, random_state=42),
            StandardScaler()
        )
        
        models['lasso'] = BaseMLModel(
            'Lasso',
            Lasso(alpha=0.1, random_state=42),
            StandardScaler()
        )
        
        models['elastic_net'] = BaseMLModel(
            'ElasticNet',
            ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42),
            StandardScaler()
        )
        
        return models
    
    async def get_ensemble_prediction(self, market_data: Dict[str, Any]) -> EnsemblePrediction:
        """è·å–é›†æˆé¢„æµ‹"""
        try:
            # å‡†å¤‡ç‰¹å¾
            features = self._prepare_features(market_data)
            if features is None:
                return self._create_fallback_prediction(market_data)
            
            # å¹¶è¡Œé¢„æµ‹
            model_predictions = []
            prediction_tasks = []
            
            for name, model in self.models.items():
                if model.is_trained:
                    task = asyncio.create_task(
                        self._async_model_predict(name, model, features)
                    )
                    prediction_tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰é¢„æµ‹å®Œæˆ
            if prediction_tasks:
                results = await asyncio.gather(*prediction_tasks, return_exceptions=True)
                model_predictions = [r for r in results if not isinstance(r, Exception)]
            
            if not model_predictions:
                return self._create_fallback_prediction(market_data)
            
            # è®¡ç®—é›†æˆé¢„æµ‹
            weighted_pred = self._calculate_weighted_prediction(model_predictions)
            voting_pred = self._calculate_voting_prediction(model_predictions)
            final_pred = (weighted_pred + voting_pred) / 2.0
            
            # è®¡ç®—ç½®ä¿¡åº¦å’Œä¸€è‡´æ€§
            confidence_score = self._calculate_ensemble_confidence(model_predictions)
            consensus_level = self._calculate_consensus_level(model_predictions)
            uncertainty_score = self._calculate_uncertainty_score(model_predictions)
            
            # åˆ›å»ºé›†æˆé¢„æµ‹ç»“æœ
            ensemble_pred = EnsemblePrediction(
                final_prediction=final_pred,
                weighted_prediction=weighted_pred,
                voting_prediction=voting_pred,
                confidence_score=confidence_score,
                model_predictions=model_predictions,
                model_weights=self.model_weights.copy(),
                consensus_level=consensus_level,
                uncertainty_score=uncertainty_score,
                timestamp=datetime.now(timezone.utc)
            )
            
            # æ›´æ–°çŠ¶æ€
            self.last_prediction = ensemble_pred
            self.last_confidence = confidence_score
            self.prediction_count += 1
            
            # è®°å½•é¢„æµ‹å†å²
            self.prediction_history.append({
                'timestamp': ensemble_pred.timestamp,
                'prediction': final_pred,
                'confidence': confidence_score,
                'consensus': consensus_level
            })
            
            # é™åˆ¶å†å²è®°å½•
            if len(self.prediction_history) > self.max_history:
                self.prediction_history = self.prediction_history[-self.max_history:]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è®­ç»ƒ
            if self.prediction_count - self.last_retrain >= self.retrain_interval:
                asyncio.create_task(self._async_retrain())
            
            logger.info(f"ğŸ§  é›†æˆé¢„æµ‹å®Œæˆ - é¢„æµ‹: {final_pred:.6f}, ç½®ä¿¡åº¦: {confidence_score:.3f}")
            return ensemble_pred
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆé¢„æµ‹å¤±è´¥: {e}")
            return self._create_fallback_prediction(market_data)
    
    async def _async_model_predict(self, name: str, model: BaseMLModel, 
                                 features: np.ndarray) -> ModelPrediction:
        """å¼‚æ­¥æ¨¡å‹é¢„æµ‹"""
        try:
            prediction, confidence, pred_time = model.predict(features)
            
            return ModelPrediction(
                model_name=name,
                prediction=prediction,
                confidence=confidence,
                feature_importance=model.feature_importance.copy(),
                training_score=model.training_score,
                validation_score=model.validation_score,
                prediction_time=pred_time
            )
            
        except Exception as e:
            logger.error(f"âŒ {name} å¼‚æ­¥é¢„æµ‹å¤±è´¥: {e}")
            return ModelPrediction(
                model_name=name,
                prediction=0.0,
                confidence=0.0,
                feature_importance={},
                training_score=0.0,
                validation_score=0.0,
                prediction_time=0.0
            )
    
    def _prepare_features(self, market_data: Dict[str, Any]) -> Optional[np.ndarray]:
        """å‡†å¤‡ç‰¹å¾å‘é‡"""
        try:
            features = []
            
            # åŸºç¡€å¸‚åœºæ•°æ®
            features.extend([
                market_data.get('price', 0.0),
                market_data.get('volume', 0.0),
                market_data.get('rsi', 50.0),
                market_data.get('macd', 0.0),
                market_data.get('bb_position', 0.5),
                market_data.get('atr', 0.0),
                market_data.get('ema_12', 0.0),
                market_data.get('ema_26', 0.0),
                market_data.get('sma_50', 0.0),
                market_data.get('volume_sma', 0.0),
                market_data.get('price_change', 0.0),
                market_data.get('volatility', 0.0),
                market_data.get('sentiment', 0.0),
                market_data.get('news_impact', 0.0),
                market_data.get('time_of_day', 0.5),
                market_data.get('day_of_week', 0.5),
                market_data.get('support_level', 0.0),
                market_data.get('resistance_level', 0.0),
                market_data.get('trend_strength', 0.0)
            ])
            
            # æ‰©å±•æŠ€æœ¯æŒ‡æ ‡
            features.extend([
                market_data.get('momentum', 0.0),
                market_data.get('stoch_k', 50.0),
                market_data.get('stoch_d', 50.0),
                market_data.get('williams_r', -50.0),
                market_data.get('cci', 0.0),
                market_data.get('roc', 0.0),
                market_data.get('trix', 0.0),
                market_data.get('dmi_plus', 25.0),
                market_data.get('dmi_minus', 25.0),
                market_data.get('adx', 25.0)
            ])
            
            return np.array(features, dtype=np.float32)
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾å‡†å¤‡å¤±è´¥: {e}")
            return None
    
    def _calculate_weighted_prediction(self, predictions: List[ModelPrediction]) -> float:
        """è®¡ç®—åŠ æƒé¢„æµ‹"""
        try:
            weighted_sum = 0.0
            total_weight = 0.0
            
            for pred in predictions:
                weight = self.model_weights.get(pred.model_name, 0.0)
                confidence_weight = pred.confidence * weight
                weighted_sum += pred.prediction * confidence_weight
                total_weight += confidence_weight
            
            return weighted_sum / max(total_weight, 1e-8)
            
        except Exception as e:
            logger.error(f"âŒ åŠ æƒé¢„æµ‹è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_voting_prediction(self, predictions: List[ModelPrediction]) -> float:
        """è®¡ç®—æŠ•ç¥¨é¢„æµ‹"""
        try:
            # ç®€å•å¹³å‡
            if not predictions:
                return 0.0
            
            return np.mean([pred.prediction for pred in predictions])
            
        except Exception as e:
            logger.error(f"âŒ æŠ•ç¥¨é¢„æµ‹è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_ensemble_confidence(self, predictions: List[ModelPrediction]) -> float:
        """è®¡ç®—é›†æˆç½®ä¿¡åº¦"""
        try:
            if not predictions:
                return 0.0
            
            # åŸºäºæ¨¡å‹ç½®ä¿¡åº¦çš„åŠ æƒå¹³å‡
            confidences = [pred.confidence for pred in predictions]
            weights = [self.model_weights.get(pred.model_name, 0.0) for pred in predictions]
            
            weighted_confidence = np.average(confidences, weights=weights)
            
            # è€ƒè™‘æ¨¡å‹ä¸€è‡´æ€§
            pred_values = [pred.prediction for pred in predictions]
            consistency = 1.0 - (np.std(pred_values) / (np.mean(np.abs(pred_values)) + 1e-8))
            consistency = max(0.0, min(1.0, consistency))
            
            return (weighted_confidence + consistency) / 2.0
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆç½®ä¿¡åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_consensus_level(self, predictions: List[ModelPrediction]) -> float:
        """è®¡ç®—ä¸€è‡´æ€§æ°´å¹³"""
        try:
            if len(predictions) < 2:
                return 1.0
            
            pred_values = [pred.prediction for pred in predictions]
            mean_pred = np.mean(pred_values)
            
            # è®¡ç®—ç›¸å¯¹æ ‡å‡†å·®
            if abs(mean_pred) > 1e-8:
                cv = np.std(pred_values) / abs(mean_pred)
                consensus = max(0.0, 1.0 - cv)
            else:
                consensus = 1.0 - np.std(pred_values)
            
            return max(0.0, min(1.0, consensus))
            
        except Exception as e:
            logger.error(f"âŒ ä¸€è‡´æ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_uncertainty_score(self, predictions: List[ModelPrediction]) -> float:
        """è®¡ç®—ä¸ç¡®å®šæ€§åˆ†æ•°"""
        try:
            if not predictions:
                return 1.0
            
            # åŸºäºé¢„æµ‹æ–¹å·®å’Œç½®ä¿¡åº¦æ–¹å·®
            pred_values = [pred.prediction for pred in predictions]
            confidences = [pred.confidence for pred in predictions]
            
            pred_uncertainty = np.std(pred_values) / (np.mean(np.abs(pred_values)) + 1e-8)
            conf_uncertainty = np.std(confidences)
            
            uncertainty = (pred_uncertainty + conf_uncertainty) / 2.0
            return max(0.0, min(1.0, uncertainty))
            
        except Exception as e:
            logger.error(f"âŒ ä¸ç¡®å®šæ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _create_fallback_prediction(self, market_data: Dict[str, Any]) -> EnsemblePrediction:
        """åˆ›å»ºåå¤‡é¢„æµ‹"""
        return EnsemblePrediction(
            final_prediction=0.0,
            weighted_prediction=0.0,
            voting_prediction=0.0,
            confidence_score=0.1,
            model_predictions=[],
            model_weights={},
            consensus_level=0.0,
            uncertainty_score=1.0,
            timestamp=datetime.now(timezone.utc)
        )
    
    async def train_models(self, training_data: Dict[str, np.ndarray]) -> Dict[str, bool]:
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        try:
            X = training_data['X']
            y = training_data['y']
            
            if len(X) < 100:  # æœ€å°‘éœ€è¦100ä¸ªæ ·æœ¬
                logger.warning("âš ï¸ è®­ç»ƒæ•°æ®ä¸è¶³")
                return {}
            
            # å¹¶è¡Œè®­ç»ƒ
            training_tasks = []
            for name, model in self.models.items():
                task = asyncio.create_task(
                    self._async_train_model(name, model, X, y)
                )
                training_tasks.append((name, task))
            
            # ç­‰å¾…è®­ç»ƒå®Œæˆ
            results = {}
            for name, task in training_tasks:
                try:
                    success = await task
                    results[name] = success
                    
                    # æ›´æ–°æƒé‡
                    if success:
                        model = self.models[name]
                        performance = model.validation_score
                        self.performance_history[name].append(performance)
                        
                        # é™åˆ¶å†å²é•¿åº¦
                        if len(self.performance_history[name]) > 100:
                            self.performance_history[name] = self.performance_history[name][-100:]
                        
                        # æ›´æ–°æƒé‡
                        self._update_model_weight(name, performance)
                    
                except Exception as e:
                    logger.error(f"âŒ {name} è®­ç»ƒä»»åŠ¡å¤±è´¥: {e}")
                    results[name] = False
            
            # æ ‡å‡†åŒ–æƒé‡
            self._normalize_weights()
            
            self.last_retrain = self.prediction_count
            logger.info(f"ğŸ§  æ¨¡å‹è®­ç»ƒå®Œæˆ - æˆåŠŸ: {sum(results.values())}/{len(results)}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {}
    
    async def _async_train_model(self, name: str, model: BaseMLModel, 
                               X: np.ndarray, y: np.ndarray) -> bool:
        """å¼‚æ­¥è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        try:
            return model.fit(X, y, self.feature_names)
        except Exception as e:
            logger.error(f"âŒ {name} å¼‚æ­¥è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def _update_model_weight(self, name: str, performance: float):
        """æ›´æ–°æ¨¡å‹æƒé‡"""
        try:
            # åŸºäºæ€§èƒ½è°ƒæ•´æƒé‡
            if performance > 0.5:  # å¥½çš„æ€§èƒ½å¢åŠ æƒé‡
                self.model_weights[name] *= (1.0 + performance * 0.1)
            else:  # å·®çš„æ€§èƒ½å‡å°‘æƒé‡
                self.model_weights[name] *= self.weight_decay
            
            # ç¡®ä¿æœ€å°æƒé‡
            self.model_weights[name] = max(self.model_weights[name], self.min_weight)
            
        except Exception as e:
            logger.error(f"âŒ æƒé‡æ›´æ–°å¤±è´¥: {e}")
    
    def _normalize_weights(self):
        """æ ‡å‡†åŒ–æƒé‡"""
        try:
            total_weight = sum(self.model_weights.values())
            if total_weight > 0:
                for name in self.model_weights:
                    self.model_weights[name] /= total_weight
        except Exception as e:
            logger.error(f"âŒ æƒé‡æ ‡å‡†åŒ–å¤±è´¥: {e}")
    
    async def _async_retrain(self):
        """å¼‚æ­¥é‡è®­ç»ƒ"""
        try:
            if len(self.training_data['X']) < 100:
                return
            
            X = np.array(self.training_data['X'])
            y = np.array(self.training_data['y'])
            
            await self.train_models({'X': X, 'y': y})
            
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥é‡è®­ç»ƒå¤±è´¥: {e}")
    
    def add_training_sample(self, features: np.ndarray, target: float):
        """æ·»åŠ è®­ç»ƒæ ·æœ¬"""
        try:
            self.training_data['X'].append(features)
            self.training_data['y'].append(target)
            
            # é™åˆ¶è®­ç»ƒæ•°æ®å¤§å°
            if len(self.training_data['X']) > self.max_training_samples:
                self.training_data['X'] = self.training_data['X'][-self.max_training_samples:]
                self.training_data['y'] = self.training_data['y'][-self.max_training_samples:]
                
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è®­ç»ƒæ ·æœ¬å¤±è´¥: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€"""
        try:
            model_status = {}
            for name, model in self.models.items():
                model_status[name] = {
                    'is_trained': model.is_trained,
                    'training_score': model.training_score,
                    'validation_score': model.validation_score,
                    'weight': self.model_weights.get(name, 0.0)
                }
            
            return {
                'model_id': 'ensemble_brain_trust',
                'model_name': 'é›†æˆå­¦ä¹ æ™ºå›Šå›¢',
                'total_models': len(self.models),
                'trained_models': sum(1 for m in self.models.values() if m.is_trained),
                'prediction_count': self.prediction_count,
                'last_confidence': self.last_confidence,
                'performance_score': self.performance_score,
                'model_weights': self.model_weights.copy(),
                'model_status': model_status,
                'training_samples': len(self.training_data['X']),
                'prediction_history_length': len(self.prediction_history)
            }
            
        except Exception as e:
            logger.error(f"âŒ çŠ¶æ€è·å–å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def save_models(self) -> bool:
        """ä¿å­˜æ‰€æœ‰æ¨¡å‹"""
        try:
            for name, model in self.models.items():
                if model.is_trained:
                    model_path = os.path.join(self.model_dir, f"{name}.joblib")
                    scaler_path = os.path.join(self.model_dir, f"{name}_scaler.joblib")
                    
                    joblib.dump(model.model, model_path)
                    joblib.dump(model.scaler, scaler_path)
            
            # ä¿å­˜æƒé‡å’Œé…ç½®
            config_path = os.path.join(self.model_dir, "ensemble_config.json")
            config = {
                'model_weights': self.model_weights,
                'performance_history': self.performance_history,
                'feature_names': self.feature_names
            }
            
            with open(config_path, 'w') as f:
                json.dump(config, f, indent=2)
            
            logger.info(f"ğŸ’¾ é›†æˆæ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_dir}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def load_models(self) -> bool:
        """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        try:
            config_path = os.path.join(self.model_dir, "ensemble_config.json")
            if os.path.exists(config_path):
                with open(config_path, 'r') as f:
                    config = json.load(f)
                
                self.model_weights = config.get('model_weights', self.model_weights)
                self.performance_history = config.get('performance_history', self.performance_history)
                self.feature_names = config.get('feature_names', self.feature_names)
            
            # åŠ è½½å„ä¸ªæ¨¡å‹
            loaded_count = 0
            for name, model in self.models.items():
                model_path = os.path.join(self.model_dir, f"{name}.joblib")
                scaler_path = os.path.join(self.model_dir, f"{name}_scaler.joblib")
                
                if os.path.exists(model_path) and os.path.exists(scaler_path):
                    try:
                        model.model = joblib.load(model_path)
                        model.scaler = joblib.load(scaler_path)
                        model.is_trained = True
                        loaded_count += 1
                    except Exception as e:
                        logger.warning(f"âš ï¸ {name} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            
            if loaded_count > 0:
                logger.info(f"ğŸ“‚ å·²åŠ è½½ {loaded_count} ä¸ªé›†æˆæ¨¡å‹")
                return True
            else:
                logger.info("ğŸ“‚ æœªæ‰¾åˆ°é¢„è®­ç»ƒçš„é›†æˆæ¨¡å‹")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

# å…¨å±€å®ä¾‹
ensemble_brain_trust = EnsembleBrainTrust()

def initialize_ensemble_brain_trust(device: str = None, model_dir: str = None) -> EnsembleBrainTrust:
    """åˆå§‹åŒ–é›†æˆå­¦ä¹ æ™ºå›Šå›¢"""
    global ensemble_brain_trust
    ensemble_brain_trust = EnsembleBrainTrust(device, model_dir)
    return ensemble_brain_trust

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_ensemble():
        ensemble = initialize_ensemble_brain_trust()
        
        # æµ‹è¯•é¢„æµ‹
        market_data = {
            'price': 50000.0,
            'volume': 1000000.0,
            'rsi': 65.0,
            'macd': 0.1,
            'bb_position': 0.7,
            'atr': 500.0,
            'price_change': 0.02,
            'volatility': 0.15,
            'sentiment': 0.3,
            'news_impact': 0.1
        }
        
        prediction = await ensemble.get_ensemble_prediction(market_data)
        print(f"é›†æˆé¢„æµ‹: {prediction}")
        
        # çŠ¶æ€æŠ¥å‘Š
        status = ensemble.get_status()
        print(f"çŠ¶æ€æŠ¥å‘Š: {json.dumps(status, indent=2, ensure_ascii=False)}")
    
    asyncio.run(test_ensemble())
