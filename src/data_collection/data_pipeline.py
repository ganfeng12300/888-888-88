"""
ğŸ”„ æ•°æ®å¤„ç†ç®¡é“
ç”Ÿäº§çº§å®æ—¶æ•°æ®å¤„ç†ã€æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹å’Œå­˜å‚¨ç³»ç»Ÿ
æ”¯æŒå¤šæ•°æ®æºã€å®æ—¶æµå¤„ç†å’Œæ‰¹é‡å¤„ç†
"""

import asyncio
import time
import json
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
from collections import deque
import redis.asyncio as redis
from clickhouse_driver import Client as ClickHouseClient
import psycopg2
from psycopg2.extras import RealDictCursor
import kafka
from kafka import KafkaProducer, KafkaConsumer
import threading
from concurrent.futures import ThreadPoolExecutor

from loguru import logger
from src.hardware.cpu_manager import CPUTaskType, assign_cpu_cores
from src.hardware.storage_manager import DataType as StorageDataType, storage_manager
from src.data_collection.exchange_connector import MarketData, DataType


class ProcessingStage(Enum):
    """å¤„ç†é˜¶æ®µ"""
    RAW = "raw"
    CLEANED = "cleaned"
    FEATURED = "featured"
    AGGREGATED = "aggregated"
    STORED = "stored"


class DataQuality(Enum):
    """æ•°æ®è´¨é‡"""
    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"
    INVALID = "invalid"


@dataclass
class ProcessedData:
    """å¤„ç†åçš„æ•°æ®ç»“æ„"""
    original_data: MarketData
    stage: ProcessingStage
    quality: DataQuality
    features: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    processing_time_ms: float = 0.0
    errors: List[str] = field(default_factory=list)


@dataclass
class PipelineConfig:
    """ç®¡é“é…ç½®"""
    enable_cleaning: bool = True
    enable_feature_engineering: bool = True
    enable_aggregation: bool = True
    enable_storage: bool = True
    batch_size: int = 1000
    max_queue_size: int = 10000
    processing_timeout: float = 5.0
    redis_url: str = "redis://localhost:6379/0"
    clickhouse_url: str = "clickhouse://localhost:8123"
    postgres_url: str = "postgresql://user:pass@localhost:5432/db"
    kafka_bootstrap_servers: str = "localhost:9092"


class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self):
        self.outlier_threshold = 3.0  # æ ‡å‡†å·®å€æ•°
        self.price_change_threshold = 0.1  # 10%ä»·æ ¼å˜åŒ–é˜ˆå€¼
        
    async def clean_ticker_data(self, data: Dict[str, Any]) -> tuple[Dict[str, Any], DataQuality]:
        """æ¸…æ´—tickeræ•°æ®"""
        try:
            cleaned_data = data.copy()
            quality = DataQuality.EXCELLENT
            
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            required_fields = ['last', 'bid', 'ask', 'volume']
            for field in required_fields:
                if field not in data or data[field] is None:
                    quality = DataQuality.POOR
                    cleaned_data[field] = 0.0
            
            # ä»·æ ¼åˆç†æ€§æ£€æŸ¥
            last_price = float(cleaned_data.get('last', 0))
            bid_price = float(cleaned_data.get('bid', 0))
            ask_price = float(cleaned_data.get('ask', 0))
            
            if last_price <= 0 or bid_price <= 0 or ask_price <= 0:
                quality = DataQuality.INVALID
                return cleaned_data, quality
            
            # ä¹°å–ä»·å·®æ£€æŸ¥
            spread = ask_price - bid_price
            if spread < 0 or spread / last_price > 0.05:  # 5%ä»·å·®é˜ˆå€¼
                quality = DataQuality.FAIR
            
            # æˆäº¤é‡æ£€æŸ¥
            volume = float(cleaned_data.get('volume', 0))
            if volume < 0:
                cleaned_data['volume'] = 0
                quality = DataQuality.FAIR
            
            return cleaned_data, quality
            
        except Exception as e:
            logger.error(f"æ¸…æ´—tickeræ•°æ®å¤±è´¥: {e}")
            return data, DataQuality.INVALID
    
    async def clean_orderbook_data(self, data: Dict[str, Any]) -> tuple[Dict[str, Any], DataQuality]:
        """æ¸…æ´—è®¢å•ç°¿æ•°æ®"""
        try:
            cleaned_data = data.copy()
            quality = DataQuality.EXCELLENT
            
            # æ£€æŸ¥ä¹°å–ç›˜æ•°æ®
            bids = data.get('bids', [])
            asks = data.get('asks', [])
            
            if not bids or not asks:
                return cleaned_data, DataQuality.INVALID
            
            # æ¸…æ´—ä¹°ç›˜æ•°æ®
            cleaned_bids = []
            for bid in bids:
                if len(bid) >= 2 and bid[0] > 0 and bid[1] > 0:
                    cleaned_bids.append([float(bid[0]), float(bid[1])])
            
            # æ¸…æ´—å–ç›˜æ•°æ®
            cleaned_asks = []
            for ask in asks:
                if len(ask) >= 2 and ask[0] > 0 and ask[1] > 0:
                    cleaned_asks.append([float(ask[0]), float(ask[1])])
            
            # æ’åºæ£€æŸ¥
            if cleaned_bids != sorted(cleaned_bids, key=lambda x: x[0], reverse=True):
                cleaned_bids = sorted(cleaned_bids, key=lambda x: x[0], reverse=True)
                quality = DataQuality.GOOD
            
            if cleaned_asks != sorted(cleaned_asks, key=lambda x: x[0]):
                cleaned_asks = sorted(cleaned_asks, key=lambda x: x[0])
                quality = DataQuality.GOOD
            
            # ä»·æ ¼äº¤å‰æ£€æŸ¥
            if cleaned_bids and cleaned_asks and cleaned_bids[0][0] >= cleaned_asks[0][0]:
                quality = DataQuality.POOR
            
            cleaned_data['bids'] = cleaned_bids
            cleaned_data['asks'] = cleaned_asks
            
            return cleaned_data, quality
            
        except Exception as e:
            logger.error(f"æ¸…æ´—è®¢å•ç°¿æ•°æ®å¤±è´¥: {e}")
            return data, DataQuality.INVALID
    
    async def clean_trade_data(self, data: Dict[str, Any]) -> tuple[Dict[str, Any], DataQuality]:
        """æ¸…æ´—äº¤æ˜“æ•°æ®"""
        try:
            cleaned_data = data.copy()
            quality = DataQuality.EXCELLENT
            
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            price = float(cleaned_data.get('price', 0))
            amount = float(cleaned_data.get('amount', 0))
            
            if price <= 0 or amount <= 0:
                return cleaned_data, DataQuality.INVALID
            
            # æ—¶é—´æˆ³æ£€æŸ¥
            timestamp = cleaned_data.get('timestamp')
            if not timestamp:
                cleaned_data['timestamp'] = time.time() * 1000
                quality = DataQuality.GOOD
            
            return cleaned_data, quality
            
        except Exception as e:
            logger.error(f"æ¸…æ´—äº¤æ˜“æ•°æ®å¤±è´¥: {e}")
            return data, DataQuality.INVALID


class FeatureEngineer:
    """ç‰¹å¾å·¥ç¨‹å™¨"""
    
    def __init__(self):
        self.price_history = {}  # ä»·æ ¼å†å²ç¼“å­˜
        self.volume_history = {}  # æˆäº¤é‡å†å²ç¼“å­˜
        self.history_size = 100  # å†å²æ•°æ®å¤§å°
        
    async def extract_ticker_features(self, data: Dict[str, Any], symbol: str) -> Dict[str, Any]:
        """æå–tickerç‰¹å¾"""
        try:
            features = {}
            
            # åŸºç¡€ç‰¹å¾
            last_price = float(data.get('last', 0))
            bid_price = float(data.get('bid', 0))
            ask_price = float(data.get('ask', 0))
            volume = float(data.get('volume', 0))
            
            # ä»·å·®ç‰¹å¾
            spread = ask_price - bid_price
            features['spread'] = spread
            features['spread_pct'] = (spread / last_price) * 100 if last_price > 0 else 0
            features['mid_price'] = (bid_price + ask_price) / 2
            
            # ä»·æ ¼å˜åŒ–ç‰¹å¾
            if symbol not in self.price_history:
                self.price_history[symbol] = deque(maxlen=self.history_size)
            
            price_hist = self.price_history[symbol]
            if price_hist:
                prev_price = price_hist[-1]
                price_change = last_price - prev_price
                features['price_change'] = price_change
                features['price_change_pct'] = (price_change / prev_price) * 100 if prev_price > 0 else 0
                
                # ä»·æ ¼æ³¢åŠ¨ç‡
                if len(price_hist) >= 20:
                    prices = np.array(list(price_hist))
                    returns = np.diff(prices) / prices[:-1]
                    features['volatility'] = np.std(returns) * np.sqrt(1440)  # æ—¥åŒ–æ³¢åŠ¨ç‡
            
            price_hist.append(last_price)
            
            # æˆäº¤é‡ç‰¹å¾
            if symbol not in self.volume_history:
                self.volume_history[symbol] = deque(maxlen=self.history_size)
            
            volume_hist = self.volume_history[symbol]
            if volume_hist:
                avg_volume = np.mean(list(volume_hist))
                features['volume_ratio'] = volume / avg_volume if avg_volume > 0 else 1
            
            volume_hist.append(volume)
            
            # æŠ€æœ¯æŒ‡æ ‡
            if len(price_hist) >= 20:
                prices = np.array(list(price_hist))
                
                # ç§»åŠ¨å¹³å‡
                features['sma_5'] = np.mean(prices[-5:])
                features['sma_20'] = np.mean(prices[-20:])
                features['price_vs_sma20'] = (last_price / features['sma_20'] - 1) * 100
                
                # RSI
                deltas = np.diff(prices)
                gains = np.where(deltas > 0, deltas, 0)
                losses = np.where(deltas < 0, -deltas, 0)
                
                if len(gains) >= 14:
                    avg_gain = np.mean(gains[-14:])
                    avg_loss = np.mean(losses[-14:])
                    
                    if avg_loss > 0:
                        rs = avg_gain / avg_loss
                        features['rsi'] = 100 - (100 / (1 + rs))
            
            return features
            
        except Exception as e:
            logger.error(f"æå–tickerç‰¹å¾å¤±è´¥: {e}")
            return {}
    
    async def extract_orderbook_features(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æå–è®¢å•ç°¿ç‰¹å¾"""
        try:
            features = {}
            
            bids = data.get('bids', [])
            asks = data.get('asks', [])
            
            if not bids or not asks:
                return features
            
            # æœ€ä¼˜ä¹°å–ä»·
            best_bid = bids[0][0]
            best_ask = asks[0][0]
            
            # æ·±åº¦ç‰¹å¾
            bid_depth_5 = sum([bid[1] for bid in bids[:5]])
            ask_depth_5 = sum([ask[1] for ask in asks[:5]])
            
            features['bid_depth_5'] = bid_depth_5
            features['ask_depth_5'] = ask_depth_5
            features['depth_imbalance'] = (bid_depth_5 - ask_depth_5) / (bid_depth_5 + ask_depth_5)
            
            # ä»·æ ¼åˆ†å¸ƒç‰¹å¾
            bid_prices = [bid[0] for bid in bids[:10]]
            ask_prices = [ask[0] for ask in asks[:10]]
            
            features['bid_price_std'] = np.std(bid_prices)
            features['ask_price_std'] = np.std(ask_prices)
            
            # è®¢å•ç°¿æ–œç‡
            if len(bids) >= 5 and len(asks) >= 5:
                bid_slope = (bids[0][0] - bids[4][0]) / 5
                ask_slope = (asks[4][0] - asks[0][0]) / 5
                features['bid_slope'] = bid_slope
                features['ask_slope'] = ask_slope
            
            return features
            
        except Exception as e:
            logger.error(f"æå–è®¢å•ç°¿ç‰¹å¾å¤±è´¥: {e}")
            return {}


class DataAggregator:
    """æ•°æ®èšåˆå™¨"""
    
    def __init__(self):
        self.aggregation_windows = [60, 300, 900, 3600]  # 1åˆ†é’Ÿã€5åˆ†é’Ÿã€15åˆ†é’Ÿã€1å°æ—¶
        self.aggregated_data = {}
        
    async def aggregate_ticker_data(self, data: List[Dict[str, Any]], 
                                  symbol: str, window: int) -> Dict[str, Any]:
        """èšåˆtickeræ•°æ®"""
        try:
            if not data:
                return {}
            
            prices = [float(d.get('last', 0)) for d in data if d.get('last')]
            volumes = [float(d.get('volume', 0)) for d in data if d.get('volume')]
            
            if not prices:
                return {}
            
            aggregated = {
                'symbol': symbol,
                'window': window,
                'timestamp': time.time(),
                'open': prices[0],
                'high': max(prices),
                'low': min(prices),
                'close': prices[-1],
                'volume': sum(volumes) if volumes else 0,
                'count': len(prices),
                'vwap': 0  # æˆäº¤é‡åŠ æƒå¹³å‡ä»·
            }
            
            # è®¡ç®—VWAP
            if volumes and sum(volumes) > 0:
                total_value = sum(p * v for p, v in zip(prices, volumes))
                aggregated['vwap'] = total_value / sum(volumes)
            
            return aggregated
            
        except Exception as e:
            logger.error(f"èšåˆtickeræ•°æ®å¤±è´¥: {e}")
            return {}


class DataStorage:
    """æ•°æ®å­˜å‚¨å™¨"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.redis_client = None
        self.clickhouse_client = None
        self.postgres_conn = None
        self.kafka_producer = None
        
    async def initialize(self):
        """åˆå§‹åŒ–å­˜å‚¨è¿æ¥"""
        try:
            # Redisè¿æ¥
            self.redis_client = redis.from_url(self.config.redis_url)
            
            # ClickHouseè¿æ¥
            self.clickhouse_client = ClickHouseClient(host='localhost')
            
            # PostgreSQLè¿æ¥
            self.postgres_conn = psycopg2.connect(self.config.postgres_url)
            
            # Kafkaç”Ÿäº§è€…
            self.kafka_producer = KafkaProducer(
                bootstrap_servers=self.config.kafka_bootstrap_servers.split(','),
                value_serializer=lambda v: json.dumps(v).encode('utf-8')
            )
            
            logger.info("æ•°æ®å­˜å‚¨è¿æ¥åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å­˜å‚¨è¿æ¥å¤±è´¥: {e}")
    
    async def store_realtime_data(self, processed_data: ProcessedData):
        """å­˜å‚¨å®æ—¶æ•°æ®åˆ°Redis"""
        try:
            if not self.redis_client:
                return
            
            key = f"realtime:{processed_data.original_data.exchange}:{processed_data.original_data.symbol}:{processed_data.original_data.data_type.value}"
            
            data_to_store = {
                'data': processed_data.original_data.data,
                'features': processed_data.features,
                'quality': processed_data.quality.value,
                'timestamp': processed_data.original_data.timestamp
            }
            
            await self.redis_client.setex(key, 3600, json.dumps(data_to_store))  # 1å°æ—¶è¿‡æœŸ
            
        except Exception as e:
            logger.error(f"å­˜å‚¨å®æ—¶æ•°æ®å¤±è´¥: {e}")
    
    async def store_historical_data(self, processed_data: ProcessedData):
        """å­˜å‚¨å†å²æ•°æ®åˆ°ClickHouse"""
        try:
            if not self.clickhouse_client:
                return
            
            # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©è¡¨
            table_name = f"market_data_{processed_data.original_data.data_type.value}"
            
            # å‡†å¤‡æ•°æ®
            data_to_insert = {
                'timestamp': processed_data.original_data.timestamp,
                'exchange': processed_data.original_data.exchange,
                'symbol': processed_data.original_data.symbol,
                'data': json.dumps(processed_data.original_data.data),
                'features': json.dumps(processed_data.features),
                'quality': processed_data.quality.value,
                'processing_time_ms': processed_data.processing_time_ms
            }
            
            # æ’å…¥æ•°æ®
            self.clickhouse_client.execute(
                f"INSERT INTO {table_name} VALUES",
                [data_to_insert]
            )
            
        except Exception as e:
            logger.error(f"å­˜å‚¨å†å²æ•°æ®å¤±è´¥: {e}")
    
    async def publish_to_kafka(self, processed_data: ProcessedData):
        """å‘å¸ƒæ•°æ®åˆ°Kafka"""
        try:
            if not self.kafka_producer:
                return
            
            topic = f"market_data_{processed_data.original_data.data_type.value}"
            
            message = {
                'exchange': processed_data.original_data.exchange,
                'symbol': processed_data.original_data.symbol,
                'data': processed_data.original_data.data,
                'features': processed_data.features,
                'quality': processed_data.quality.value,
                'timestamp': processed_data.original_data.timestamp
            }
            
            self.kafka_producer.send(topic, value=message)
            
        except Exception as e:
            logger.error(f"å‘å¸ƒKafkaæ¶ˆæ¯å¤±è´¥: {e}")


class DataPipeline:
    """æ•°æ®å¤„ç†ç®¡é“"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.cleaner = DataCleaner()
        self.feature_engineer = FeatureEngineer()
        self.aggregator = DataAggregator()
        self.storage = DataStorage(config)
        
        # å¤„ç†é˜Ÿåˆ—
        self.processing_queue = asyncio.Queue(maxsize=config.max_queue_size)
        self.is_running = False
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'processed_count': 0,
            'error_count': 0,
            'avg_processing_time': 0.0,
            'quality_distribution': {q.value: 0 for q in DataQuality}
        }
        
        # åˆ†é…CPUæ ¸å¿ƒ
        assign_cpu_cores(CPUTaskType.FEATURE_ENGINEERING, [5, 6, 7, 8])
        
        logger.info("æ•°æ®å¤„ç†ç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    async def start(self):
        """å¯åŠ¨ç®¡é“"""
        try:
            await self.storage.initialize()
            self.is_running = True
            
            # å¯åŠ¨å¤„ç†ä»»åŠ¡
            for i in range(4):  # 4ä¸ªå¤„ç†åç¨‹
                asyncio.create_task(self._process_worker(f"worker-{i}"))
            
            logger.info("æ•°æ®å¤„ç†ç®¡é“å·²å¯åŠ¨")
            
        except Exception as e:
            logger.error(f"å¯åŠ¨ç®¡é“å¤±è´¥: {e}")
    
    async def stop(self):
        """åœæ­¢ç®¡é“"""
        self.is_running = False
        logger.info("æ•°æ®å¤„ç†ç®¡é“å·²åœæ­¢")
    
    async def process_data(self, market_data: MarketData) -> Optional[ProcessedData]:
        """å¤„ç†å•æ¡æ•°æ®"""
        try:
            if self.processing_queue.full():
                logger.warning("å¤„ç†é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒæ•°æ®")
                return None
            
            await self.processing_queue.put(market_data)
            return None
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ•°æ®åˆ°å¤„ç†é˜Ÿåˆ—å¤±è´¥: {e}")
            return None
    
    async def _process_worker(self, worker_name: str):
        """å¤„ç†å·¥ä½œåç¨‹"""
        logger.info(f"å¯åŠ¨å¤„ç†å·¥ä½œåç¨‹: {worker_name}")
        
        while self.is_running:
            try:
                # è·å–æ•°æ®
                market_data = await asyncio.wait_for(
                    self.processing_queue.get(), 
                    timeout=1.0
                )
                
                # å¤„ç†æ•°æ®
                processed_data = await self._process_single_data(market_data)
                
                if processed_data:
                    # å­˜å‚¨æ•°æ®
                    await self._store_processed_data(processed_data)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self._update_stats(processed_data)
                
                self.processing_queue.task_done()
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"å¤„ç†å·¥ä½œåç¨‹ {worker_name} å‡ºé”™: {e}")
                self.stats['error_count'] += 1
    
    async def _process_single_data(self, market_data: MarketData) -> Optional[ProcessedData]:
        """å¤„ç†å•æ¡æ•°æ®"""
        start_time = time.time()
        
        try:
            processed_data = ProcessedData(
                original_data=market_data,
                stage=ProcessingStage.RAW,
                quality=DataQuality.EXCELLENT
            )
            
            # æ•°æ®æ¸…æ´—
            if self.config.enable_cleaning:
                cleaned_data, quality = await self._clean_data(market_data)
                processed_data.original_data.data = cleaned_data
                processed_data.quality = quality
                processed_data.stage = ProcessingStage.CLEANED
                
                if quality == DataQuality.INVALID:
                    processed_data.errors.append("æ•°æ®è´¨é‡æ— æ•ˆ")
                    return processed_data
            
            # ç‰¹å¾å·¥ç¨‹
            if self.config.enable_feature_engineering:
                features = await self._extract_features(market_data)
                processed_data.features = features
                processed_data.stage = ProcessingStage.FEATURED
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (time.time() - start_time) * 1000
            processed_data.processing_time_ms = processing_time
            
            return processed_data
            
        except Exception as e:
            logger.error(f"å¤„ç†æ•°æ®å¤±è´¥: {e}")
            return None
    
    async def _clean_data(self, market_data: MarketData) -> tuple[Dict[str, Any], DataQuality]:
        """æ¸…æ´—æ•°æ®"""
        try:
            if market_data.data_type == DataType.TICKER:
                return await self.cleaner.clean_ticker_data(market_data.data)
            elif market_data.data_type == DataType.ORDERBOOK:
                return await self.cleaner.clean_orderbook_data(market_data.data)
            elif market_data.data_type == DataType.TRADES:
                return await self.cleaner.clean_trade_data(market_data.data)
            else:
                return market_data.data, DataQuality.EXCELLENT
                
        except Exception as e:
            logger.error(f"æ¸…æ´—æ•°æ®å¤±è´¥: {e}")
            return market_data.data, DataQuality.INVALID
    
    async def _extract_features(self, market_data: MarketData) -> Dict[str, Any]:
        """æå–ç‰¹å¾"""
        try:
            if market_data.data_type == DataType.TICKER:
                return await self.feature_engineer.extract_ticker_features(
                    market_data.data, market_data.symbol
                )
            elif market_data.data_type == DataType.ORDERBOOK:
                return await self.feature_engineer.extract_orderbook_features(market_data.data)
            else:
                return {}
                
        except Exception as e:
            logger.error(f"æå–ç‰¹å¾å¤±è´¥: {e}")
            return {}
    
    async def _store_processed_data(self, processed_data: ProcessedData):
        """å­˜å‚¨å¤„ç†åçš„æ•°æ®"""
        try:
            # å­˜å‚¨åˆ°Redisï¼ˆå®æ—¶æ•°æ®ï¼‰
            await self.storage.store_realtime_data(processed_data)
            
            # å­˜å‚¨åˆ°ClickHouseï¼ˆå†å²æ•°æ®ï¼‰
            if self.config.enable_storage:
                await self.storage.store_historical_data(processed_data)
            
            # å‘å¸ƒåˆ°Kafka
            await self.storage.publish_to_kafka(processed_data)
            
        except Exception as e:
            logger.error(f"å­˜å‚¨å¤„ç†æ•°æ®å¤±è´¥: {e}")
    
    def _update_stats(self, processed_data: ProcessedData):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['processed_count'] += 1
        self.stats['quality_distribution'][processed_data.quality.value] += 1
        
        # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
        current_avg = self.stats['avg_processing_time']
        count = self.stats['processed_count']
        new_avg = (current_avg * (count - 1) + processed_data.processing_time_ms) / count
        self.stats['avg_processing_time'] = new_avg
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()


# å…¨å±€æ•°æ®ç®¡é“å®ä¾‹
data_pipeline = None


def create_pipeline(config: PipelineConfig) -> DataPipeline:
    """åˆ›å»ºæ•°æ®ç®¡é“"""
    global data_pipeline
    data_pipeline = DataPipeline(config)
    return data_pipeline


async def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨æ•°æ®å¤„ç†ç®¡é“æµ‹è¯•...")
    
    # åˆ›å»ºé…ç½®
    config = PipelineConfig()
    
    # åˆ›å»ºç®¡é“
    pipeline = create_pipeline(config)
    
    try:
        # å¯åŠ¨ç®¡é“
        await pipeline.start()
        
        # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
        from src.data_collection.exchange_connector import MarketData, DataType
        
        test_data = MarketData(
            exchange="binance",
            symbol="BTC/USDT",
            data_type=DataType.TICKER,
            timestamp=time.time(),
            data={
                'last': 50000.0,
                'bid': 49999.0,
                'ask': 50001.0,
                'volume': 1000.0
            }
        )
        
        # å¤„ç†æµ‹è¯•æ•°æ®
        for i in range(100):
            test_data.data['last'] = 50000 + i * 10
            await pipeline.process_data(test_data)
            await asyncio.sleep(0.1)
        
        # ç­‰å¾…å¤„ç†å®Œæˆ
        await asyncio.sleep(5)
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = pipeline.get_stats()
        logger.info(f"å¤„ç†ç»Ÿè®¡: {stats}")
        
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·...")
    finally:
        await pipeline.stop()


if __name__ == "__main__":
    asyncio.run(main())
