"""
ğŸ”— ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•ç³»ç»Ÿ
ç”Ÿäº§çº§å®Œæ•´é“¾è·¯æµ‹è¯•ï¼Œä»AIå†³ç­–åˆ°è®¢å•æ‰§è¡Œçš„å…¨æµç¨‹éªŒè¯
å®ç°çœŸå®å¸‚åœºæ•°æ®æµ‹è¯•ã€å¼‚å¸¸å¤„ç†å’Œç³»ç»Ÿç¨³å®šæ€§éªŒè¯
"""

import asyncio
import time
import threading
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
from collections import deque
import json
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed

from loguru import logger
from src.hardware.cpu_manager import CPUTaskType, assign_cpu_cores
from src.ai_models.ai_evolution_system import AIEvolutionSystem, ModelType
from src.risk_management.risk_controller import AILevelRiskController
from src.trading_execution.smart_order_router import SmartOrderRouter, OrderRequest, OrderSide, OrderType
from src.trading_execution.low_latency_engine import LowLatencyExecutionEngine, ExecutionPriority
from src.data_collection.exchange_connector import ExchangeConnector


class TestType(Enum):
    """æµ‹è¯•ç±»å‹"""
    UNIT_TEST = "unit_test"
    INTEGRATION_TEST = "integration_test"
    STRESS_TEST = "stress_test"
    CHAOS_TEST = "chaos_test"
    PERFORMANCE_TEST = "performance_test"


class TestStatus(Enum):
    """æµ‹è¯•çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    ERROR = "error"


@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹"""
    test_id: str
    name: str
    description: str
    test_type: TestType
    test_function: Callable
    timeout_seconds: float = 60.0
    retry_count: int = 0
    max_retries: int = 3
    dependencies: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)
    expected_result: Any = None
    setup_function: Optional[Callable] = None
    teardown_function: Optional[Callable] = None


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    test_id: str
    status: TestStatus
    start_time: float
    end_time: float
    duration: float
    result_data: Any = None
    error_message: Optional[str] = None
    stack_trace: Optional[str] = None
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    assertions_passed: int = 0
    assertions_failed: int = 0


@dataclass
class SystemHealthMetrics:
    """ç³»ç»Ÿå¥åº·æŒ‡æ ‡"""
    timestamp: float
    cpu_usage: float
    memory_usage: float
    gpu_usage: float
    disk_usage: float
    network_latency: float
    active_connections: int
    queue_sizes: Dict[str, int]
    error_rate: float
    throughput: float


class EndToEndTester:
    """ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        # ç³»ç»Ÿç»„ä»¶
        self.ai_evolution_system: Optional[AIEvolutionSystem] = None
        self.risk_controller: Optional[AILevelRiskController] = None
        self.order_router: Optional[SmartOrderRouter] = None
        self.execution_engine: Optional[LowLatencyExecutionEngine] = None
        self.exchange_connectors: Dict[str, ExchangeConnector] = {}
        
        # æµ‹è¯•ç®¡ç†
        self.test_cases: Dict[str, TestCase] = {}
        self.test_results: Dict[str, TestResult] = {}
        self.test_queue: deque = deque()
        self.running_tests: Dict[str, asyncio.Task] = {}
        
        # æ€§èƒ½ç›‘æ§
        self.health_metrics: deque = deque(maxlen=1000)
        self.performance_baseline: Dict[str, float] = {}
        
        # æµ‹è¯•é…ç½®
        self.max_concurrent_tests = 5
        self.test_timeout = 300.0  # 5åˆ†é’Ÿ
        self.health_check_interval = 10.0  # 10ç§’
        
        # åˆ†é…CPUæ ¸å¿ƒ
        assign_cpu_cores(CPUTaskType.INTEGRATION_TESTING, [15, 16])
        
        # å¯åŠ¨å¥åº·ç›‘æ§
        self.monitoring_active = False
        self._start_health_monitoring()
        
        logger.info("ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def initialize_system_components(self, components: Dict[str, Any]):
        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
        try:
            self.ai_evolution_system = components.get('ai_evolution_system')
            self.risk_controller = components.get('risk_controller')
            self.order_router = components.get('order_router')
            self.execution_engine = components.get('execution_engine')
            self.exchange_connectors = components.get('exchange_connectors', {})
            
            logger.info("ç³»ç»Ÿç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶å¤±è´¥: {e}")
            raise
    
    def register_test_case(self, test_case: TestCase):
        """æ³¨å†Œæµ‹è¯•ç”¨ä¾‹"""
        self.test_cases[test_case.test_id] = test_case
        logger.info(f"æ³¨å†Œæµ‹è¯•ç”¨ä¾‹: {test_case.test_id} - {test_case.name}")
    
    def _start_health_monitoring(self):
        """å¯åŠ¨å¥åº·ç›‘æ§"""
        self.monitoring_active = True
        
        def monitor_health():
            while self.monitoring_active:
                try:
                    asyncio.run(self._collect_health_metrics())
                    time.sleep(self.health_check_interval)
                except Exception as e:
                    logger.error(f"å¥åº·ç›‘æ§å‡ºé”™: {e}")
                    time.sleep(1)
        
        monitor_thread = threading.Thread(target=monitor_health, daemon=True)
        monitor_thread.start()
        
        logger.info("ç³»ç»Ÿå¥åº·ç›‘æ§å·²å¯åŠ¨")
    
    async def _collect_health_metrics(self):
        """æ”¶é›†å¥åº·æŒ‡æ ‡"""
        try:
            import psutil
            
            # ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡
            cpu_usage = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            # GPUä½¿ç”¨ç‡ (ç®€åŒ–)
            gpu_usage = 0.0
            try:
                import GPUtil
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu_usage = gpus[0].load * 100
            except:
                pass
            
            # ç½‘ç»œå»¶è¿Ÿ (ç®€åŒ–)
            network_latency = 0.0
            
            # é˜Ÿåˆ—å¤§å°
            queue_sizes = {}
            if self.execution_engine:
                queue_sizes = {
                    'execution_queue': sum(
                        q.qsize() for q in self.execution_engine.execution_queues.values()
                    )
                }
            
            # é”™è¯¯ç‡å’Œååé‡
            error_rate = self._calculate_error_rate()
            throughput = self._calculate_throughput()
            
            # åˆ›å»ºå¥åº·æŒ‡æ ‡
            metrics = SystemHealthMetrics(
                timestamp=time.time(),
                cpu_usage=cpu_usage,
                memory_usage=memory.percent,
                gpu_usage=gpu_usage,
                disk_usage=disk.percent,
                network_latency=network_latency,
                active_connections=len(self.running_tests),
                queue_sizes=queue_sizes,
                error_rate=error_rate,
                throughput=throughput
            )
            
            self.health_metrics.append(metrics)
            
        except Exception as e:
            logger.error(f"æ”¶é›†å¥åº·æŒ‡æ ‡å¤±è´¥: {e}")
    
    def _calculate_error_rate(self) -> float:
        """è®¡ç®—é”™è¯¯ç‡"""
        if not self.test_results:
            return 0.0
        
        failed_tests = sum(1 for result in self.test_results.values() 
                          if result.status in [TestStatus.FAILED, TestStatus.ERROR])
        total_tests = len(self.test_results)
        
        return failed_tests / total_tests if total_tests > 0 else 0.0
    
    def _calculate_throughput(self) -> float:
        """è®¡ç®—ååé‡"""
        if len(self.test_results) < 2:
            return 0.0
        
        recent_results = list(self.test_results.values())[-10:]  # æœ€è¿‘10ä¸ªæµ‹è¯•
        if not recent_results:
            return 0.0
        
        time_span = recent_results[-1].end_time - recent_results[0].start_time
        return len(recent_results) / time_span if time_span > 0 else 0.0
    
    async def run_single_test(self, test_case: TestCase) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        start_time = time.time()
        
        try:
            logger.info(f"å¼€å§‹æµ‹è¯•: {test_case.test_id} - {test_case.name}")
            
            # æ‰§è¡Œsetup
            if test_case.setup_function:
                await test_case.setup_function()
            
            # æ‰§è¡Œæµ‹è¯•
            result_data = await asyncio.wait_for(
                test_case.test_function(),
                timeout=test_case.timeout_seconds
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            # éªŒè¯ç»“æœ
            status = TestStatus.PASSED
            if test_case.expected_result is not None:
                if result_data != test_case.expected_result:
                    status = TestStatus.FAILED
            
            # åˆ›å»ºæµ‹è¯•ç»“æœ
            test_result = TestResult(
                test_id=test_case.test_id,
                status=status,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                result_data=result_data,
                performance_metrics=self._extract_performance_metrics(result_data)
            )
            
            logger.info(f"æµ‹è¯•å®Œæˆ: {test_case.test_id}, çŠ¶æ€={status.value}, è€—æ—¶={duration:.2f}s")
            
            return test_result
            
        except asyncio.TimeoutError:
            end_time = time.time()
            return TestResult(
                test_id=test_case.test_id,
                status=TestStatus.FAILED,
                start_time=start_time,
                end_time=end_time,
                duration=end_time - start_time,
                error_message="æµ‹è¯•è¶…æ—¶"
            )
            
        except Exception as e:
            end_time = time.time()
            return TestResult(
                test_id=test_case.test_id,
                status=TestStatus.ERROR,
                start_time=start_time,
                end_time=end_time,
                duration=end_time - start_time,
                error_message=str(e),
                stack_trace=traceback.format_exc()
            )
            
        finally:
            # æ‰§è¡Œteardown
            if test_case.teardown_function:
                try:
                    await test_case.teardown_function()
                except Exception as e:
                    logger.error(f"Teardownå¤±è´¥: {e}")
    
    def _extract_performance_metrics(self, result_data: Any) -> Dict[str, float]:
        """æå–æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}
        
        if isinstance(result_data, dict):
            # æå–å»¶è¿ŸæŒ‡æ ‡
            if 'latency_ms' in result_data:
                metrics['latency_ms'] = float(result_data['latency_ms'])
            
            # æå–ååé‡æŒ‡æ ‡
            if 'throughput' in result_data:
                metrics['throughput'] = float(result_data['throughput'])
            
            # æå–æˆåŠŸç‡æŒ‡æ ‡
            if 'success_rate' in result_data:
                metrics['success_rate'] = float(result_data['success_rate'])
        
        return metrics
    
    async def run_test_suite(self, test_ids: List[str] = None) -> Dict[str, TestResult]:
        """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
        if test_ids is None:
            test_ids = list(self.test_cases.keys())
        
        logger.info(f"å¼€å§‹è¿è¡Œæµ‹è¯•å¥—ä»¶ï¼Œå…±{len(test_ids)}ä¸ªæµ‹è¯•")
        
        # æŒ‰ä¾èµ–å…³ç³»æ’åºæµ‹è¯•
        sorted_test_ids = self._sort_tests_by_dependencies(test_ids)
        
        # å¹¶å‘æ‰§è¡Œæµ‹è¯•
        semaphore = asyncio.Semaphore(self.max_concurrent_tests)
        
        async def run_with_semaphore(test_id: str):
            async with semaphore:
                test_case = self.test_cases[test_id]
                result = await self.run_single_test(test_case)
                self.test_results[test_id] = result
                return result
        
        # åˆ›å»ºä»»åŠ¡
        tasks = [run_with_semaphore(test_id) for test_id in sorted_test_ids]
        
        # ç­‰å¾…æ‰€æœ‰æµ‹è¯•å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        passed = sum(1 for r in results if isinstance(r, TestResult) and r.status == TestStatus.PASSED)
        failed = sum(1 for r in results if isinstance(r, TestResult) and r.status == TestStatus.FAILED)
        errors = sum(1 for r in results if isinstance(r, TestResult) and r.status == TestStatus.ERROR)
        
        logger.info(f"æµ‹è¯•å¥—ä»¶å®Œæˆ: é€šè¿‡={passed}, å¤±è´¥={failed}, é”™è¯¯={errors}")
        
        return self.test_results
    
    def _sort_tests_by_dependencies(self, test_ids: List[str]) -> List[str]:
        """æŒ‰ä¾èµ–å…³ç³»æ’åºæµ‹è¯•"""
        # ç®€åŒ–çš„æ‹“æ‰‘æ’åº
        sorted_ids = []
        remaining_ids = set(test_ids)
        
        while remaining_ids:
            # æ‰¾åˆ°æ²¡æœ‰æœªæ»¡è¶³ä¾èµ–çš„æµ‹è¯•
            ready_tests = []
            for test_id in remaining_ids:
                test_case = self.test_cases[test_id]
                dependencies_satisfied = all(
                    dep in sorted_ids or dep not in test_ids 
                    for dep in test_case.dependencies
                )
                if dependencies_satisfied:
                    ready_tests.append(test_id)
            
            if not ready_tests:
                # å¦‚æœæ²¡æœ‰å°±ç»ªçš„æµ‹è¯•ï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯ä¾èµ–
                logger.warning("æ£€æµ‹åˆ°å¯èƒ½çš„å¾ªç¯ä¾èµ–ï¼ŒæŒ‰åŸé¡ºåºæ‰§è¡Œå‰©ä½™æµ‹è¯•")
                ready_tests = list(remaining_ids)
            
            # æ·»åŠ å°±ç»ªçš„æµ‹è¯•
            for test_id in ready_tests:
                sorted_ids.append(test_id)
                remaining_ids.remove(test_id)
        
        return sorted_ids
    
    # å…·ä½“çš„æµ‹è¯•ç”¨ä¾‹å®ç°
    async def test_ai_decision_making(self) -> Dict[str, Any]:
        """æµ‹è¯•AIå†³ç­–åˆ¶å®š"""
        if not self.ai_evolution_system:
            raise Exception("AIè¿›åŒ–ç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        start_time = time.time()
        
        # æ¨¡æ‹Ÿå¸‚åœºæ•°æ®
        market_data = {
            'price': 50000.0,
            'volume': 1000000.0,
            'volatility': 0.02
        }
        
        # è·å–AIå†³ç­–
        decision = self.ai_evolution_system.get_fusion_decision(market_data)
        
        end_time = time.time()
        latency = (end_time - start_time) * 1000
        
        # éªŒè¯å†³ç­–ç»“æœ
        assert decision is not None, "AIå†³ç­–ä¸èƒ½ä¸ºç©º"
        assert 'final_action' in decision, "å†³ç­–å¿…é¡»åŒ…å«æœ€ç»ˆåŠ¨ä½œ"
        assert decision['final_action'] in ['buy', 'sell', 'hold'], "æ— æ•ˆçš„å†³ç­–åŠ¨ä½œ"
        assert 'confidence' in decision, "å†³ç­–å¿…é¡»åŒ…å«ç½®ä¿¡åº¦"
        assert 0 <= decision['confidence'] <= 1, "ç½®ä¿¡åº¦å¿…é¡»åœ¨0-1ä¹‹é—´"
        
        return {
            'decision': decision,
            'latency_ms': latency,
            'success': True
        }
    
    async def test_risk_control_integration(self) -> Dict[str, Any]:
        """æµ‹è¯•é£é™©æ§åˆ¶é›†æˆ"""
        if not self.risk_controller:
            raise Exception("é£é™©æ§åˆ¶å™¨æœªåˆå§‹åŒ–")
        
        start_time = time.time()
        
        # æ¨¡æ‹ŸæŒä»“æ›´æ–°
        symbol = "BTC/USDT"
        position_size = 1.0
        current_price = 50000.0
        entry_price = 49500.0
        
        self.risk_controller.update_position(symbol, position_size, current_price, entry_price)
        
        # è®¡ç®—æŠ•èµ„ç»„åˆé£é™©
        risk_metrics = self.risk_controller.calculate_portfolio_risk()
        
        end_time = time.time()
        latency = (end_time - start_time) * 1000
        
        # éªŒè¯é£é™©æŒ‡æ ‡
        assert risk_metrics is not None, "é£é™©æŒ‡æ ‡ä¸èƒ½ä¸ºç©º"
        assert risk_metrics.portfolio_value > 0, "æŠ•èµ„ç»„åˆä»·å€¼å¿…é¡»å¤§äº0"
        assert risk_metrics.leverage_ratio >= 0, "æ æ†æ¯”ç‡ä¸èƒ½ä¸ºè´Ÿ"
        
        return {
            'risk_metrics': {
                'portfolio_value': risk_metrics.portfolio_value,
                'leverage_ratio': risk_metrics.leverage_ratio,
                'max_drawdown': risk_metrics.max_drawdown
            },
            'latency_ms': latency,
            'success': True
        }
    
    async def test_order_routing_execution(self) -> Dict[str, Any]:
        """æµ‹è¯•è®¢å•è·¯ç”±æ‰§è¡Œ"""
        if not self.order_router or not self.execution_engine:
            raise Exception("è®¢å•è·¯ç”±å™¨æˆ–æ‰§è¡Œå¼•æ“æœªåˆå§‹åŒ–")
        
        start_time = time.time()
        
        # åˆ›å»ºæµ‹è¯•è®¢å•
        order = OrderRequest(
            symbol="BTC/USDT",
            side=OrderSide.BUY,
            order_type=OrderType.MARKET,
            quantity=0.1,
            max_slippage=0.002,
            urgency=0.7,
            client_order_id="test_order_e2e"
        )
        
        # è·¯ç”±è®¢å•
        route = await self.order_router.route_order(order)
        
        routing_time = time.time()
        routing_latency = (routing_time - start_time) * 1000
        
        # éªŒè¯è·¯ç”±ç»“æœ
        assert route is not None, "è·¯ç”±ç»“æœä¸èƒ½ä¸ºç©º"
        assert len(route.segments) > 0, "è·¯ç”±å¿…é¡»åŒ…å«æ‰§è¡Œç‰‡æ®µ"
        assert route.confidence_score > 0, "è·¯ç”±ç½®ä¿¡åº¦å¿…é¡»å¤§äº0"
        
        # æ‰§è¡Œè·¯ç”±
        task_ids = await self.execution_engine.execute_route(route, ExecutionPriority.HIGH)
        
        execution_time = time.time()
        execution_latency = (execution_time - routing_time) * 1000
        total_latency = (execution_time - start_time) * 1000
        
        # éªŒè¯æ‰§è¡Œç»“æœ
        assert len(task_ids) > 0, "æ‰§è¡Œä»»åŠ¡IDä¸èƒ½ä¸ºç©º"
        
        return {
            'route': {
                'segments': len(route.segments),
                'confidence_score': route.confidence_score,
                'expected_slippage': route.expected_slippage
            },
            'execution': {
                'task_count': len(task_ids)
            },
            'latency_ms': total_latency,
            'routing_latency_ms': routing_latency,
            'execution_latency_ms': execution_latency,
            'success': True
        }
    
    async def test_end_to_end_trading_flow(self) -> Dict[str, Any]:
        """æµ‹è¯•ç«¯åˆ°ç«¯äº¤æ˜“æµç¨‹"""
        start_time = time.time()
        
        # 1. AIå†³ç­–
        ai_decision = await self.test_ai_decision_making()
        decision_time = time.time()
        
        # 2. é£é™©æ§åˆ¶
        risk_result = await self.test_risk_control_integration()
        risk_time = time.time()
        
        # 3. è®¢å•æ‰§è¡Œ (å¦‚æœå†³ç­–æ˜¯ä¹°å…¥æˆ–å–å‡º)
        execution_result = None
        if ai_decision['decision']['final_action'] in ['buy', 'sell']:
            execution_result = await self.test_order_routing_execution()
        
        end_time = time.time()
        total_latency = (end_time - start_time) * 1000
        
        return {
            'ai_decision': ai_decision,
            'risk_control': risk_result,
            'execution': execution_result,
            'total_latency_ms': total_latency,
            'decision_latency_ms': (decision_time - start_time) * 1000,
            'risk_latency_ms': (risk_time - decision_time) * 1000,
            'success': True
        }
    
    async def test_system_stress(self) -> Dict[str, Any]:
        """ç³»ç»Ÿå‹åŠ›æµ‹è¯•"""
        start_time = time.time()
        
        # å¹¶å‘æ‰§è¡Œå¤šä¸ªäº¤æ˜“æµç¨‹
        concurrent_flows = 10
        tasks = [self.test_end_to_end_trading_flow() for _ in range(concurrent_flows)]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # ç»Ÿè®¡ç»“æœ
        successful_flows = sum(1 for r in results if isinstance(r, dict) and r.get('success'))
        failed_flows = concurrent_flows - successful_flows
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        if successful_flows > 0:
            avg_latency = np.mean([
                r['total_latency_ms'] for r in results 
                if isinstance(r, dict) and 'total_latency_ms' in r
            ])
        else:
            avg_latency = 0
        
        throughput = successful_flows / duration
        
        return {
            'concurrent_flows': concurrent_flows,
            'successful_flows': successful_flows,
            'failed_flows': failed_flows,
            'success_rate': successful_flows / concurrent_flows,
            'avg_latency_ms': avg_latency,
            'throughput': throughput,
            'duration_seconds': duration,
            'success': failed_flows == 0
        }
    
    def setup_standard_test_suite(self):
        """è®¾ç½®æ ‡å‡†æµ‹è¯•å¥—ä»¶"""
        # AIå†³ç­–æµ‹è¯•
        self.register_test_case(TestCase(
            test_id="ai_decision_test",
            name="AIå†³ç­–åˆ¶å®šæµ‹è¯•",
            description="æµ‹è¯•AIç³»ç»Ÿçš„å†³ç­–åˆ¶å®šèƒ½åŠ›",
            test_type=TestType.INTEGRATION_TEST,
            test_function=self.test_ai_decision_making,
            timeout_seconds=30.0,
            tags=["ai", "decision"]
        ))
        
        # é£é™©æ§åˆ¶æµ‹è¯•
        self.register_test_case(TestCase(
            test_id="risk_control_test",
            name="é£é™©æ§åˆ¶é›†æˆæµ‹è¯•",
            description="æµ‹è¯•é£é™©æ§åˆ¶ç³»ç»Ÿçš„é›†æˆåŠŸèƒ½",
            test_type=TestType.INTEGRATION_TEST,
            test_function=self.test_risk_control_integration,
            timeout_seconds=30.0,
            tags=["risk", "control"]
        ))
        
        # è®¢å•è·¯ç”±æ‰§è¡Œæµ‹è¯•
        self.register_test_case(TestCase(
            test_id="order_routing_test",
            name="è®¢å•è·¯ç”±æ‰§è¡Œæµ‹è¯•",
            description="æµ‹è¯•è®¢å•è·¯ç”±å’Œæ‰§è¡Œçš„å®Œæ•´æµç¨‹",
            test_type=TestType.INTEGRATION_TEST,
            test_function=self.test_order_routing_execution,
            timeout_seconds=60.0,
            dependencies=["ai_decision_test"],
            tags=["routing", "execution"]
        ))
        
        # ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•
        self.register_test_case(TestCase(
            test_id="e2e_trading_flow_test",
            name="ç«¯åˆ°ç«¯äº¤æ˜“æµç¨‹æµ‹è¯•",
            description="æµ‹è¯•å®Œæ•´çš„äº¤æ˜“å†³ç­–åˆ°æ‰§è¡Œæµç¨‹",
            test_type=TestType.INTEGRATION_TEST,
            test_function=self.test_end_to_end_trading_flow,
            timeout_seconds=120.0,
            dependencies=["ai_decision_test", "risk_control_test", "order_routing_test"],
            tags=["e2e", "trading"]
        ))
        
        # å‹åŠ›æµ‹è¯•
        self.register_test_case(TestCase(
            test_id="system_stress_test",
            name="ç³»ç»Ÿå‹åŠ›æµ‹è¯•",
            description="æµ‹è¯•ç³»ç»Ÿåœ¨é«˜å¹¶å‘ä¸‹çš„æ€§èƒ½è¡¨ç°",
            test_type=TestType.STRESS_TEST,
            test_function=self.test_system_stress,
            timeout_seconds=300.0,
            dependencies=["e2e_trading_flow_test"],
            tags=["stress", "performance"]
        ))
        
        logger.info("æ ‡å‡†æµ‹è¯•å¥—ä»¶è®¾ç½®å®Œæˆ")
    
    def get_test_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.test_results:
            return {"message": "æ²¡æœ‰æµ‹è¯•ç»“æœ"}
        
        results = list(self.test_results.values())
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.status == TestStatus.PASSED)
        failed_tests = sum(1 for r in results if r.status == TestStatus.FAILED)
        error_tests = sum(1 for r in results if r.status == TestStatus.ERROR)
        
        # æ€§èƒ½ç»Ÿè®¡
        durations = [r.duration for r in results if r.duration > 0]
        avg_duration = np.mean(durations) if durations else 0
        max_duration = np.max(durations) if durations else 0
        
        # å¥åº·æŒ‡æ ‡
        latest_health = self.health_metrics[-1] if self.health_metrics else None
        
        return {
            'summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': failed_tests,
                'error_tests': error_tests,
                'success_rate': passed_tests / total_tests if total_tests > 0 else 0,
                'avg_duration': avg_duration,
                'max_duration': max_duration
            },
            'test_results': {
                test_id: {
                    'status': result.status.value,
                    'duration': result.duration,
                    'error_message': result.error_message,
                    'performance_metrics': result.performance_metrics
                }
                for test_id, result in self.test_results.items()
            },
            'system_health': {
                'cpu_usage': latest_health.cpu_usage if latest_health else 0,
                'memory_usage': latest_health.memory_usage if latest_health else 0,
                'error_rate': latest_health.error_rate if latest_health else 0,
                'throughput': latest_health.throughput if latest_health else 0
            } if latest_health else {},
            'timestamp': time.time()
        }
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring_active = False
        logger.info("ç«¯åˆ°ç«¯æµ‹è¯•ç›‘æ§å·²åœæ­¢")


# å…¨å±€æµ‹è¯•å™¨å®ä¾‹
e2e_tester = EndToEndTester()


async def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•...")
    
    try:
        # è®¾ç½®æ ‡å‡†æµ‹è¯•å¥—ä»¶
        e2e_tester.setup_standard_test_suite()
        
        # æ¨¡æ‹Ÿç³»ç»Ÿç»„ä»¶åˆå§‹åŒ–
        components = {
            'ai_evolution_system': None,  # å®é™…åº”è¯¥æ˜¯çœŸå®çš„ç»„ä»¶
            'risk_controller': None,
            'order_router': None,
            'execution_engine': None,
            'exchange_connectors': {}
        }
        
        e2e_tester.initialize_system_components(components)
        
        # è¿è¡Œæµ‹è¯•å¥—ä»¶
        results = await e2e_tester.run_test_suite()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = e2e_tester.get_test_report()
        
        logger.info("æµ‹è¯•æŠ¥å‘Š:")
        logger.info(json.dumps(report, indent=2))
        
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·...")
    finally:
        e2e_tester.stop_monitoring()


if __name__ == "__main__":
    asyncio.run(main())
