"""
ğŸ”§ æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨
ç”Ÿäº§çº§å¤šç­–ç•¥ç»„åˆä¼˜åŒ–ç³»ç»Ÿï¼Œå®ç°é©¬ç§‘ç»´èŒ¨ä¼˜åŒ–ã€é£é™©å¹³ä»·ã€Kellyå…¬å¼ç­‰å®Œæ•´ç®—æ³•
æ”¯æŒå®æ—¶æƒé‡è°ƒæ•´ã€é£é™©çº¦æŸå’Œæ€§èƒ½ç›‘æ§
"""

import asyncio
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import scipy.optimize as sco
from scipy import linalg
import cvxpy as cp
from datetime import datetime, timedelta
import logging
from concurrent.futures import ThreadPoolExecutor
import warnings
warnings.filterwarnings('ignore')

from loguru import logger
from src.core.config import settings


class OptimizationMethod(Enum):
    """ä¼˜åŒ–æ–¹æ³•æšä¸¾"""
    MARKOWITZ = "markowitz"              # é©¬ç§‘ç»´èŒ¨å‡å€¼æ–¹å·®ä¼˜åŒ–
    RISK_PARITY = "risk_parity"          # é£é™©å¹³ä»·
    KELLY_CRITERION = "kelly_criterion"   # Kellyå…¬å¼
    BLACK_LITTERMAN = "black_litterman"   # Black-Littermanæ¨¡å‹
    HIERARCHICAL_RISK_PARITY = "hrp"     # å±‚æ¬¡é£é™©å¹³ä»·
    MINIMUM_VARIANCE = "min_variance"     # æœ€å°æ–¹å·®
    MAXIMUM_SHARPE = "max_sharpe"        # æœ€å¤§å¤æ™®æ¯”ç‡
    EQUAL_WEIGHT = "equal_weight"        # ç­‰æƒé‡


@dataclass
class OptimizationConstraints:
    """ä¼˜åŒ–çº¦æŸæ¡ä»¶"""
    min_weight: float = 0.0              # æœ€å°æƒé‡
    max_weight: float = 1.0              # æœ€å¤§æƒé‡
    max_concentration: float = 0.4       # æœ€å¤§é›†ä¸­åº¦
    target_return: Optional[float] = None # ç›®æ ‡æ”¶ç›Šç‡
    max_volatility: Optional[float] = None# æœ€å¤§æ³¢åŠ¨ç‡
    max_drawdown: float = 0.2            # æœ€å¤§å›æ’¤é™åˆ¶
    turnover_limit: float = 0.5          # æ¢æ‰‹ç‡é™åˆ¶
    sector_limits: Dict[str, float] = field(default_factory=dict)  # è¡Œä¸šé™åˆ¶


@dataclass
class PortfolioMetrics:
    """ç»„åˆæ€§èƒ½æŒ‡æ ‡"""
    weights: np.ndarray                  # æƒé‡å‘é‡
    expected_return: float               # é¢„æœŸæ”¶ç›Šç‡
    volatility: float                    # æ³¢åŠ¨ç‡
    sharpe_ratio: float                  # å¤æ™®æ¯”ç‡
    sortino_ratio: float                 # Sortinoæ¯”ç‡
    max_drawdown: float                  # æœ€å¤§å›æ’¤
    var_95: float                        # 95% VaR
    cvar_95: float                       # 95% CVaR
    diversification_ratio: float         # åˆ†æ•£åŒ–æ¯”ç‡
    concentration_index: float           # é›†ä¸­åº¦æŒ‡æ•°
    turnover: float                      # æ¢æ‰‹ç‡
    tracking_error: Optional[float] = None # è·Ÿè¸ªè¯¯å·®


class PortfolioOptimizer:
    """æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.risk_free_rate = 0.02  # æ— é£é™©åˆ©ç‡2%
        self.lookback_period = 252  # å›æœ›æœŸ252ä¸ªäº¤æ˜“æ—¥
        self.rebalance_frequency = 5  # 5å¤©é‡æ–°å¹³è¡¡ä¸€æ¬¡
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # ä¼˜åŒ–å†å²è®°å½•
        self.optimization_history: List[Dict[str, Any]] = []
        
        logger.info("æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def optimize_portfolio(
        self,
        returns: pd.DataFrame,
        method: OptimizationMethod = OptimizationMethod.MARKOWITZ,
        constraints: Optional[OptimizationConstraints] = None,
        current_weights: Optional[np.ndarray] = None,
        market_views: Optional[Dict[str, float]] = None
    ) -> PortfolioMetrics:
        """
        ä¼˜åŒ–æŠ•èµ„ç»„åˆ
        
        Args:
            returns: æ”¶ç›Šç‡æ•°æ® (DataFrame)
            method: ä¼˜åŒ–æ–¹æ³•
            constraints: çº¦æŸæ¡ä»¶
            current_weights: å½“å‰æƒé‡
            market_views: å¸‚åœºè§‚ç‚¹ (ç”¨äºBlack-Litterman)
        
        Returns:
            PortfolioMetrics: ä¼˜åŒ–åçš„ç»„åˆæŒ‡æ ‡
        """
        try:
            if constraints is None:
                constraints = OptimizationConstraints()
            
            # æ•°æ®é¢„å¤„ç†
            returns_clean = self._preprocess_returns(returns)
            
            # è®¡ç®—åæ–¹å·®çŸ©é˜µå’ŒæœŸæœ›æ”¶ç›Š
            cov_matrix = self._calculate_covariance_matrix(returns_clean)
            expected_returns = self._calculate_expected_returns(returns_clean, method)
            
            # æ ¹æ®æ–¹æ³•é€‰æ‹©ä¼˜åŒ–ç®—æ³•
            if method == OptimizationMethod.MARKOWITZ:
                weights = await self._markowitz_optimization(
                    expected_returns, cov_matrix, constraints
                )
            elif method == OptimizationMethod.RISK_PARITY:
                weights = await self._risk_parity_optimization(
                    cov_matrix, constraints
                )
            elif method == OptimizationMethod.KELLY_CRITERION:
                weights = await self._kelly_optimization(
                    returns_clean, constraints
                )
            elif method == OptimizationMethod.BLACK_LITTERMAN:
                weights = await self._black_litterman_optimization(
                    returns_clean, cov_matrix, market_views, constraints
                )
            elif method == OptimizationMethod.HIERARCHICAL_RISK_PARITY:
                weights = await self._hrp_optimization(
                    returns_clean, constraints
                )
            elif method == OptimizationMethod.MINIMUM_VARIANCE:
                weights = await self._minimum_variance_optimization(
                    cov_matrix, constraints
                )
            elif method == OptimizationMethod.MAXIMUM_SHARPE:
                weights = await self._maximum_sharpe_optimization(
                    expected_returns, cov_matrix, constraints
                )
            else:  # EQUAL_WEIGHT
                weights = await self._equal_weight_optimization(
                    len(returns_clean.columns), constraints
                )
            
            # è®¡ç®—ç»„åˆæ€§èƒ½æŒ‡æ ‡
            metrics = self._calculate_portfolio_metrics(
                weights, expected_returns, cov_matrix, returns_clean, current_weights
            )
            
            # è®°å½•ä¼˜åŒ–å†å²
            self._record_optimization(method, constraints, metrics)
            
            logger.info(f"ç»„åˆä¼˜åŒ–å®Œæˆ - æ–¹æ³•: {method.value}, å¤æ™®æ¯”ç‡: {metrics.sharpe_ratio:.3f}")
            return metrics
            
        except Exception as e:
            logger.error(f"ç»„åˆä¼˜åŒ–å¤±è´¥: {e}")
            raise
    
    def _preprocess_returns(self, returns: pd.DataFrame) -> pd.DataFrame:
        """é¢„å¤„ç†æ”¶ç›Šç‡æ•°æ®"""
        # å»é™¤ç¼ºå¤±å€¼
        returns_clean = returns.dropna()
        
        # å»é™¤å¼‚å¸¸å€¼ (3å€æ ‡å‡†å·®)
        for col in returns_clean.columns:
            mean = returns_clean[col].mean()
            std = returns_clean[col].std()
            returns_clean[col] = returns_clean[col].clip(
                lower=mean - 3*std, upper=mean + 3*std
            )
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
        if len(returns_clean) < 30:
            raise ValueError("æ”¶ç›Šç‡æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦30ä¸ªè§‚æµ‹å€¼")
        
        return returns_clean
    
    def _calculate_covariance_matrix(self, returns: pd.DataFrame) -> np.ndarray:
        """è®¡ç®—åæ–¹å·®çŸ©é˜µ"""
        # ä½¿ç”¨æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡
        lambda_decay = 0.94
        weights = np.array([lambda_decay**i for i in range(len(returns))][::-1])
        weights = weights / weights.sum()
        
        # è®¡ç®—åŠ æƒåæ–¹å·®çŸ©é˜µ
        returns_centered = returns - returns.mean()
        cov_matrix = np.cov(returns_centered.T, aweights=weights)
        
        # ç¡®ä¿åæ–¹å·®çŸ©é˜µæ­£å®š
        eigenvals, eigenvecs = linalg.eigh(cov_matrix)
        eigenvals = np.maximum(eigenvals, 1e-8)
        cov_matrix = eigenvecs @ np.diag(eigenvals) @ eigenvecs.T
        
        return cov_matrix
    
    def _calculate_expected_returns(
        self, 
        returns: pd.DataFrame, 
        method: OptimizationMethod
    ) -> np.ndarray:
        """è®¡ç®—æœŸæœ›æ”¶ç›Šç‡"""
        if method in [OptimizationMethod.MINIMUM_VARIANCE, OptimizationMethod.RISK_PARITY]:
            # æœ€å°æ–¹å·®å’Œé£é™©å¹³ä»·ä¸éœ€è¦æœŸæœ›æ”¶ç›Š
            return np.zeros(len(returns.columns))
        
        # ä½¿ç”¨æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡è®¡ç®—æœŸæœ›æ”¶ç›Š
        lambda_decay = 0.94
        weights = np.array([lambda_decay**i for i in range(len(returns))][::-1])
        weights = weights / weights.sum()
        
        expected_returns = np.average(returns.values, axis=0, weights=weights)
        
        # å¹´åŒ–æ”¶ç›Šç‡
        expected_returns = expected_returns * 252
        
        return expected_returns
    
    async def _markowitz_optimization(
        self,
        expected_returns: np.ndarray,
        cov_matrix: np.ndarray,
        constraints: OptimizationConstraints
    ) -> np.ndarray:
        """é©¬ç§‘ç»´èŒ¨å‡å€¼æ–¹å·®ä¼˜åŒ–"""
        n_assets = len(expected_returns)
        
        # å®šä¹‰ä¼˜åŒ–å˜é‡
        w = cp.Variable(n_assets)
        
        # ç›®æ ‡å‡½æ•°ï¼šæœ€å¤§åŒ–æ•ˆç”¨ (æ”¶ç›Š - é£é™©åŒæ¶ç³»æ•° * æ–¹å·®)
        risk_aversion = 1.0
        portfolio_return = expected_returns.T @ w
        portfolio_variance = cp.quad_form(w, cov_matrix)
        utility = portfolio_return - 0.5 * risk_aversion * portfolio_variance
        
        # çº¦æŸæ¡ä»¶
        constraints_list = [
            cp.sum(w) == 1,  # æƒé‡å’Œä¸º1
            w >= constraints.min_weight,  # æœ€å°æƒé‡
            w <= constraints.max_weight   # æœ€å¤§æƒé‡
        ]
        
        # ç›®æ ‡æ”¶ç›Šçº¦æŸ
        if constraints.target_return is not None:
            constraints_list.append(portfolio_return >= constraints.target_return)
        
        # æœ€å¤§æ³¢åŠ¨ç‡çº¦æŸ
        if constraints.max_volatility is not None:
            constraints_list.append(
                cp.sqrt(portfolio_variance) <= constraints.max_volatility
            )
        
        # æœ€å¤§é›†ä¸­åº¦çº¦æŸ
        if constraints.max_concentration < 1.0:
            constraints_list.append(w <= constraints.max_concentration)
        
        # æ±‚è§£ä¼˜åŒ–é—®é¢˜
        problem = cp.Problem(cp.Maximize(utility), constraints_list)
        problem.solve(solver=cp.ECOS)
        
        if problem.status not in ["infeasible", "unbounded"]:
            return w.value
        else:
            logger.warning("é©¬ç§‘ç»´èŒ¨ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç­‰æƒé‡")
            return np.ones(n_assets) / n_assets
    
    async def _risk_parity_optimization(
        self,
        cov_matrix: np.ndarray,
        constraints: OptimizationConstraints
    ) -> np.ndarray:
        """é£é™©å¹³ä»·ä¼˜åŒ–"""
        n_assets = len(cov_matrix)
        
        def risk_parity_objective(weights):
            """é£é™©å¹³ä»·ç›®æ ‡å‡½æ•°"""
            weights = np.array(weights)
            portfolio_vol = np.sqrt(weights.T @ cov_matrix @ weights)
            
            # è®¡ç®—è¾¹é™…é£é™©è´¡çŒ®
            marginal_contrib = cov_matrix @ weights / portfolio_vol
            contrib = weights * marginal_contrib
            
            # ç›®æ ‡ï¼šæœ€å°åŒ–é£é™©è´¡çŒ®çš„å·®å¼‚
            target_contrib = portfolio_vol / n_assets
            return np.sum((contrib - target_contrib) ** 2)
        
        # çº¦æŸæ¡ä»¶
        constraints_opt = [
            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},  # æƒé‡å’Œä¸º1
        ]
        
        # è¾¹ç•Œæ¡ä»¶
        bounds = [(constraints.min_weight, constraints.max_weight) for _ in range(n_assets)]
        
        # åˆå§‹æƒé‡
        x0 = np.ones(n_assets) / n_assets
        
        # ä¼˜åŒ–
        result = sco.minimize(
            risk_parity_objective,
            x0,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints_opt,
            options={'maxiter': 1000}
        )
        
        if result.success:
            return result.x
        else:
            logger.warning("é£é™©å¹³ä»·ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç­‰æƒé‡")
            return np.ones(n_assets) / n_assets
    
    async def _kelly_optimization(
        self,
        returns: pd.DataFrame,
        constraints: OptimizationConstraints
    ) -> np.ndarray:
        """Kellyå…¬å¼ä¼˜åŒ–"""
        n_assets = len(returns.columns)
        
        def kelly_objective(weights):
            """Kellyå…¬å¼ç›®æ ‡å‡½æ•°"""
            weights = np.array(weights)
            
            # è®¡ç®—ç»„åˆæ”¶ç›Šç‡
            portfolio_returns = (returns.values @ weights)
            
            # Kellyå…¬å¼ï¼šæœ€å¤§åŒ–å¯¹æ•°æœŸæœ›æ”¶ç›Š
            # é¿å…è´Ÿæ”¶ç›Šå¯¼è‡´çš„å¯¹æ•°é—®é¢˜
            portfolio_returns = np.maximum(portfolio_returns, -0.99)
            log_returns = np.log(1 + portfolio_returns)
            
            return -np.mean(log_returns)  # è´Ÿå·å› ä¸ºè¦æœ€å¤§åŒ–
        
        # çº¦æŸæ¡ä»¶
        constraints_opt = [
            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},  # æƒé‡å’Œä¸º1
        ]
        
        # è¾¹ç•Œæ¡ä»¶
        bounds = [(constraints.min_weight, constraints.max_weight) for _ in range(n_assets)]
        
        # åˆå§‹æƒé‡
        x0 = np.ones(n_assets) / n_assets
        
        # ä¼˜åŒ–
        result = sco.minimize(
            kelly_objective,
            x0,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints_opt,
            options={'maxiter': 1000}
        )
        
        if result.success:
            return result.x
        else:
            logger.warning("Kellyä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç­‰æƒé‡")
            return np.ones(n_assets) / n_assets
    
    async def _black_litterman_optimization(
        self,
        returns: pd.DataFrame,
        cov_matrix: np.ndarray,
        market_views: Optional[Dict[str, float]],
        constraints: OptimizationConstraints
    ) -> np.ndarray:
        """Black-Littermanæ¨¡å‹ä¼˜åŒ–"""
        n_assets = len(returns.columns)
        
        # å¸‚åœºå‡è¡¡æ”¶ç›Šç‡ (ä½¿ç”¨å†å²æ”¶ç›Šç‡)
        market_returns = returns.mean().values * 252
        
        # å¦‚æœæ²¡æœ‰å¸‚åœºè§‚ç‚¹ï¼Œä½¿ç”¨å¸‚åœºå‡è¡¡
        if market_views is None:
            expected_returns = market_returns
        else:
            # æ„å»ºè§‚ç‚¹çŸ©é˜µ
            P = np.zeros((len(market_views), n_assets))
            Q = np.zeros(len(market_views))
            
            for i, (asset, view) in enumerate(market_views.items()):
                if asset in returns.columns:
                    asset_idx = returns.columns.get_loc(asset)
                    P[i, asset_idx] = 1
                    Q[i] = view
            
            # Black-Littermanå…¬å¼
            tau = 0.025  # ä¸ç¡®å®šæ€§å‚æ•°
            omega = np.eye(len(market_views)) * 0.01  # è§‚ç‚¹ä¸ç¡®å®šæ€§
            
            # è®¡ç®—æ–°çš„æœŸæœ›æ”¶ç›Šç‡
            M1 = linalg.inv(tau * cov_matrix)
            M2 = P.T @ linalg.inv(omega) @ P
            M3 = linalg.inv(M1 + M2)
            
            mu_bl = M3 @ (M1 @ market_returns + P.T @ linalg.inv(omega) @ Q)
            expected_returns = mu_bl
        
        # ä½¿ç”¨é©¬ç§‘ç»´èŒ¨ä¼˜åŒ–
        return await self._markowitz_optimization(expected_returns, cov_matrix, constraints)
    
    async def _hrp_optimization(
        self,
        returns: pd.DataFrame,
        constraints: OptimizationConstraints
    ) -> np.ndarray:
        """å±‚æ¬¡é£é™©å¹³ä»·ä¼˜åŒ–"""
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        corr_matrix = returns.corr().values
        
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        distance_matrix = np.sqrt(0.5 * (1 - corr_matrix))
        
        # å±‚æ¬¡èšç±»
        from scipy.cluster.hierarchy import linkage, dendrogram
        from scipy.spatial.distance import squareform
        
        # è½¬æ¢ä¸ºè·ç¦»å‘é‡
        distance_vector = squareform(distance_matrix, checks=False)
        
        # æ‰§è¡Œèšç±»
        linkage_matrix = linkage(distance_vector, method='single')
        
        # é€’å½’äºŒåˆ†æ³•åˆ†é…æƒé‡
        def _get_cluster_var(cov_matrix, cluster_items):
            """è®¡ç®—èšç±»æ–¹å·®"""
            cluster_cov = cov_matrix[np.ix_(cluster_items, cluster_items)]
            inv_diag = 1 / np.diag(cluster_cov)
            weights = inv_diag / inv_diag.sum()
            cluster_var = weights.T @ cluster_cov @ weights
            return cluster_var
        
        def _get_quasi_diag(linkage_matrix):
            """è·å–å‡†å¯¹è§’åŒ–æ’åº"""
            link = linkage_matrix.astype(int)
            sort_ix = pd.Series([link[-1, 0], link[-1, 1]])
            num_items = link[-1, 3]
            
            while sort_ix.max() >= num_items:
                sort_ix.index = range(0, sort_ix.shape[0] * 2, 2)
                df0 = sort_ix[sort_ix >= num_items]
                i = df0.index
                j = df0.values - num_items
                sort_ix[i] = link[j, 0]
                df0 = pd.Series(link[j, 1], index=i + 1)
                sort_ix = sort_ix.append(df0).sort_index()
                sort_ix.index = range(sort_ix.shape[0])
            
            return sort_ix.tolist()
        
        def _get_rec_bipart(cov_matrix, sort_ix):
            """é€’å½’äºŒåˆ†æ³•"""
            weights = pd.Series(1, index=sort_ix)
            cluster_items = [sort_ix]
            
            while len(cluster_items) > 0:
                cluster_items = [
                    i[j:k] for i in cluster_items
                    for j, k in ((0, len(i) // 2), (len(i) // 2, len(i)))
                    if len(i) > 1
                ]
                
                for i in range(0, len(cluster_items), 2):
                    cluster0 = cluster_items[i]
                    cluster1 = cluster_items[i + 1]
                    
                    cluster_var0 = _get_cluster_var(cov_matrix, cluster0)
                    cluster_var1 = _get_cluster_var(cov_matrix, cluster1)
                    
                    alpha = 1 - cluster_var0 / (cluster_var0 + cluster_var1)
                    
                    weights[cluster0] *= alpha
                    weights[cluster1] *= 1 - alpha
            
            return weights.values
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
        cov_matrix = returns.cov().values * 252
        
        # è·å–æ’åº
        sort_ix = _get_quasi_diag(linkage_matrix)
        
        # è®¡ç®—HRPæƒé‡
        hrp_weights = _get_rec_bipart(cov_matrix, sort_ix)
        
        # é‡æ–°æ’åºåˆ°åŸå§‹é¡ºåº
        weights = np.zeros(len(returns.columns))
        for i, idx in enumerate(sort_ix):
            weights[idx] = hrp_weights[i]
        
        return weights
    
    async def _minimum_variance_optimization(
        self,
        cov_matrix: np.ndarray,
        constraints: OptimizationConstraints
    ) -> np.ndarray:
        """æœ€å°æ–¹å·®ä¼˜åŒ–"""
        n_assets = len(cov_matrix)
        
        # å®šä¹‰ä¼˜åŒ–å˜é‡
        w = cp.Variable(n_assets)
        
        # ç›®æ ‡å‡½æ•°ï¼šæœ€å°åŒ–æ–¹å·®
        portfolio_variance = cp.quad_form(w, cov_matrix)
        
        # çº¦æŸæ¡ä»¶
        constraints_list = [
            cp.sum(w) == 1,  # æƒé‡å’Œä¸º1
            w >= constraints.min_weight,  # æœ€å°æƒé‡
            w <= constraints.max_weight   # æœ€å¤§æƒé‡
        ]
        
        # æœ€å¤§é›†ä¸­åº¦çº¦æŸ
        if constraints.max_concentration < 1.0:
            constraints_list.append(w <= constraints.max_concentration)
        
        # æ±‚è§£ä¼˜åŒ–é—®é¢˜
        problem = cp.Problem(cp.Minimize(portfolio_variance), constraints_list)
        problem.solve(solver=cp.ECOS)
        
        if problem.status not in ["infeasible", "unbounded"]:
            return w.value
        else:
            logger.warning("æœ€å°æ–¹å·®ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç­‰æƒé‡")
            return np.ones(n_assets) / n_assets
    
    async def _maximum_sharpe_optimization(
        self,
        expected_returns: np.ndarray,
        cov_matrix: np.ndarray,
        constraints: OptimizationConstraints
    ) -> np.ndarray:
        """æœ€å¤§å¤æ™®æ¯”ç‡ä¼˜åŒ–"""
        n_assets = len(expected_returns)
        
        # å®šä¹‰ä¼˜åŒ–å˜é‡
        w = cp.Variable(n_assets)
        
        # ç›®æ ‡å‡½æ•°ï¼šæœ€å¤§åŒ–å¤æ™®æ¯”ç‡
        # ä½¿ç”¨äºŒæ¬¡è§„åˆ’å½¢å¼
        portfolio_return = expected_returns.T @ w - self.risk_free_rate
        portfolio_variance = cp.quad_form(w, cov_matrix)
        
        # çº¦æŸæ¡ä»¶
        constraints_list = [
            cp.sum(w) == 1,  # æƒé‡å’Œä¸º1
            w >= constraints.min_weight,  # æœ€å°æƒé‡
            w <= constraints.max_weight,  # æœ€å¤§æƒé‡
            portfolio_return >= 0.01  # æœ€å°è¶…é¢æ”¶ç›Š
        ]
        
        # æœ€å¤§é›†ä¸­åº¦çº¦æŸ
        if constraints.max_concentration < 1.0:
            constraints_list.append(w <= constraints.max_concentration)
        
        # æ±‚è§£ä¼˜åŒ–é—®é¢˜ (æœ€å¤§åŒ–æ”¶ç›Š/é£é™©æ¯”)
        problem = cp.Problem(
            cp.Maximize(portfolio_return / cp.sqrt(portfolio_variance)),
            constraints_list
        )
        
        try:
            problem.solve(solver=cp.ECOS)
            if problem.status not in ["infeasible", "unbounded"]:
                return w.value
        except:
            pass
        
        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨æ›¿ä»£æ–¹æ³•
        logger.warning("æœ€å¤§å¤æ™®æ¯”ç‡ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é©¬ç§‘ç»´èŒ¨ä¼˜åŒ–")
        return await self._markowitz_optimization(expected_returns, cov_matrix, constraints)
    
    async def _equal_weight_optimization(
        self,
        n_assets: int,
        constraints: OptimizationConstraints
    ) -> np.ndarray:
        """ç­‰æƒé‡ä¼˜åŒ–"""
        return np.ones(n_assets) / n_assets
    
    def _calculate_portfolio_metrics(
        self,
        weights: np.ndarray,
        expected_returns: np.ndarray,
        cov_matrix: np.ndarray,
        returns: pd.DataFrame,
        current_weights: Optional[np.ndarray] = None
    ) -> PortfolioMetrics:
        """è®¡ç®—ç»„åˆæ€§èƒ½æŒ‡æ ‡"""
        # åŸºæœ¬æŒ‡æ ‡
        portfolio_return = np.dot(weights, expected_returns)
        portfolio_variance = np.dot(weights.T, np.dot(cov_matrix, weights))
        portfolio_volatility = np.sqrt(portfolio_variance)
        
        # å¤æ™®æ¯”ç‡
        sharpe_ratio = (portfolio_return - self.risk_free_rate) / portfolio_volatility
        
        # è®¡ç®—å†å²ç»„åˆæ”¶ç›Šç‡
        portfolio_returns = (returns.values @ weights)
        
        # Sortinoæ¯”ç‡
        downside_returns = portfolio_returns[portfolio_returns < 0]
        downside_deviation = np.sqrt(np.mean(downside_returns**2)) if len(downside_returns) > 0 else 0
        sortino_ratio = (portfolio_return - self.risk_free_rate) / (downside_deviation * np.sqrt(252)) if downside_deviation > 0 else 0
        
        # æœ€å¤§å›æ’¤
        cumulative_returns = (1 + portfolio_returns).cumprod()
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdown = (cumulative_returns - running_max) / running_max
        max_drawdown = np.min(drawdown)
        
        # VaRå’ŒCVaR (95%)
        var_95 = np.percentile(portfolio_returns, 5)
        cvar_95 = np.mean(portfolio_returns[portfolio_returns <= var_95])
        
        # åˆ†æ•£åŒ–æ¯”ç‡
        individual_vols = np.sqrt(np.diag(cov_matrix))
        weighted_avg_vol = np.dot(weights, individual_vols)
        diversification_ratio = weighted_avg_vol / portfolio_volatility
        
        # é›†ä¸­åº¦æŒ‡æ•° (HHI)
        concentration_index = np.sum(weights**2)
        
        # æ¢æ‰‹ç‡
        turnover = 0.0
        if current_weights is not None:
            turnover = np.sum(np.abs(weights - current_weights)) / 2
        
        return PortfolioMetrics(
            weights=weights,
            expected_return=portfolio_return,
            volatility=portfolio_volatility,
            sharpe_ratio=sharpe_ratio,
            sortino_ratio=sortino_ratio,
            max_drawdown=max_drawdown,
            var_95=var_95,
            cvar_95=cvar_95,
            diversification_ratio=diversification_ratio,
            concentration_index=concentration_index,
            turnover=turnover
        )
    
    def _record_optimization(
        self,
        method: OptimizationMethod,
        constraints: OptimizationConstraints,
        metrics: PortfolioMetrics
    ):
        """è®°å½•ä¼˜åŒ–å†å²"""
        record = {
            'timestamp': datetime.now(),
            'method': method.value,
            'constraints': constraints,
            'metrics': metrics,
            'weights': metrics.weights.tolist(),
            'sharpe_ratio': metrics.sharpe_ratio,
            'volatility': metrics.volatility,
            'expected_return': metrics.expected_return
        }
        
        self.optimization_history.append(record)
        
        # ä¿æŒæœ€è¿‘100æ¬¡è®°å½•
        if len(self.optimization_history) > 100:
            self.optimization_history = self.optimization_history[-100:]
    
    async def get_optimization_history(self) -> List[Dict[str, Any]]:
        """è·å–ä¼˜åŒ–å†å²"""
        return self.optimization_history.copy()
    
    async def compare_methods(
        self,
        returns: pd.DataFrame,
        methods: List[OptimizationMethod],
        constraints: Optional[OptimizationConstraints] = None
    ) -> Dict[str, PortfolioMetrics]:
        """æ¯”è¾ƒä¸åŒä¼˜åŒ–æ–¹æ³•"""
        results = {}
        
        for method in methods:
            try:
                metrics = await self.optimize_portfolio(returns, method, constraints)
                results[method.value] = metrics
                logger.info(f"{method.value} - å¤æ™®æ¯”ç‡: {metrics.sharpe_ratio:.3f}")
            except Exception as e:
                logger.error(f"{method.value} ä¼˜åŒ–å¤±è´¥: {e}")
        
        return results
    
    async def shutdown(self):
        """å…³é—­ä¼˜åŒ–å™¨"""
        self.executor.shutdown(wait=True)
        logger.info("æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨å·²å…³é—­")


# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
portfolio_optimizer = PortfolioOptimizer()

