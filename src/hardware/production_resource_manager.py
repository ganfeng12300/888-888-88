"""
ğŸš€ ç”Ÿäº§çº§ç¡¬ä»¶èµ„æºç®¡ç†å™¨
ä¸“ä¸º20æ ¸CPU + GTX3060 12GB + 128GBå†…å­˜ + 1TB NVMe SSDä¼˜åŒ–
å®ç°æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡å’Œæ€§èƒ½ä¼˜åŒ–
"""

import os
import psutil
import threading
import asyncio
import time
import GPUtil
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from loguru import logger
import torch
import multiprocessing as mp

class ResourceType(Enum):
    """èµ„æºç±»å‹"""
    CPU_CORE = "cpu_core"
    GPU_MEMORY = "gpu_memory"
    SYSTEM_MEMORY = "system_memory"
    DISK_SPACE = "disk_space"
    NETWORK_BANDWIDTH = "network_bandwidth"

class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§"""
    CRITICAL = 1    # å…³é”®ä»»åŠ¡ï¼šå®æ—¶äº¤æ˜“æ‰§è¡Œ
    HIGH = 2        # é«˜ä¼˜å…ˆçº§ï¼šAIæ¨¡å‹æ¨ç†
    MEDIUM = 3      # ä¸­ä¼˜å…ˆçº§ï¼šAIæ¨¡å‹è®­ç»ƒ
    LOW = 4         # ä½ä¼˜å…ˆçº§ï¼šæ•°æ®æ¸…ç†ã€æ—¥å¿—

@dataclass
class ResourceAllocation:
    """èµ„æºåˆ†é…é…ç½®"""
    cpu_cores: List[int]
    gpu_memory_mb: int
    system_memory_mb: int
    disk_space_gb: int
    priority: TaskPriority
    task_name: str
    max_threads: int = 1

@dataclass
class HardwareSpecs:
    """ç¡¬ä»¶è§„æ ¼"""
    cpu_cores: int = 20
    gpu_memory_gb: int = 12
    system_memory_gb: int = 128
    disk_space_tb: int = 1
    gpu_model: str = "GTX 3060"

class ProductionResourceManager:
    """ç”Ÿäº§çº§èµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.hardware_specs = HardwareSpecs()
        self.resource_allocations: Dict[str, ResourceAllocation] = {}
        self.active_tasks: Dict[str, Any] = {}
        self.resource_monitor = ResourceMonitor()
        
        # çº¿ç¨‹æ± å’Œè¿›ç¨‹æ± 
        self.thread_pool = ThreadPoolExecutor(max_workers=20)
        self.process_pool = ProcessPoolExecutor(max_workers=10)
        
        # èµ„æºé”
        self.resource_lock = threading.RLock()
        
        # åˆå§‹åŒ–èµ„æºåˆ†é…
        self._initialize_resource_allocation()
        
        logger.info("ğŸš€ ç”Ÿäº§çº§èµ„æºç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ’» ç¡¬ä»¶é…ç½®: {self.hardware_specs.cpu_cores}æ ¸CPU, {self.hardware_specs.gpu_model} {self.hardware_specs.gpu_memory_gb}GB, {self.hardware_specs.system_memory_gb}GBå†…å­˜")

    def _initialize_resource_allocation(self):
        """åˆå§‹åŒ–èµ„æºåˆ†é…ç­–ç•¥"""
        
        # CPUæ ¸å¿ƒåˆ†é…ç­–ç•¥ (20æ ¸)
        cpu_allocations = {
            "real_time_data_collection": [0, 1, 2, 3],      # æ ¸å¿ƒ0-3: å®æ—¶æ•°æ®é‡‡é›†
            "ai_model_inference": [4, 5, 6, 7],             # æ ¸å¿ƒ4-7: AIæ¨¡å‹æ¨ç†
            "ai_model_training": [8, 9, 10, 11],            # æ ¸å¿ƒ8-11: AIæ¨¡å‹è®­ç»ƒ
            "risk_order_management": [12, 13, 14, 15],      # æ ¸å¿ƒ12-15: é£é™©æ§åˆ¶å’Œè®¢å•ç®¡ç†
            "web_monitoring": [16, 17, 18, 19]              # æ ¸å¿ƒ16-19: WebæœåŠ¡å’Œç›‘æ§
        }
        
        # GPUæ˜¾å­˜åˆ†é…ç­–ç•¥ (12GB)
        gpu_allocations = {
            "reinforcement_learning": 4096,      # 4GB: å¼ºåŒ–å­¦ä¹ æ¨¡å‹
            "time_series_deep": 3072,           # 3GB: æ—¶åºæ·±åº¦å­¦ä¹ 
            "ensemble_learning": 2048,          # 2GB: é›†æˆå­¦ä¹ 
            "meta_learning": 2048,              # 2GB: å…ƒå­¦ä¹ 
            "gpu_cache": 896                    # 896MB: GPUç¼“å­˜å’Œä¸´æ—¶è®¡ç®—
        }
        
        # ç³»ç»Ÿå†…å­˜åˆ†é…ç­–ç•¥ (128GB)
        memory_allocations = {
            "real_time_data_cache": 32 * 1024,     # 32GB: å®æ—¶æ•°æ®ç¼“å­˜
            "ai_model_parameters": 24 * 1024,      # 24GB: AIæ¨¡å‹å‚æ•°
            "historical_data_cache": 16 * 1024,    # 16GB: å†å²æ•°æ®ç¼“å­˜
            "system_web_services": 16 * 1024,      # 16GB: ç³»ç»Ÿå’ŒWebæœåŠ¡
            "buffer_reserve": 40 * 1024             # 40GB: é¢„ç•™ç¼“å†²åŒº
        }
        
        # åˆ›å»ºèµ„æºåˆ†é…é…ç½®
        allocations = [
            # å®æ—¶æ•°æ®é‡‡é›† - æœ€é«˜ä¼˜å…ˆçº§
            ResourceAllocation(
                cpu_cores=cpu_allocations["real_time_data_collection"],
                gpu_memory_mb=0,
                system_memory_mb=memory_allocations["real_time_data_cache"],
                disk_space_gb=50,
                priority=TaskPriority.CRITICAL,
                task_name="real_time_data_collection",
                max_threads=4
            ),
            
            # AIæ¨¡å‹æ¨ç† - é«˜ä¼˜å…ˆçº§
            ResourceAllocation(
                cpu_cores=cpu_allocations["ai_model_inference"],
                gpu_memory_mb=gpu_allocations["reinforcement_learning"],
                system_memory_mb=memory_allocations["ai_model_parameters"] // 2,
                disk_space_gb=20,
                priority=TaskPriority.HIGH,
                task_name="ai_model_inference",
                max_threads=4
            ),
            
            # AIæ¨¡å‹è®­ç»ƒ - ä¸­ä¼˜å…ˆçº§
            ResourceAllocation(
                cpu_cores=cpu_allocations["ai_model_training"],
                gpu_memory_mb=gpu_allocations["time_series_deep"] + gpu_allocations["ensemble_learning"],
                system_memory_mb=memory_allocations["ai_model_parameters"] // 2,
                disk_space_gb=100,
                priority=TaskPriority.MEDIUM,
                task_name="ai_model_training",
                max_threads=4
            ),
            
            # é£é™©æ§åˆ¶å’Œè®¢å•ç®¡ç† - æœ€é«˜ä¼˜å…ˆçº§
            ResourceAllocation(
                cpu_cores=cpu_allocations["risk_order_management"],
                gpu_memory_mb=0,
                system_memory_mb=8 * 1024,  # 8GB
                disk_space_gb=30,
                priority=TaskPriority.CRITICAL,
                task_name="risk_order_management",
                max_threads=4
            ),
            
            # WebæœåŠ¡å’Œç›‘æ§ - ä½ä¼˜å…ˆçº§
            ResourceAllocation(
                cpu_cores=cpu_allocations["web_monitoring"],
                gpu_memory_mb=gpu_allocations["gpu_cache"],
                system_memory_mb=memory_allocations["system_web_services"],
                disk_space_gb=50,
                priority=TaskPriority.LOW,
                task_name="web_monitoring",
                max_threads=4
            )
        ]
        
        # æ³¨å†Œèµ„æºåˆ†é…
        for allocation in allocations:
            self.resource_allocations[allocation.task_name] = allocation
            
        logger.info("âœ… èµ„æºåˆ†é…ç­–ç•¥åˆå§‹åŒ–å®Œæˆ")
        self._log_resource_allocation()

    def _log_resource_allocation(self):
        """è®°å½•èµ„æºåˆ†é…æƒ…å†µ"""
        logger.info("ğŸ“Š èµ„æºåˆ†é…è¯¦æƒ…:")
        
        for task_name, allocation in self.resource_allocations.items():
            logger.info(f"  ğŸ”§ {task_name}:")
            logger.info(f"    CPUæ ¸å¿ƒ: {allocation.cpu_cores}")
            logger.info(f"    GPUæ˜¾å­˜: {allocation.gpu_memory_mb}MB")
            logger.info(f"    ç³»ç»Ÿå†…å­˜: {allocation.system_memory_mb//1024}GB")
            logger.info(f"    ç£ç›˜ç©ºé—´: {allocation.disk_space_gb}GB")
            logger.info(f"    ä¼˜å…ˆçº§: {allocation.priority.name}")
            logger.info(f"    æœ€å¤§çº¿ç¨‹: {allocation.max_threads}")

    def allocate_cpu_cores(self, task_name: str) -> List[int]:
        """åˆ†é…CPUæ ¸å¿ƒ"""
        with self.resource_lock:
            if task_name in self.resource_allocations:
                cores = self.resource_allocations[task_name].cpu_cores
                
                # è®¾ç½®CPUäº²å’Œæ€§
                try:
                    os.sched_setaffinity(0, cores)
                    logger.info(f"âœ… ä¸ºä»»åŠ¡ {task_name} åˆ†é…CPUæ ¸å¿ƒ: {cores}")
                    return cores
                except Exception as e:
                    logger.warning(f"âš ï¸ è®¾ç½®CPUäº²å’Œæ€§å¤±è´¥: {e}")
                    return cores
            else:
                logger.error(f"âŒ æœªæ‰¾åˆ°ä»»åŠ¡ {task_name} çš„èµ„æºåˆ†é…é…ç½®")
                return []

    def allocate_gpu_memory(self, task_name: str) -> Optional[int]:
        """åˆ†é…GPUæ˜¾å­˜"""
        with self.resource_lock:
            if task_name in self.resource_allocations:
                gpu_memory = self.resource_allocations[task_name].gpu_memory_mb
                
                if gpu_memory > 0:
                    try:
                        # è®¾ç½®PyTorch GPUå†…å­˜é™åˆ¶
                        if torch.cuda.is_available():
                            torch.cuda.set_per_process_memory_fraction(gpu_memory / (12 * 1024))
                            logger.info(f"âœ… ä¸ºä»»åŠ¡ {task_name} åˆ†é…GPUæ˜¾å­˜: {gpu_memory}MB")
                            return gpu_memory
                    except Exception as e:
                        logger.warning(f"âš ï¸ è®¾ç½®GPUå†…å­˜é™åˆ¶å¤±è´¥: {e}")
                
                return gpu_memory
            else:
                logger.error(f"âŒ æœªæ‰¾åˆ°ä»»åŠ¡ {task_name} çš„èµ„æºåˆ†é…é…ç½®")
                return None

    def allocate_system_memory(self, task_name: str) -> Optional[int]:
        """åˆ†é…ç³»ç»Ÿå†…å­˜"""
        with self.resource_lock:
            if task_name in self.resource_allocations:
                memory_mb = self.resource_allocations[task_name].system_memory_mb
                logger.info(f"âœ… ä¸ºä»»åŠ¡ {task_name} åˆ†é…ç³»ç»Ÿå†…å­˜: {memory_mb//1024}GB")
                return memory_mb
            else:
                logger.error(f"âŒ æœªæ‰¾åˆ°ä»»åŠ¡ {task_name} çš„èµ„æºåˆ†é…é…ç½®")
                return None

    def get_thread_pool(self, task_name: str) -> ThreadPoolExecutor:
        """è·å–çº¿ç¨‹æ± """
        if task_name in self.resource_allocations:
            max_workers = self.resource_allocations[task_name].max_threads
            return ThreadPoolExecutor(max_workers=max_workers)
        else:
            return self.thread_pool

    def get_process_pool(self) -> ProcessPoolExecutor:
        """è·å–è¿›ç¨‹æ± """
        return self.process_pool

    async def start_task(self, task_name: str, task_func, *args, **kwargs):
        """å¯åŠ¨ä»»åŠ¡å¹¶åˆ†é…èµ„æº"""
        with self.resource_lock:
            if task_name in self.active_tasks:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task_name} å·²åœ¨è¿è¡Œ")
                return
            
            # åˆ†é…èµ„æº
            cpu_cores = self.allocate_cpu_cores(task_name)
            gpu_memory = self.allocate_gpu_memory(task_name)
            system_memory = self.allocate_system_memory(task_name)
            
            # å¯åŠ¨ä»»åŠ¡
            try:
                task = asyncio.create_task(task_func(*args, **kwargs))
                self.active_tasks[task_name] = {
                    'task': task,
                    'start_time': time.time(),
                    'cpu_cores': cpu_cores,
                    'gpu_memory': gpu_memory,
                    'system_memory': system_memory
                }
                
                logger.info(f"ğŸš€ ä»»åŠ¡ {task_name} å¯åŠ¨æˆåŠŸ")
                return task
                
            except Exception as e:
                logger.error(f"âŒ å¯åŠ¨ä»»åŠ¡ {task_name} å¤±è´¥: {e}")
                return None

    async def stop_task(self, task_name: str):
        """åœæ­¢ä»»åŠ¡å¹¶é‡Šæ”¾èµ„æº"""
        with self.resource_lock:
            if task_name in self.active_tasks:
                task_info = self.active_tasks[task_name]
                task = task_info['task']
                
                # å–æ¶ˆä»»åŠ¡
                task.cancel()
                
                try:
                    await task
                except asyncio.CancelledError:
                    pass
                
                # é‡Šæ”¾èµ„æº
                del self.active_tasks[task_name]
                
                run_time = time.time() - task_info['start_time']
                logger.info(f"ğŸ›‘ ä»»åŠ¡ {task_name} å·²åœæ­¢ï¼Œè¿è¡Œæ—¶é—´: {run_time:.2f}ç§’")
            else:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task_name} æœªåœ¨è¿è¡Œ")

    def get_resource_usage(self) -> Dict[str, Any]:
        """è·å–èµ„æºä½¿ç”¨æƒ…å†µ"""
        return self.resource_monitor.get_current_usage()

    def optimize_performance(self):
        """æ€§èƒ½ä¼˜åŒ–"""
        logger.info("ğŸ”§ å¼€å§‹æ€§èƒ½ä¼˜åŒ–...")
        
        # CPUä¼˜åŒ–
        self._optimize_cpu()
        
        # GPUä¼˜åŒ–
        self._optimize_gpu()
        
        # å†…å­˜ä¼˜åŒ–
        self._optimize_memory()
        
        # ç£ç›˜ä¼˜åŒ–
        self._optimize_disk()
        
        logger.info("âœ… æ€§èƒ½ä¼˜åŒ–å®Œæˆ")

    def _optimize_cpu(self):
        """CPUä¼˜åŒ–"""
        try:
            # è®¾ç½®CPUè°ƒåº¦ç­–ç•¥
            os.system("echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor")
            logger.info("âœ… CPUè°ƒåº¦ç­–ç•¥è®¾ç½®ä¸ºæ€§èƒ½æ¨¡å¼")
        except Exception as e:
            logger.warning(f"âš ï¸ CPUä¼˜åŒ–å¤±è´¥: {e}")

    def _optimize_gpu(self):
        """GPUä¼˜åŒ–"""
        try:
            if torch.cuda.is_available():
                # å¯ç”¨GPUæ€§èƒ½æ¨¡å¼
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                logger.info("âœ… GPUæ€§èƒ½ä¼˜åŒ–å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ GPUä¼˜åŒ–å¤±è´¥: {e}")

    def _optimize_memory(self):
        """å†…å­˜ä¼˜åŒ–"""
        try:
            # è®¾ç½®å†…å­˜äº¤æ¢ç­–ç•¥
            os.system("echo 10 | sudo tee /proc/sys/vm/swappiness")
            logger.info("âœ… å†…å­˜äº¤æ¢ç­–ç•¥ä¼˜åŒ–å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ å†…å­˜ä¼˜åŒ–å¤±è´¥: {e}")

    def _optimize_disk(self):
        """ç£ç›˜ä¼˜åŒ–"""
        try:
            # è®¾ç½®ç£ç›˜è°ƒåº¦å™¨
            os.system("echo mq-deadline | sudo tee /sys/block/nvme*/queue/scheduler")
            logger.info("âœ… NVMe SSDè°ƒåº¦å™¨ä¼˜åŒ–å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ ç£ç›˜ä¼˜åŒ–å¤±è´¥: {e}")


class ResourceMonitor:
    """èµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self.monitoring = False
        self.monitor_thread = None

    def start_monitoring(self, interval: int = 5):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, args=(interval,))
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        logger.info("ğŸ“Š èµ„æºç›‘æ§å·²å¯åŠ¨")

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        logger.info("ğŸ“Š èµ„æºç›‘æ§å·²åœæ­¢")

    def _monitor_loop(self, interval: int):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                usage = self.get_current_usage()
                self._log_usage(usage)
                time.sleep(interval)
            except Exception as e:
                logger.error(f"âŒ èµ„æºç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(interval)

    def get_current_usage(self) -> Dict[str, Any]:
        """è·å–å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1, percpu=True)
            cpu_avg = psutil.cpu_percent(interval=1)
            
            # å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory = psutil.virtual_memory()
            
            # ç£ç›˜ä½¿ç”¨æƒ…å†µ
            disk = psutil.disk_usage('/')
            
            # GPUä½¿ç”¨æƒ…å†µ
            gpu_usage = self._get_gpu_usage()
            
            return {
                'timestamp': time.time(),
                'cpu': {
                    'per_core': cpu_percent,
                    'average': cpu_avg,
                    'core_count': psutil.cpu_count()
                },
                'memory': {
                    'total_gb': memory.total / (1024**3),
                    'used_gb': memory.used / (1024**3),
                    'available_gb': memory.available / (1024**3),
                    'percent': memory.percent
                },
                'disk': {
                    'total_gb': disk.total / (1024**3),
                    'used_gb': disk.used / (1024**3),
                    'free_gb': disk.free / (1024**3),
                    'percent': (disk.used / disk.total) * 100
                },
                'gpu': gpu_usage
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–èµ„æºä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
            return {}

    def _get_gpu_usage(self) -> Dict[str, Any]:
        """è·å–GPUä½¿ç”¨æƒ…å†µ"""
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]  # å‡è®¾åªæœ‰ä¸€ä¸ªGPU
                return {
                    'name': gpu.name,
                    'memory_total_mb': gpu.memoryTotal,
                    'memory_used_mb': gpu.memoryUsed,
                    'memory_free_mb': gpu.memoryFree,
                    'memory_percent': (gpu.memoryUsed / gpu.memoryTotal) * 100,
                    'gpu_percent': gpu.load * 100,
                    'temperature': gpu.temperature
                }
            else:
                return {'error': 'No GPU found'}
        except Exception as e:
            return {'error': str(e)}

    def _log_usage(self, usage: Dict[str, Any]):
        """è®°å½•ä½¿ç”¨æƒ…å†µ"""
        if not usage:
            return
            
        cpu_avg = usage.get('cpu', {}).get('average', 0)
        memory_percent = usage.get('memory', {}).get('percent', 0)
        disk_percent = usage.get('disk', {}).get('percent', 0)
        gpu_info = usage.get('gpu', {})
        
        logger.info(f"ğŸ“Š èµ„æºä½¿ç”¨: CPU {cpu_avg:.1f}%, å†…å­˜ {memory_percent:.1f}%, ç£ç›˜ {disk_percent:.1f}%")
        
        if 'memory_percent' in gpu_info:
            logger.info(f"ğŸ® GPUä½¿ç”¨: {gpu_info['gpu_percent']:.1f}%, æ˜¾å­˜ {gpu_info['memory_percent']:.1f}%")


# å…¨å±€èµ„æºç®¡ç†å™¨å®ä¾‹
_resource_manager = None

def get_resource_manager() -> ProductionResourceManager:
    """è·å–å…¨å±€èµ„æºç®¡ç†å™¨å®ä¾‹"""
    global _resource_manager
    if _resource_manager is None:
        _resource_manager = ProductionResourceManager()
    return _resource_manager

def initialize_production_resources():
    """åˆå§‹åŒ–ç”Ÿäº§ç¯å¢ƒèµ„æº"""
    manager = get_resource_manager()
    manager.optimize_performance()
    manager.resource_monitor.start_monitoring()
    return manager

if __name__ == "__main__":
    # æµ‹è¯•èµ„æºç®¡ç†å™¨
    async def test_resource_manager():
        manager = initialize_production_resources()
        
        # æµ‹è¯•èµ„æºåˆ†é…
        cpu_cores = manager.allocate_cpu_cores("ai_model_inference")
        gpu_memory = manager.allocate_gpu_memory("ai_model_inference")
        system_memory = manager.allocate_system_memory("ai_model_inference")
        
        print(f"åˆ†é…çš„CPUæ ¸å¿ƒ: {cpu_cores}")
        print(f"åˆ†é…çš„GPUæ˜¾å­˜: {gpu_memory}MB")
        print(f"åˆ†é…çš„ç³»ç»Ÿå†…å­˜: {system_memory//1024}GB")
        
        # ç›‘æ§èµ„æºä½¿ç”¨
        await asyncio.sleep(10)
        
        usage = manager.get_resource_usage()
        print(f"å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ: {usage}")
        
        manager.resource_monitor.stop_monitoring()

    asyncio.run(test_resource_manager())
