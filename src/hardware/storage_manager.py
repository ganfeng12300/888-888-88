"""
ğŸ’¾ 1TB NVMeæ™ºèƒ½å­˜å‚¨ç®¡ç†å™¨
ç”Ÿäº§çº§å­˜å‚¨èµ„æºç®¡ç†ã€è‡ªåŠ¨æ¸…ç†å’Œæ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†ç³»ç»Ÿ
å®ç°æ™ºèƒ½åˆ†åŒºã€å‹ç¼©å­˜å‚¨å’Œæ€§èƒ½ä¼˜åŒ–
"""

import asyncio
import os
import shutil
import time
import threading
import schedule
import lz4.frame
import json
import sqlite3
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from datetime import datetime, timedelta
import psutil
import subprocess

from loguru import logger


class DataType(Enum):
    """æ•°æ®ç±»å‹"""
    TICK_DATA = "tick_data"
    KLINE_DATA = "kline_data"
    ORDER_BOOK = "order_book"
    AI_MODEL = "ai_model"
    MODEL_CHECKPOINT = "model_checkpoint"
    SYSTEM_LOG = "system_log"
    TRADE_LOG = "trade_log"
    ERROR_LOG = "error_log"
    TEMP_FILE = "temp_file"
    CACHE_DATA = "cache_data"


@dataclass
class StoragePartition:
    """å­˜å‚¨åˆ†åŒºé…ç½®"""
    name: str
    path: str
    size_gb: float
    usage_limit: float  # ä½¿ç”¨ç‡é™åˆ¶ (0-1)
    description: str
    auto_cleanup: bool = True
    compression_enabled: bool = True


@dataclass
class DataLifecycleRule:
    """æ•°æ®ç”Ÿå‘½å‘¨æœŸè§„åˆ™"""
    data_type: DataType
    hot_days: int  # çƒ­æ•°æ®ä¿ç•™å¤©æ•°
    warm_days: int  # æ¸©æ•°æ®ä¿ç•™å¤©æ•°
    cold_days: int  # å†·æ•°æ®ä¿ç•™å¤©æ•°
    archive_days: int  # å½’æ¡£ååˆ é™¤å¤©æ•°
    compression_after_days: int  # å¤šå°‘å¤©åå‹ç¼©
    sampling_ratio: float = 1.0  # é‡‡æ ·æ¯”ä¾‹


@dataclass
class StorageMetrics:
    """å­˜å‚¨æ€§èƒ½æŒ‡æ ‡"""
    timestamp: float
    total_size_gb: float
    used_size_gb: float
    free_size_gb: float
    usage_percent: float
    read_speed_mb_s: float
    write_speed_mb_s: float
    iops_read: int
    iops_write: int
    partition_usage: Dict[str, float]


class IntelligentStorageManager:
    """æ™ºèƒ½å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, storage_path: str = "/app/data"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # å­˜å‚¨åˆ†åŒºé…ç½® (åŸºäº1TB NVMe)
        self.partitions = {
            "system": StoragePartition(
                name="system",
                path=str(self.storage_path / "system"),
                size_gb=200.0,
                usage_limit=0.90,
                description="ç³»ç»Ÿå’Œç¨‹åºæ–‡ä»¶",
                auto_cleanup=False,
                compression_enabled=False
            ),
            "realtime": StoragePartition(
                name="realtime",
                path=str(self.storage_path / "realtime"),
                size_gb=300.0,
                usage_limit=0.85,
                description="å®æ—¶æ•°æ®ç¼“å­˜(7å¤©æ»šåŠ¨)",
                auto_cleanup=True,
                compression_enabled=True
            ),
            "models": StoragePartition(
                name="models",
                path=str(self.storage_path / "models"),
                size_gb=200.0,
                usage_limit=0.80,
                description="AIæ¨¡å‹å’Œæ£€æŸ¥ç‚¹",
                auto_cleanup=True,
                compression_enabled=True
            ),
            "historical": StoragePartition(
                name="historical",
                path=str(self.storage_path / "historical"),
                size_gb=150.0,
                usage_limit=0.85,
                description="å†å²æ•°æ®(å‹ç¼©å­˜å‚¨30å¤©)",
                auto_cleanup=True,
                compression_enabled=True
            ),
            "logs": StoragePartition(
                name="logs",
                path=str(self.storage_path / "logs"),
                size_gb=100.0,
                usage_limit=0.80,
                description="æ—¥å¿—å’Œç›‘æ§æ•°æ®(15å¤©)",
                auto_cleanup=True,
                compression_enabled=True
            ),
            "temp": StoragePartition(
                name="temp",
                path=str(self.storage_path / "temp"),
                size_gb=50.0,
                usage_limit=0.70,
                description="ä¸´æ—¶æ–‡ä»¶å’Œç¼“å†²åŒº",
                auto_cleanup=True,
                compression_enabled=False
            ),
        }
        
        # æ•°æ®ç”Ÿå‘½å‘¨æœŸè§„åˆ™
        self.lifecycle_rules = {
            DataType.TICK_DATA: DataLifecycleRule(
                data_type=DataType.TICK_DATA,
                hot_days=1,
                warm_days=7,
                cold_days=30,
                archive_days=90,
                compression_after_days=3,
                sampling_ratio=0.01  # ä¿ç•™1%çš„tickæ•°æ®
            ),
            DataType.KLINE_DATA: DataLifecycleRule(
                data_type=DataType.KLINE_DATA,
                hot_days=7,
                warm_days=30,
                cold_days=90,
                archive_days=365,
                compression_after_days=7,
                sampling_ratio=1.0  # ä¿ç•™æ‰€æœ‰Kçº¿æ•°æ®
            ),
            DataType.AI_MODEL: DataLifecycleRule(
                data_type=DataType.AI_MODEL,
                hot_days=30,
                warm_days=90,
                cold_days=180,
                archive_days=365,
                compression_after_days=30,
                sampling_ratio=1.0
            ),
            DataType.MODEL_CHECKPOINT: DataLifecycleRule(
                data_type=DataType.MODEL_CHECKPOINT,
                hot_days=1,
                warm_days=7,
                cold_days=30,
                archive_days=90,
                compression_after_days=1,
                sampling_ratio=0.1  # åªä¿ç•™10%çš„æ£€æŸ¥ç‚¹
            ),
            DataType.SYSTEM_LOG: DataLifecycleRule(
                data_type=DataType.SYSTEM_LOG,
                hot_days=7,
                warm_days=15,
                cold_days=30,
                archive_days=90,
                compression_after_days=7,
                sampling_ratio=1.0
            ),
            DataType.TRADE_LOG: DataLifecycleRule(
                data_type=DataType.TRADE_LOG,
                hot_days=30,
                warm_days=90,
                cold_days=180,
                archive_days=365,
                compression_after_days=30,
                sampling_ratio=1.0
            ),
        }
        
        # åˆå§‹åŒ–åˆ†åŒº
        self._initialize_partitions()
        
        # æ€§èƒ½ç›‘æ§
        self.metrics_history: List[StorageMetrics] = []
        self.max_history_size = 3600  # 1å°æ—¶å†å²æ•°æ®
        self.monitoring = False
        
        # æ¸…ç†è°ƒåº¦å™¨
        self.cleanup_scheduler_running = False
        self.cleanup_lock = threading.Lock()
        
        # æ•°æ®åº“è¿æ¥ (ç”¨äºå…ƒæ•°æ®ç®¡ç†)
        self.db_path = self.storage_path / "metadata.db"
        self._initialize_database()
        
        logger.info("æ™ºèƒ½å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_partitions(self):
        """åˆå§‹åŒ–å­˜å‚¨åˆ†åŒº"""
        for partition in self.partitions.values():
            Path(partition.path).mkdir(parents=True, exist_ok=True)
            logger.info(f"åˆå§‹åŒ–åˆ†åŒº: {partition.name} -> {partition.path}")
    
    def _initialize_database(self):
        """åˆå§‹åŒ–å…ƒæ•°æ®æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()
            
            # åˆ›å»ºæ–‡ä»¶å…ƒæ•°æ®è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS file_metadata (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_path TEXT UNIQUE NOT NULL,
                    data_type TEXT NOT NULL,
                    size_bytes INTEGER NOT NULL,
                    created_time REAL NOT NULL,
                    last_access_time REAL NOT NULL,
                    compressed BOOLEAN DEFAULT FALSE,
                    partition_name TEXT NOT NULL,
                    importance_score REAL DEFAULT 1.0
                )
            ''')
            
            # åˆ›å»ºæ¸…ç†å†å²è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS cleanup_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cleanup_time REAL NOT NULL,
                    files_deleted INTEGER NOT NULL,
                    space_freed_mb REAL NOT NULL,
                    cleanup_type TEXT NOT NULL
                )
            ''')
            
            conn.commit()
            conn.close()
            
            logger.info("å…ƒæ•°æ®æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {e}")
    
    def store_file(self, file_path: str, data_type: DataType, 
                   partition_name: str = None, importance_score: float = 1.0) -> bool:
        """å­˜å‚¨æ–‡ä»¶å¹¶è®°å½•å…ƒæ•°æ®"""
        try:
            source_path = Path(file_path)
            if not source_path.exists():
                logger.error(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            
            # ç¡®å®šç›®æ ‡åˆ†åŒº
            if partition_name is None:
                partition_name = self._determine_partition(data_type)
            
            if partition_name not in self.partitions:
                logger.error(f"æœªçŸ¥åˆ†åŒº: {partition_name}")
                return False
            
            partition = self.partitions[partition_name]
            
            # æ£€æŸ¥åˆ†åŒºç©ºé—´
            if not self._check_partition_space(partition_name, source_path.stat().st_size):
                logger.warning(f"åˆ†åŒº {partition_name} ç©ºé—´ä¸è¶³")
                # å°è¯•æ¸…ç†ç©ºé—´
                self._cleanup_partition(partition_name)
                
                # å†æ¬¡æ£€æŸ¥
                if not self._check_partition_space(partition_name, source_path.stat().st_size):
                    logger.error(f"åˆ†åŒº {partition_name} ç©ºé—´ä¸è¶³ï¼Œæ— æ³•å­˜å‚¨æ–‡ä»¶")
                    return False
            
            # ç”Ÿæˆç›®æ ‡è·¯å¾„
            target_dir = Path(partition.path) / data_type.value
            target_dir.mkdir(parents=True, exist_ok=True)
            target_path = target_dir / source_path.name
            
            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(source_path, target_path)
            
            # è®°å½•å…ƒæ•°æ®
            self._record_file_metadata(
                str(target_path), data_type, target_path.stat().st_size,
                partition_name, importance_score
            )
            
            logger.info(f"æ–‡ä»¶å·²å­˜å‚¨: {target_path}")
            return True
            
        except Exception as e:
            logger.error(f"å­˜å‚¨æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _determine_partition(self, data_type: DataType) -> str:
        """æ ¹æ®æ•°æ®ç±»å‹ç¡®å®šåˆ†åŒº"""
        partition_mapping = {
            DataType.TICK_DATA: "realtime",
            DataType.KLINE_DATA: "realtime",
            DataType.ORDER_BOOK: "realtime",
            DataType.AI_MODEL: "models",
            DataType.MODEL_CHECKPOINT: "models",
            DataType.SYSTEM_LOG: "logs",
            DataType.TRADE_LOG: "logs",
            DataType.ERROR_LOG: "logs",
            DataType.TEMP_FILE: "temp",
            DataType.CACHE_DATA: "temp",
        }
        return partition_mapping.get(data_type, "temp")
    
    def _check_partition_space(self, partition_name: str, required_bytes: int) -> bool:
        """æ£€æŸ¥åˆ†åŒºæ˜¯å¦æœ‰è¶³å¤Ÿç©ºé—´"""
        partition = self.partitions[partition_name]
        partition_path = Path(partition.path)
        
        if not partition_path.exists():
            return True
        
        # è·å–åˆ†åŒºä½¿ç”¨æƒ…å†µ
        usage = shutil.disk_usage(partition_path)
        current_usage = (usage.total - usage.free) / (partition.size_gb * 1024**3)
        
        # è®¡ç®—æ·»åŠ æ–‡ä»¶åçš„ä½¿ç”¨ç‡
        new_usage = (usage.total - usage.free + required_bytes) / (partition.size_gb * 1024**3)
        
        return new_usage <= partition.usage_limit
    
    def _record_file_metadata(self, file_path: str, data_type: DataType, 
                             size_bytes: int, partition_name: str, importance_score: float):
        """è®°å½•æ–‡ä»¶å…ƒæ•°æ®"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()
            
            current_time = time.time()
            
            cursor.execute('''
                INSERT OR REPLACE INTO file_metadata 
                (file_path, data_type, size_bytes, created_time, last_access_time, 
                 partition_name, importance_score)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (file_path, data_type.value, size_bytes, current_time, 
                  current_time, partition_name, importance_score))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"è®°å½•æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥: {e}")
    
    async def start_performance_monitoring(self, interval: float = 10.0):
        """å¯åŠ¨å­˜å‚¨æ€§èƒ½ç›‘æ§"""
        self.monitoring = True
        logger.info("å¼€å§‹å­˜å‚¨æ€§èƒ½ç›‘æ§...")
        
        while self.monitoring:
            try:
                metrics = await self._collect_storage_metrics()
                self._store_metrics(metrics)
                
                # æ£€æŸ¥å­˜å‚¨é—®é¢˜
                await self._check_storage_issues(metrics)
                
                await asyncio.sleep(interval)
                
            except Exception as e:
                logger.error(f"å­˜å‚¨æ€§èƒ½ç›‘æ§å‡ºé”™: {e}")
                await asyncio.sleep(interval)
    
    async def _collect_storage_metrics(self) -> StorageMetrics:
        """æ”¶é›†å­˜å‚¨æ€§èƒ½æŒ‡æ ‡"""
        timestamp = time.time()
        
        # è·å–æ€»ä½“å­˜å‚¨ä½¿ç”¨æƒ…å†µ
        usage = shutil.disk_usage(self.storage_path)
        total_size_gb = usage.total / (1024**3)
        used_size_gb = (usage.total - usage.free) / (1024**3)
        free_size_gb = usage.free / (1024**3)
        usage_percent = (used_size_gb / total_size_gb) * 100
        
        # è·å–I/Oæ€§èƒ½æŒ‡æ ‡
        disk_io = psutil.disk_io_counters()
        if hasattr(self, '_last_disk_io'):
            time_delta = timestamp - self._last_io_timestamp
            read_speed_mb_s = (disk_io.read_bytes - self._last_disk_io.read_bytes) / (1024**2) / time_delta
            write_speed_mb_s = (disk_io.write_bytes - self._last_disk_io.write_bytes) / (1024**2) / time_delta
            iops_read = int((disk_io.read_count - self._last_disk_io.read_count) / time_delta)
            iops_write = int((disk_io.write_count - self._last_disk_io.write_count) / time_delta)
        else:
            read_speed_mb_s = 0
            write_speed_mb_s = 0
            iops_read = 0
            iops_write = 0
        
        self._last_disk_io = disk_io
        self._last_io_timestamp = timestamp
        
        # è·å–å„åˆ†åŒºä½¿ç”¨æƒ…å†µ
        partition_usage = {}
        for name, partition in self.partitions.items():
            try:
                partition_path = Path(partition.path)
                if partition_path.exists():
                    partition_usage_info = shutil.disk_usage(partition_path)
                    partition_used = (partition_usage_info.total - partition_usage_info.free) / (1024**3)
                    partition_usage[name] = (partition_used / partition.size_gb) * 100
                else:
                    partition_usage[name] = 0.0
            except:
                partition_usage[name] = 0.0
        
        return StorageMetrics(
            timestamp=timestamp,
            total_size_gb=total_size_gb,
            used_size_gb=used_size_gb,
            free_size_gb=free_size_gb,
            usage_percent=usage_percent,
            read_speed_mb_s=read_speed_mb_s,
            write_speed_mb_s=write_speed_mb_s,
            iops_read=iops_read,
            iops_write=iops_write,
            partition_usage=partition_usage
        )
    
    def _store_metrics(self, metrics: StorageMetrics):
        """å­˜å‚¨æ€§èƒ½æŒ‡æ ‡"""
        self.metrics_history.append(metrics)
        
        # é™åˆ¶å†å²æ•°æ®å¤§å°
        if len(self.metrics_history) > self.max_history_size:
            self.metrics_history = self.metrics_history[-self.max_history_size:]
    
    async def _check_storage_issues(self, metrics: StorageMetrics):
        """æ£€æŸ¥å­˜å‚¨é—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥æ€»ä½“ä½¿ç”¨ç‡
        if metrics.usage_percent > 85:
            issues.append(f"å­˜å‚¨ä½¿ç”¨ç‡è¿‡é«˜: {metrics.usage_percent:.1f}%")
        
        # æ£€æŸ¥å„åˆ†åŒºä½¿ç”¨ç‡
        for partition_name, usage in metrics.partition_usage.items():
            partition = self.partitions[partition_name]
            if usage > partition.usage_limit * 100:
                issues.append(f"åˆ†åŒº {partition_name} ä½¿ç”¨ç‡è¿‡é«˜: {usage:.1f}%")
        
        # æ£€æŸ¥I/Oæ€§èƒ½
        if metrics.read_speed_mb_s < 100 and metrics.read_speed_mb_s > 0:
            issues.append(f"è¯»å–é€Ÿåº¦è¾ƒæ…¢: {metrics.read_speed_mb_s:.1f}MB/s")
        
        if metrics.write_speed_mb_s < 100 and metrics.write_speed_mb_s > 0:
            issues.append(f"å†™å…¥é€Ÿåº¦è¾ƒæ…¢: {metrics.write_speed_mb_s:.1f}MB/s")
        
        if issues:
            logger.warning(f"å­˜å‚¨é—®é¢˜: {'; '.join(issues)}")
            # è§¦å‘æ¸…ç†
            await self._trigger_emergency_cleanup()
    
    async def _trigger_emergency_cleanup(self):
        """è§¦å‘ç´§æ€¥æ¸…ç†"""
        logger.info("è§¦å‘ç´§æ€¥å­˜å‚¨æ¸…ç†...")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        self._cleanup_partition("temp")
        
        # æ¸…ç†è¿‡æœŸæ•°æ®
        await self._cleanup_expired_data()
        
        # å‹ç¼©æ•°æ®
        await self._compress_old_data()
    
    def start_cleanup_scheduler(self):
        """å¯åŠ¨æ¸…ç†è°ƒåº¦å™¨"""
        if self.cleanup_scheduler_running:
            return
        
        self.cleanup_scheduler_running = True
        
        # æ¯å°æ—¶æ¸…ç†
        schedule.every().hour.do(self._hourly_cleanup)
        
        # æ¯å¤©æ¸…ç†
        schedule.every().day.at("02:00").do(self._daily_cleanup)
        
        # æ¯å‘¨æ¸…ç†
        schedule.every().sunday.at("03:00").do(self._weekly_cleanup)
        
        # å¯åŠ¨è°ƒåº¦å™¨çº¿ç¨‹
        def run_scheduler():
            while self.cleanup_scheduler_running:
                schedule.run_pending()
                time.sleep(60)
        
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        
        logger.info("æ¸…ç†è°ƒåº¦å™¨å·²å¯åŠ¨")
    
    def _hourly_cleanup(self):
        """æ¯å°æ—¶æ¸…ç†ä»»åŠ¡"""
        with self.cleanup_lock:
            logger.info("æ‰§è¡Œæ¯å°æ—¶æ¸…ç†ä»»åŠ¡...")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_partition("temp")
            
            # æ£€æŸ¥å¹¶æ¸…ç†è¿‡æœŸç¼“å­˜
            self._cleanup_expired_cache()
    
    def _daily_cleanup(self):
        """æ¯æ—¥æ¸…ç†ä»»åŠ¡"""
        with self.cleanup_lock:
            logger.info("æ‰§è¡Œæ¯æ—¥æ¸…ç†ä»»åŠ¡...")
            
            # å‹ç¼©æ˜¨å¤©çš„æ•°æ®
            asyncio.run(self._compress_yesterday_data())
            
            # æ¸…ç†è¿‡æœŸæ¨¡å‹æ£€æŸ¥ç‚¹
            self._cleanup_old_model_checkpoints()
            
            # æ¸…ç†è¿‡æœŸæ—¥å¿—
            self._cleanup_old_logs()
    
    def _weekly_cleanup(self):
        """æ¯å‘¨æ¸…ç†ä»»åŠ¡"""
        with self.cleanup_lock:
            logger.info("æ‰§è¡Œæ¯å‘¨æ¸…ç†ä»»åŠ¡...")
            
            # æ·±åº¦æ•°æ®å‹ç¼©
            asyncio.run(self._deep_data_compression())
            
            # æ¸…ç†ä½ä»·å€¼å†å²æ•°æ®
            self._cleanup_low_value_data()
            
            # æ•°æ®åº“ç»´æŠ¤
            self._maintain_database()
    
    def _cleanup_partition(self, partition_name: str) -> int:
        """æ¸…ç†æŒ‡å®šåˆ†åŒº"""
        try:
            partition = self.partitions[partition_name]
            partition_path = Path(partition.path)
            
            if not partition_path.exists():
                return 0
            
            files_deleted = 0
            space_freed = 0
            
            # è·å–åˆ†åŒºä¸­çš„æ‰€æœ‰æ–‡ä»¶
            for file_path in partition_path.rglob("*"):
                if file_path.is_file():
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«æ¸…ç†
                    if self._should_cleanup_file(file_path):
                        file_size = file_path.stat().st_size
                        file_path.unlink()
                        files_deleted += 1
                        space_freed += file_size
            
            if files_deleted > 0:
                logger.info(f"åˆ†åŒº {partition_name} æ¸…ç†å®Œæˆ: åˆ é™¤ {files_deleted} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {space_freed / (1024**2):.1f}MB")
            
            return files_deleted
            
        except Exception as e:
            logger.error(f"æ¸…ç†åˆ†åŒº {partition_name} å¤±è´¥: {e}")
            return 0
    
    def _should_cleanup_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«æ¸…ç†"""
        try:
            # è·å–æ–‡ä»¶ä¿¡æ¯
            stat = file_path.stat()
            file_age_days = (time.time() - stat.st_mtime) / 86400
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹å’Œå¹´é¾„åˆ¤æ–­
            if file_path.suffix in ['.tmp', '.temp', '.cache']:
                return file_age_days > 1  # ä¸´æ—¶æ–‡ä»¶1å¤©ååˆ é™¤
            
            if 'checkpoint' in file_path.name.lower():
                return file_age_days > 7  # æ£€æŸ¥ç‚¹7å¤©ååˆ é™¤
            
            if file_path.suffix in ['.log']:
                return file_age_days > 15  # æ—¥å¿—æ–‡ä»¶15å¤©ååˆ é™¤
            
            return False
            
        except Exception:
            return False
    
    async def _compress_old_data(self):
        """å‹ç¼©æ—§æ•°æ®"""
        try:
            logger.info("å¼€å§‹å‹ç¼©æ—§æ•°æ®...")
            
            compressed_count = 0
            space_saved = 0
            
            # éå†æ‰€æœ‰åˆ†åŒº
            for partition_name, partition in self.partitions.items():
                if not partition.compression_enabled:
                    continue
                
                partition_path = Path(partition.path)
                if not partition_path.exists():
                    continue
                
                # æŸ¥æ‰¾éœ€è¦å‹ç¼©çš„æ–‡ä»¶
                for file_path in partition_path.rglob("*"):
                    if file_path.is_file() and not file_path.name.endswith('.lz4'):
                        # æ£€æŸ¥æ–‡ä»¶å¹´é¾„
                        file_age_days = (time.time() - file_path.stat().st_mtime) / 86400
                        
                        if file_age_days > 3:  # 3å¤©åå‹ç¼©
                            original_size = file_path.stat().st_size
                            if await self._compress_file(file_path):
                                compressed_size = file_path.with_suffix(file_path.suffix + '.lz4').stat().st_size
                                space_saved += original_size - compressed_size
                                compressed_count += 1
            
            if compressed_count > 0:
                logger.info(f"æ•°æ®å‹ç¼©å®Œæˆ: å‹ç¼© {compressed_count} ä¸ªæ–‡ä»¶ï¼ŒèŠ‚çœ {space_saved / (1024**2):.1f}MB")
            
        except Exception as e:
            logger.error(f"å‹ç¼©æ—§æ•°æ®å¤±è´¥: {e}")
    
    async def _compress_file(self, file_path: Path) -> bool:
        """å‹ç¼©å•ä¸ªæ–‡ä»¶"""
        try:
            compressed_path = file_path.with_suffix(file_path.suffix + '.lz4')
            
            with open(file_path, 'rb') as f_in:
                data = f_in.read()
            
            compressed_data = lz4.frame.compress(data)
            
            with open(compressed_path, 'wb') as f_out:
                f_out.write(compressed_data)
            
            # åˆ é™¤åŸæ–‡ä»¶
            file_path.unlink()
            
            return True
            
        except Exception as e:
            logger.error(f"å‹ç¼©æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            return False
    
    def get_storage_summary(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨æ‘˜è¦"""
        if not self.metrics_history:
            return {}
        
        latest_metrics = self.metrics_history[-1]
        
        return {
            "total_size_gb": latest_metrics.total_size_gb,
            "used_size_gb": latest_metrics.used_size_gb,
            "free_size_gb": latest_metrics.free_size_gb,
            "usage_percent": latest_metrics.usage_percent,
            "partition_usage": latest_metrics.partition_usage,
            "partitions": {
                name: {
                    "size_gb": partition.size_gb,
                    "usage_limit": partition.usage_limit * 100,
                    "description": partition.description,
                    "auto_cleanup": partition.auto_cleanup,
                    "compression_enabled": partition.compression_enabled
                }
                for name, partition in self.partitions.items()
            }
        }
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        self.cleanup_scheduler_running = False
        logger.info("å­˜å‚¨ç›‘æ§å·²åœæ­¢")


# å…¨å±€å­˜å‚¨ç®¡ç†å™¨å®ä¾‹
storage_manager = IntelligentStorageManager()


async def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨å­˜å‚¨ç®¡ç†å™¨æµ‹è¯•...")
    
    # å¯åŠ¨æ¸…ç†è°ƒåº¦å™¨
    storage_manager.start_cleanup_scheduler()
    
    # å¯åŠ¨æ€§èƒ½ç›‘æ§
    monitor_task = asyncio.create_task(storage_manager.start_performance_monitoring())
    
    try:
        # è¿è¡Œ30ç§’æµ‹è¯•
        await asyncio.sleep(30)
        
        # è·å–å­˜å‚¨æ‘˜è¦
        summary = storage_manager.get_storage_summary()
        logger.info(f"å­˜å‚¨æ‘˜è¦: {json.dumps(summary, indent=2)}")
        
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·...")
    finally:
        storage_manager.stop_monitoring()
        monitor_task.cancel()


if __name__ == "__main__":
    asyncio.run(main())
