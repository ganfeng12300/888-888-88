"""
ğŸ® GPUæ˜¾å­˜ä¼˜åŒ–ç®¡ç†å™¨
ç”Ÿäº§çº§RTX3060 12GBæ˜¾å­˜ç²¾ç¡®åˆ†é…å’Œæ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿ
å®ç°GPUæ˜¾å­˜æ± ç®¡ç†ã€åŠ¨æ€åˆ†é…å’ŒAIæ¨¡å‹ä¼˜åŒ–
"""

import asyncio
import time
import threading
import subprocess
import json
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import os

try:
    import torch
    import pynvml
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False

from loguru import logger


class GPUTaskType(Enum):
    """GPUä»»åŠ¡ç±»å‹"""
    REINFORCEMENT_LEARNING = "reinforcement_learning"
    TIME_SERIES_DEEP = "time_series_deep"
    INFERENCE_CACHE = "inference_cache"
    MODEL_TRAINING = "model_training"
    DATA_PREPROCESSING = "data_preprocessing"


@dataclass
class GPUMemoryAllocation:
    """GPUæ˜¾å­˜åˆ†é…é…ç½®"""
    task_type: GPUTaskType
    memory_gb: float
    priority: int  # ä¼˜å…ˆçº§ (1-10, 10æœ€é«˜)
    description: str
    allocated_memory: int = 0  # å·²åˆ†é…çš„æ˜¾å­˜ (bytes)
    active_models: List[str] = field(default_factory=list)


@dataclass
class GPUMetrics:
    """GPUæ€§èƒ½æŒ‡æ ‡"""
    timestamp: float
    gpu_id: int
    usage_percent: float
    memory_used_gb: float
    memory_total_gb: float
    memory_usage_percent: float
    temperature: float
    power_usage: float
    fan_speed: int
    clock_graphics: int
    clock_memory: int


class GPUMemoryManager:
    """GPUæ˜¾å­˜ç®¡ç†å™¨"""
    
    def __init__(self, gpu_id: int = 0):
        self.gpu_id = gpu_id
        self.gpu_available = GPU_AVAILABLE
        self.monitoring = False
        
        if not self.gpu_available:
            logger.error("GPUä¸å¯ç”¨ï¼Œè¯·å®‰è£…PyTorchå’Œpynvml")
            return
        
        # åˆå§‹åŒ–GPU
        try:
            pynvml.nvmlInit()
            self.handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_id)
            self.gpu_name = pynvml.nvmlDeviceGetName(self.handle).decode('utf-8')
            
            # è·å–GPUå†…å­˜ä¿¡æ¯
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(self.handle)
            self.total_memory = memory_info.total
            self.total_memory_gb = self.total_memory / (1024**3)
            
            logger.info(f"GPUç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ: {self.gpu_name}, æ€»æ˜¾å­˜: {self.total_memory_gb:.1f}GB")
            
        except Exception as e:
            logger.error(f"GPUåˆå§‹åŒ–å¤±è´¥: {e}")
            self.gpu_available = False
            return
        
        # æ˜¾å­˜åˆ†é…ç­–ç•¥ (åŸºäºRTX3060 12GB)
        self.memory_allocations = {
            GPUTaskType.REINFORCEMENT_LEARNING: GPUMemoryAllocation(
                task_type=GPUTaskType.REINFORCEMENT_LEARNING,
                memory_gb=6.0,  # 6GBç”¨äºå¼ºåŒ–å­¦ä¹ 
                priority=9,
                description="å¼ºåŒ–å­¦ä¹ AIæ¨¡å‹(PPO/SAC)"
            ),
            GPUTaskType.TIME_SERIES_DEEP: GPUMemoryAllocation(
                task_type=GPUTaskType.TIME_SERIES_DEEP,
                memory_gb=4.0,  # 4GBç”¨äºæ—¶åºæ·±åº¦å­¦ä¹ 
                priority=8,
                description="æ—¶åºæ·±åº¦å­¦ä¹ AI(Transformer/LSTM)"
            ),
            GPUTaskType.INFERENCE_CACHE: GPUMemoryAllocation(
                task_type=GPUTaskType.INFERENCE_CACHE,
                memory_gb=1.5,  # 1.5GBç”¨äºæ¨ç†ç¼“å­˜
                priority=7,
                description="æ¨¡å‹æ¨ç†å’Œé¢„æµ‹ç¼“å­˜"
            ),
            GPUTaskType.MODEL_TRAINING: GPUMemoryAllocation(
                task_type=GPUTaskType.MODEL_TRAINING,
                memory_gb=0.5,  # 0.5GBç”¨äºå…¶ä»–æ¨¡å‹è®­ç»ƒ
                priority=5,
                description="å…¶ä»–AIæ¨¡å‹è®­ç»ƒ"
            ),
        }
        
        # æ€§èƒ½ç›‘æ§æ•°æ®
        self.metrics_history: List[GPUMetrics] = []
        self.max_history_size = 3600  # 1å°æ—¶å†å²æ•°æ®
        
        # å†…å­˜æ± ç®¡ç†
        self.memory_pool: Dict[str, torch.Tensor] = {}
        self.memory_lock = threading.Lock()
        
        # PyTorch GPUè®¾ç½®
        if torch.cuda.is_available():
            torch.cuda.set_device(gpu_id)
            # å¯ç”¨æ˜¾å­˜å¢é•¿æ¨¡å¼
            torch.cuda.set_per_process_memory_fraction(0.95)
            # å¯ç”¨å†…å­˜æ± 
            torch.cuda.empty_cache()
            logger.info("PyTorch GPUè®¾ç½®å®Œæˆ")
    
    def allocate_memory(self, task_type: GPUTaskType, model_name: str, 
                       memory_size_mb: Optional[int] = None) -> Optional[torch.Tensor]:
        """åˆ†é…GPUæ˜¾å­˜"""
        if not self.gpu_available:
            return None
        
        try:
            allocation = self.memory_allocations.get(task_type)
            if not allocation:
                logger.error(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")
                return None
            
            # è®¡ç®—éœ€è¦åˆ†é…çš„æ˜¾å­˜å¤§å°
            if memory_size_mb is None:
                memory_size_mb = int(allocation.memory_gb * 1024)
            
            memory_size_bytes = memory_size_mb * 1024 * 1024
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ˜¾å­˜
            if allocation.allocated_memory + memory_size_bytes > allocation.memory_gb * (1024**3):
                logger.warning(f"ä»»åŠ¡ç±»å‹ {task_type.value} æ˜¾å­˜ä¸è¶³")
                return None
            
            with self.memory_lock:
                # åˆ†é…æ˜¾å­˜
                memory_tensor = torch.empty(memory_size_bytes // 4, dtype=torch.float32, device=f'cuda:{self.gpu_id}')
                
                # è®°å½•åˆ†é…
                allocation.allocated_memory += memory_size_bytes
                allocation.active_models.append(model_name)
                self.memory_pool[model_name] = memory_tensor
            
            logger.info(f"ä¸º {model_name} åˆ†é…äº† {memory_size_mb}MB æ˜¾å­˜ ({task_type.value})")
            return memory_tensor
            
        except Exception as e:
            logger.error(f"åˆ†é…æ˜¾å­˜å¤±è´¥: {e}")
            return None
    
    def deallocate_memory(self, model_name: str) -> bool:
        """é‡Šæ”¾GPUæ˜¾å­˜"""
        if not self.gpu_available:
            return False
        
        try:
            with self.memory_lock:
                if model_name not in self.memory_pool:
                    return False
                
                # è·å–æ˜¾å­˜å¤§å°
                memory_tensor = self.memory_pool[model_name]
                memory_size_bytes = memory_tensor.numel() * 4  # float32 = 4 bytes
                
                # æ‰¾åˆ°å¯¹åº”çš„åˆ†é…è®°å½•
                for allocation in self.memory_allocations.values():
                    if model_name in allocation.active_models:
                        allocation.allocated_memory -= memory_size_bytes
                        allocation.active_models.remove(model_name)
                        break
                
                # é‡Šæ”¾æ˜¾å­˜
                del self.memory_pool[model_name]
                del memory_tensor
                torch.cuda.empty_cache()
            
            logger.info(f"å·²é‡Šæ”¾ {model_name} çš„æ˜¾å­˜")
            return True
            
        except Exception as e:
            logger.error(f"é‡Šæ”¾æ˜¾å­˜å¤±è´¥: {e}")
            return False
    
    async def start_performance_monitoring(self, interval: float = 1.0):
        """å¯åŠ¨GPUæ€§èƒ½ç›‘æ§"""
        if not self.gpu_available:
            logger.warning("GPUä¸å¯ç”¨ï¼Œæ— æ³•å¯åŠ¨æ€§èƒ½ç›‘æ§")
            return
        
        self.monitoring = True
        logger.info("å¼€å§‹GPUæ€§èƒ½ç›‘æ§...")
        
        while self.monitoring:
            try:
                metrics = await self._collect_gpu_metrics()
                self._store_metrics(metrics)
                
                # æ£€æŸ¥æ€§èƒ½é—®é¢˜
                await self._check_performance_issues(metrics)
                
                # åŠ¨æ€ä¼˜åŒ–
                await self._optimize_gpu_performance(metrics)
                
                await asyncio.sleep(interval)
                
            except Exception as e:
                logger.error(f"GPUæ€§èƒ½ç›‘æ§å‡ºé”™: {e}")
                await asyncio.sleep(interval)
    
    async def _collect_gpu_metrics(self) -> GPUMetrics:
        """æ”¶é›†GPUæ€§èƒ½æŒ‡æ ‡"""
        timestamp = time.time()
        
        try:
            # GPUä½¿ç”¨ç‡
            utilization = pynvml.nvmlDeviceGetUtilizationRates(self.handle)
            usage_percent = utilization.gpu
            
            # GPUå†…å­˜
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(self.handle)
            memory_used_gb = memory_info.used / (1024**3)
            memory_total_gb = memory_info.total / (1024**3)
            memory_usage_percent = (memory_info.used / memory_info.total) * 100
            
            # GPUæ¸©åº¦
            try:
                temperature = pynvml.nvmlDeviceGetTemperature(self.handle, pynvml.NVML_TEMPERATURE_GPU)
            except:
                temperature = 0
            
            # GPUåŠŸè€—
            try:
                power_usage = pynvml.nvmlDeviceGetPowerUsage(self.handle) / 1000.0  # mW to W
            except:
                power_usage = 0
            
            # é£æ‰‡è½¬é€Ÿ
            try:
                fan_speed = pynvml.nvmlDeviceGetFanSpeed(self.handle)
            except:
                fan_speed = 0
            
            # GPUæ—¶é’Ÿé¢‘ç‡
            try:
                clock_graphics = pynvml.nvmlDeviceGetClockInfo(self.handle, pynvml.NVML_CLOCK_GRAPHICS)
                clock_memory = pynvml.nvmlDeviceGetClockInfo(self.handle, pynvml.NVML_CLOCK_MEM)
            except:
                clock_graphics = 0
                clock_memory = 0
            
            return GPUMetrics(
                timestamp=timestamp,
                gpu_id=self.gpu_id,
                usage_percent=usage_percent,
                memory_used_gb=memory_used_gb,
                memory_total_gb=memory_total_gb,
                memory_usage_percent=memory_usage_percent,
                temperature=temperature,
                power_usage=power_usage,
                fan_speed=fan_speed,
                clock_graphics=clock_graphics,
                clock_memory=clock_memory
            )
            
        except Exception as e:
            logger.error(f"æ”¶é›†GPUæŒ‡æ ‡å¤±è´¥: {e}")
            return GPUMetrics(
                timestamp=timestamp,
                gpu_id=self.gpu_id,
                usage_percent=0,
                memory_used_gb=0,
                memory_total_gb=self.total_memory_gb,
                memory_usage_percent=0,
                temperature=0,
                power_usage=0,
                fan_speed=0,
                clock_graphics=0,
                clock_memory=0
            )
    
    def _store_metrics(self, metrics: GPUMetrics):
        """å­˜å‚¨æ€§èƒ½æŒ‡æ ‡"""
        self.metrics_history.append(metrics)
        
        # é™åˆ¶å†å²æ•°æ®å¤§å°
        if len(self.metrics_history) > self.max_history_size:
            self.metrics_history = self.metrics_history[-self.max_history_size:]
    
    async def _check_performance_issues(self, metrics: GPUMetrics):
        """æ£€æŸ¥æ€§èƒ½é—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥GPUä½¿ç”¨ç‡
        if metrics.usage_percent > 98:
            issues.append(f"GPUä½¿ç”¨ç‡è¿‡é«˜: {metrics.usage_percent}%")
        elif metrics.usage_percent < 50:
            issues.append(f"GPUä½¿ç”¨ç‡åä½: {metrics.usage_percent}%")
        
        # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨ç‡
        if metrics.memory_usage_percent > 95:
            issues.append(f"GPUæ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜: {metrics.memory_usage_percent:.1f}%")
        
        # æ£€æŸ¥æ¸©åº¦
        if metrics.temperature > 80:
            issues.append(f"GPUæ¸©åº¦è¿‡é«˜: {metrics.temperature}Â°C")
        elif metrics.temperature > 75:
            issues.append(f"GPUæ¸©åº¦è­¦å‘Š: {metrics.temperature}Â°C")
        
        # æ£€æŸ¥åŠŸè€—
        if metrics.power_usage > 170:  # RTX3060é¢å®šåŠŸè€—170W
            issues.append(f"GPUåŠŸè€—è¿‡é«˜: {metrics.power_usage:.1f}W")
        
        if issues:
            logger.warning(f"GPUæ€§èƒ½é—®é¢˜: {'; '.join(issues)}")
    
    async def _optimize_gpu_performance(self, metrics: GPUMetrics):
        """åŠ¨æ€ä¼˜åŒ–GPUæ€§èƒ½"""
        try:
            # æ¸©åº¦æ§åˆ¶
            if metrics.temperature > 75:
                await self._reduce_gpu_power_limit()
            elif metrics.temperature < 65 and metrics.usage_percent > 90:
                await self._increase_gpu_power_limit()
            
            # æ˜¾å­˜ä¼˜åŒ–
            if metrics.memory_usage_percent > 90:
                await self._optimize_memory_usage()
            
            # æ—¶é’Ÿé¢‘ç‡ä¼˜åŒ–
            if metrics.usage_percent > 95 and metrics.temperature < 70:
                await self._increase_gpu_clocks()
            elif metrics.temperature > 80:
                await self._reduce_gpu_clocks()
                
        except Exception as e:
            logger.error(f"GPUæ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
    
    async def _reduce_gpu_power_limit(self):
        """é™ä½GPUåŠŸè€—é™åˆ¶"""
        try:
            subprocess.run(['nvidia-smi', '-pl', '150'], capture_output=True, check=False)
            logger.info("å·²é™ä½GPUåŠŸè€—é™åˆ¶åˆ°150W")
        except Exception as e:
            logger.debug(f"é™ä½GPUåŠŸè€—å¤±è´¥: {e}")
    
    async def _increase_gpu_power_limit(self):
        """æé«˜GPUåŠŸè€—é™åˆ¶"""
        try:
            subprocess.run(['nvidia-smi', '-pl', '170'], capture_output=True, check=False)
            logger.info("å·²æé«˜GPUåŠŸè€—é™åˆ¶åˆ°170W")
        except Exception as e:
            logger.debug(f"æé«˜GPUåŠŸè€—å¤±è´¥: {e}")
    
    async def _optimize_memory_usage(self):
        """ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨"""
        try:
            # æ¸…ç†PyTorchç¼“å­˜
            torch.cuda.empty_cache()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            logger.info("å·²ä¼˜åŒ–GPUæ˜¾å­˜ä½¿ç”¨")
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨å¤±è´¥: {e}")
    
    async def _increase_gpu_clocks(self):
        """æé«˜GPUæ—¶é’Ÿé¢‘ç‡"""
        try:
            # ä½¿ç”¨nvidia-smiæé«˜æ—¶é’Ÿé¢‘ç‡
            subprocess.run(['nvidia-smi', '-ac', '6001,1890'], capture_output=True, check=False)
            logger.info("å·²æé«˜GPUæ—¶é’Ÿé¢‘ç‡")
        except Exception as e:
            logger.debug(f"æé«˜GPUæ—¶é’Ÿé¢‘ç‡å¤±è´¥: {e}")
    
    async def _reduce_gpu_clocks(self):
        """é™ä½GPUæ—¶é’Ÿé¢‘ç‡"""
        try:
            subprocess.run(['nvidia-smi', '-ac', '5001,1590'], capture_output=True, check=False)
            logger.info("å·²é™ä½GPUæ—¶é’Ÿé¢‘ç‡")
        except Exception as e:
            logger.debug(f"é™ä½GPUæ—¶é’Ÿé¢‘ç‡å¤±è´¥: {e}")
    
    def get_memory_allocation_status(self) -> Dict[str, Any]:
        """è·å–æ˜¾å­˜åˆ†é…çŠ¶æ€"""
        status = {}
        
        for task_type, allocation in self.memory_allocations.items():
            allocated_gb = allocation.allocated_memory / (1024**3)
            utilization = (allocated_gb / allocation.memory_gb) * 100 if allocation.memory_gb > 0 else 0
            
            status[task_type.value] = {
                "allocated_gb": round(allocated_gb, 2),
                "total_gb": allocation.memory_gb,
                "utilization_percent": round(utilization, 1),
                "active_models": len(allocation.active_models),
                "model_names": allocation.active_models.copy(),
                "description": allocation.description
            }
        
        return status
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-60:]  # æœ€è¿‘1åˆ†é’Ÿ
        
        avg_usage = sum(m.usage_percent for m in recent_metrics) / len(recent_metrics)
        max_usage = max(m.usage_percent for m in recent_metrics)
        avg_memory = sum(m.memory_usage_percent for m in recent_metrics) / len(recent_metrics)
        max_memory = max(m.memory_usage_percent for m in recent_metrics)
        avg_temp = sum(m.temperature for m in recent_metrics) / len(recent_metrics)
        max_temp = max(m.temperature for m in recent_metrics)
        avg_power = sum(m.power_usage for m in recent_metrics) / len(recent_metrics)
        max_power = max(m.power_usage for m in recent_metrics)
        
        return {
            "gpu_name": self.gpu_name,
            "total_memory_gb": self.total_memory_gb,
            "average_usage_1min": round(avg_usage, 1),
            "max_usage_1min": round(max_usage, 1),
            "average_memory_usage": round(avg_memory, 1),
            "max_memory_usage": round(max_memory, 1),
            "average_temperature": round(avg_temp, 1),
            "max_temperature": round(max_temp, 1),
            "average_power": round(avg_power, 1),
            "max_power": round(max_power, 1),
            "memory_allocations": self.get_memory_allocation_status()
        }
    
    def optimize_for_ai_training(self):
        """ä¸ºAIè®­ç»ƒä¼˜åŒ–GPUè®¾ç½®"""
        try:
            logger.info("å¼€å§‹ä¸ºAIè®­ç»ƒä¼˜åŒ–GPUè®¾ç½®...")
            
            # è®¾ç½®PyTorchä¼˜åŒ–
            if torch.cuda.is_available():
                # å¯ç”¨cudnnåŸºå‡†æ¨¡å¼
                torch.backends.cudnn.benchmark = True
                
                # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
                torch.backends.cudnn.allow_tf32 = True
                torch.backends.cuda.matmul.allow_tf32 = True
                
                # è®¾ç½®æ˜¾å­˜åˆ†é…ç­–ç•¥
                torch.cuda.set_per_process_memory_fraction(0.95)
                
                logger.info("PyTorch GPUä¼˜åŒ–å®Œæˆ")
            
            # è®¾ç½®GPUæ€§èƒ½æ¨¡å¼
            subprocess.run(['nvidia-smi', '-pm', '1'], capture_output=True, check=False)  # æŒä¹…æ¨¡å¼
            subprocess.run(['nvidia-smi', '-pl', '170'], capture_output=True, check=False)  # æœ€å¤§åŠŸè€—
            
            logger.info("GPU AIè®­ç»ƒä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"GPU AIè®­ç»ƒä¼˜åŒ–å¤±è´¥: {e}")
    
    def create_memory_pool(self, pool_name: str, size_gb: float) -> bool:
        """åˆ›å»ºæ˜¾å­˜æ± """
        try:
            size_bytes = int(size_gb * 1024**3)
            memory_tensor = torch.empty(size_bytes // 4, dtype=torch.float32, device=f'cuda:{self.gpu_id}')
            
            with self.memory_lock:
                self.memory_pool[f"pool_{pool_name}"] = memory_tensor
            
            logger.info(f"åˆ›å»ºæ˜¾å­˜æ±  {pool_name}: {size_gb}GB")
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ˜¾å­˜æ± å¤±è´¥: {e}")
            return False
    
    def get_available_memory(self) -> float:
        """è·å–å¯ç”¨æ˜¾å­˜ (GB)"""
        try:
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(self.handle)
            return memory_info.free / (1024**3)
        except:
            return 0.0
    
    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self.monitoring = False
        logger.info("GPUæ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def cleanup_memory(self):
        """æ¸…ç†æ‰€æœ‰æ˜¾å­˜"""
        try:
            with self.memory_lock:
                for model_name in list(self.memory_pool.keys()):
                    del self.memory_pool[model_name]
                
                # é‡ç½®åˆ†é…è®°å½•
                for allocation in self.memory_allocations.values():
                    allocation.allocated_memory = 0
                    allocation.active_models.clear()
            
            torch.cuda.empty_cache()
            logger.info("å·²æ¸…ç†æ‰€æœ‰GPUæ˜¾å­˜")
            
        except Exception as e:
            logger.error(f"æ¸…ç†GPUæ˜¾å­˜å¤±è´¥: {e}")


# å…¨å±€GPUç®¡ç†å™¨å®ä¾‹
gpu_manager = GPUMemoryManager()


def allocate_gpu_memory(task_type: GPUTaskType, model_name: str, 
                       memory_size_mb: Optional[int] = None) -> Optional[torch.Tensor]:
    """åˆ†é…GPUæ˜¾å­˜"""
    return gpu_manager.allocate_memory(task_type, model_name, memory_size_mb)


def deallocate_gpu_memory(model_name: str) -> bool:
    """é‡Šæ”¾GPUæ˜¾å­˜"""
    return gpu_manager.deallocate_memory(model_name)


async def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨GPUç®¡ç†å™¨æµ‹è¯•...")
    
    if not gpu_manager.gpu_available:
        logger.error("GPUä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")
        return
    
    # ä¼˜åŒ–GPUè®¾ç½®
    gpu_manager.optimize_for_ai_training()
    
    # å¯åŠ¨æ€§èƒ½ç›‘æ§
    monitor_task = asyncio.create_task(gpu_manager.start_performance_monitoring())
    
    try:
        # æµ‹è¯•æ˜¾å­˜åˆ†é…
        memory1 = allocate_gpu_memory(GPUTaskType.REINFORCEMENT_LEARNING, "test_model_1", 1024)  # 1GB
        memory2 = allocate_gpu_memory(GPUTaskType.TIME_SERIES_DEEP, "test_model_2", 512)  # 512MB
        
        # è¿è¡Œ30ç§’æµ‹è¯•
        await asyncio.sleep(30)
        
        # è·å–æ€§èƒ½æ‘˜è¦
        summary = gpu_manager.get_performance_summary()
        logger.info(f"GPUæ€§èƒ½æ‘˜è¦: {json.dumps(summary, indent=2)}")
        
        # æ¸…ç†æ˜¾å­˜
        deallocate_gpu_memory("test_model_1")
        deallocate_gpu_memory("test_model_2")
        
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·...")
    finally:
        gpu_manager.stop_monitoring()
        monitor_task.cancel()
        gpu_manager.cleanup_memory()


if __name__ == "__main__":
    asyncio.run(main())
