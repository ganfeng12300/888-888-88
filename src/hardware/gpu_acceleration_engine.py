"""
ğŸ”§ GPUåŠ é€Ÿè®¡ç®—å¼•æ“
ç”Ÿäº§çº§GPUåŠ é€Ÿè®¡ç®—ç³»ç»Ÿï¼Œæ”¯æŒCUDA/OpenCLå¤šå¹³å°åŠ é€Ÿ
å®ç°å®Œæ•´çš„GPUå†…å­˜ç®¡ç†ã€å¤šGPUå¹¶è¡Œè®¡ç®—ã€æ€§èƒ½ç›‘æ§ç­‰åŠŸèƒ½
ä¸“ä¸ºé«˜é¢‘é‡åŒ–äº¤æ˜“åœºæ™¯ä¼˜åŒ–ï¼Œæä¾›å¾®ç§’çº§è®¡ç®—å“åº”
"""
import asyncio
import numpy as np
import cupy as cp
import threading
import time
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
import logging
from datetime import datetime
from collections import deque
import psutil
import GPUtil

try:
    import pycuda.driver as cuda
    import pycuda.autoinit
    from pycuda.compiler import SourceModule
    CUDA_AVAILABLE = True
except ImportError:
    CUDA_AVAILABLE = False
    
try:
    import pyopencl as cl
    OPENCL_AVAILABLE = True
except ImportError:
    OPENCL_AVAILABLE = False

@dataclass
class GPUDevice:
    """GPUè®¾å¤‡ä¿¡æ¯"""
    device_id: int
    name: str
    memory_total: int
    memory_free: int
    compute_capability: Tuple[int, int]
    multiprocessor_count: int
    max_threads_per_block: int
    is_available: bool = True
    utilization: float = 0.0
    temperature: float = 0.0

@dataclass
class ComputeTask:
    """è®¡ç®—ä»»åŠ¡"""
    task_id: str
    task_type: str  # 'matrix_multiply', 'convolution', 'fft', 'reduction'
    input_data: np.ndarray
    parameters: Dict[str, Any]
    priority: int = 1
    created_at: datetime = field(default_factory=datetime.now)
    device_id: Optional[int] = None

@dataclass
class ComputeResult:
    """è®¡ç®—ç»“æœ"""
    task_id: str
    result_data: np.ndarray
    execution_time: float
    device_id: int
    memory_used: int
    completed_at: datetime = field(default_factory=datetime.now)

class CUDAKernelManager:
    """CUDAå†…æ ¸ç®¡ç†å™¨"""
    
    def __init__(self):
        self.kernels = {}
        self._compile_kernels()
        
    def _compile_kernels(self):
        """ç¼–è¯‘CUDAå†…æ ¸"""
        if not CUDA_AVAILABLE:
            return
            
        # çŸ©é˜µä¹˜æ³•å†…æ ¸
        matrix_multiply_kernel = """
        __global__ void matrix_multiply(float *A, float *B, float *C, int M, int N, int K) {
            int row = blockIdx.y * blockDim.y + threadIdx.y;
            int col = blockIdx.x * blockDim.x + threadIdx.x;
            
            if (row < M && col < N) {
                float sum = 0.0f;
                for (int k = 0; k < K; k++) {
                    sum += A[row * K + k] * B[k * N + col];
                }
                C[row * N + col] = sum;
            }
        }
        """
        
        # å·ç§¯å†…æ ¸
        convolution_kernel = """
        __global__ void convolution_2d(float *input, float *kernel, float *output,
                                     int input_height, int input_width,
                                     int kernel_height, int kernel_width,
                                     int output_height, int output_width) {
            int out_row = blockIdx.y * blockDim.y + threadIdx.y;
            int out_col = blockIdx.x * blockDim.x + threadIdx.x;
            
            if (out_row < output_height && out_col < output_width) {
                float sum = 0.0f;
                for (int k_row = 0; k_row < kernel_height; k_row++) {
                    for (int k_col = 0; k_col < kernel_width; k_col++) {
                        int in_row = out_row + k_row;
                        int in_col = out_col + k_col;
                        if (in_row < input_height && in_col < input_width) {
                            sum += input[in_row * input_width + in_col] * 
                                   kernel[k_row * kernel_width + k_col];
                        }
                    }
                }
                output[out_row * output_width + out_col] = sum;
            }
        }
        """
        
        # å½’çº¦å†…æ ¸
        reduction_kernel = """
        __global__ void reduce_sum(float *input, float *output, int n) {
            extern __shared__ float sdata[];
            
            unsigned int tid = threadIdx.x;
            unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
            
            sdata[tid] = (i < n) ? input[i] : 0;
            __syncthreads();
            
            for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
                if (tid < s) {
                    sdata[tid] += sdata[tid + s];
                }
                __syncthreads();
            }
            
            if (tid == 0) output[blockIdx.x] = sdata[0];
        }
        """
        
        try:
            self.kernels['matrix_multiply'] = SourceModule(matrix_multiply_kernel)
            self.kernels['convolution'] = SourceModule(convolution_kernel)
            self.kernels['reduction'] = SourceModule(reduction_kernel)
        except Exception as e:
            logging.error(f"CUDAå†…æ ¸ç¼–è¯‘å¤±è´¥: {e}")

class GPUAccelerationEngine:
    """GPUåŠ é€Ÿè®¡ç®—å¼•æ“"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.is_running = False
        self.executor = ThreadPoolExecutor(max_workers=8)
        
        # GPUè®¾å¤‡ç®¡ç†
        self.gpu_devices: List[GPUDevice] = []
        self.current_device = 0
        self.device_locks = {}
        
        # è®¡ç®—ä»»åŠ¡ç®¡ç†
        self.task_queue = asyncio.Queue()
        self.result_cache = {}
        self.active_tasks = {}
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = deque(maxlen=10000)
        self.device_utilization = {}
        
        # å†…æ ¸ç®¡ç†
        self.kernel_manager = CUDAKernelManager()
        
        # å†…å­˜ç®¡ç†
        self.memory_pools = {}
        self.memory_usage = {}
        
        self._initialize_devices()
        
    def _initialize_devices(self):
        """åˆå§‹åŒ–GPUè®¾å¤‡"""
        try:
            if CUDA_AVAILABLE:
                self._initialize_cuda_devices()
            elif OPENCL_AVAILABLE:
                self._initialize_opencl_devices()
            else:
                self.logger.warning("æœªæ£€æµ‹åˆ°GPUåŠ é€Ÿæ”¯æŒï¼Œå°†ä½¿ç”¨CPUè®¡ç®—")
                
        except Exception as e:
            self.logger.error(f"GPUè®¾å¤‡åˆå§‹åŒ–å¤±è´¥: {e}")
            
    def _initialize_cuda_devices(self):
        """åˆå§‹åŒ–CUDAè®¾å¤‡"""
        try:
            gpus = GPUtil.getGPUs()
            for i, gpu in enumerate(gpus):
                device = GPUDevice(
                    device_id=i,
                    name=gpu.name,
                    memory_total=int(gpu.memoryTotal),
                    memory_free=int(gpu.memoryFree),
                    compute_capability=(gpu.driver, 0),  # ç®€åŒ–ç‰ˆæœ¬
                    multiprocessor_count=0,  # éœ€è¦é€šè¿‡CUDA APIè·å–
                    max_threads_per_block=1024,  # é»˜è®¤å€¼
                    utilization=gpu.load * 100,
                    temperature=gpu.temperature
                )
                self.gpu_devices.append(device)
                self.device_locks[i] = threading.Lock()
                self.memory_pools[i] = {}
                self.memory_usage[i] = 0
                
            self.logger.info(f"åˆå§‹åŒ–äº† {len(self.gpu_devices)} ä¸ªCUDAè®¾å¤‡")
            
        except Exception as e:
            self.logger.error(f"CUDAè®¾å¤‡åˆå§‹åŒ–å¤±è´¥: {e}")
            
    def _initialize_opencl_devices(self):
        """åˆå§‹åŒ–OpenCLè®¾å¤‡"""
        try:
            platforms = cl.get_platforms()
            device_id = 0
            
            for platform in platforms:
                devices = platform.get_devices(cl.device_type.GPU)
                for device in devices:
                    gpu_device = GPUDevice(
                        device_id=device_id,
                        name=device.name.strip(),
                        memory_total=device.global_mem_size // (1024 * 1024),
                        memory_free=device.global_mem_size // (1024 * 1024),
                        compute_capability=(1, 0),
                        multiprocessor_count=device.max_compute_units,
                        max_threads_per_block=device.max_work_group_size
                    )
                    self.gpu_devices.append(gpu_device)
                    self.device_locks[device_id] = threading.Lock()
                    self.memory_pools[device_id] = {}
                    self.memory_usage[device_id] = 0
                    device_id += 1
                    
            self.logger.info(f"åˆå§‹åŒ–äº† {len(self.gpu_devices)} ä¸ªOpenCLè®¾å¤‡")
            
        except Exception as e:
            self.logger.error(f"OpenCLè®¾å¤‡åˆå§‹åŒ–å¤±è´¥: {e}")
            
    async def start(self):
        """å¯åŠ¨GPUåŠ é€Ÿå¼•æ“"""
        self.is_running = True
        self.logger.info("ğŸ”§ GPUåŠ é€Ÿå¼•æ“å¯åŠ¨")
        
        # å¯åŠ¨ä»»åŠ¡å¤„ç†å¾ªç¯
        tasks = [
            asyncio.create_task(self._task_processing_loop()),
            asyncio.create_task(self._performance_monitoring_loop()),
            asyncio.create_task(self._memory_management_loop()),
            asyncio.create_task(self._device_health_monitoring_loop())
        ]
        
        await asyncio.gather(*tasks)
        
    async def stop(self):
        """åœæ­¢GPUåŠ é€Ÿå¼•æ“"""
        self.is_running = False
        self.executor.shutdown(wait=True)
        self.logger.info("ğŸ”§ GPUåŠ é€Ÿå¼•æ“åœæ­¢")
        
    async def _task_processing_loop(self):
        """ä»»åŠ¡å¤„ç†å¾ªç¯"""
        while self.is_running:
            try:
                # è·å–ä»»åŠ¡
                task = await asyncio.wait_for(self.task_queue.get(), timeout=1.0)
                
                # é€‰æ‹©æœ€ä¼˜è®¾å¤‡
                device_id = await self._select_optimal_device(task)
                
                if device_id is not None:
                    # æ‰§è¡Œä»»åŠ¡
                    result = await self._execute_task(task, device_id)
                    
                    # ç¼“å­˜ç»“æœ
                    self.result_cache[task.task_id] = result
                    
                    # è®°å½•æ€§èƒ½æŒ‡æ ‡
                    await self._record_performance_metrics(task, result)
                    
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                self.logger.error(f"ä»»åŠ¡å¤„ç†é”™è¯¯: {e}")
                await asyncio.sleep(1)
                
    async def _select_optimal_device(self, task: ComputeTask) -> Optional[int]:
        """é€‰æ‹©æœ€ä¼˜GPUè®¾å¤‡"""
        if not self.gpu_devices:
            return None
            
        # å¦‚æœæŒ‡å®šäº†è®¾å¤‡ï¼Œç›´æ¥ä½¿ç”¨
        if task.device_id is not None and task.device_id < len(self.gpu_devices):
            return task.device_id
            
        # æ ¹æ®è®¾å¤‡è´Ÿè½½å’Œå†…å­˜ä½¿ç”¨æƒ…å†µé€‰æ‹©
        best_device = None
        best_score = float('inf')
        
        for device in self.gpu_devices:
            if not device.is_available:
                continue
                
            # è®¡ç®—è®¾å¤‡è¯„åˆ†ï¼ˆè´Ÿè½½ + å†…å­˜ä½¿ç”¨ç‡ï¼‰
            memory_usage_ratio = self.memory_usage[device.device_id] / device.memory_total
            score = device.utilization + memory_usage_ratio * 100
            
            if score < best_score:
                best_score = score
                best_device = device.device_id
                
        return best_device
        
    async def _execute_task(self, task: ComputeTask, device_id: int) -> ComputeResult:
        """æ‰§è¡Œè®¡ç®—ä»»åŠ¡"""
        start_time = time.time()
        
        try:
            with self.device_locks[device_id]:
                # è®¾ç½®å½“å‰è®¾å¤‡
                if CUDA_AVAILABLE:
                    cp.cuda.Device(device_id).use()
                    
                # æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œè®¡ç®—
                if task.task_type == 'matrix_multiply':
                    result_data = await self._execute_matrix_multiply(task, device_id)
                elif task.task_type == 'convolution':
                    result_data = await self._execute_convolution(task, device_id)
                elif task.task_type == 'fft':
                    result_data = await self._execute_fft(task, device_id)
                elif task.task_type == 'reduction':
                    result_data = await self._execute_reduction(task, device_id)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task.task_type}")
                    
                execution_time = time.time() - start_time
                
                return ComputeResult(
                    task_id=task.task_id,
                    result_data=result_data,
                    execution_time=execution_time,
                    device_id=device_id,
                    memory_used=result_data.nbytes
                )
                
        except Exception as e:
            self.logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ {task.task_id}: {e}")
            raise
            
    async def _execute_matrix_multiply(self, task: ComputeTask, device_id: int) -> np.ndarray:
        """æ‰§è¡ŒçŸ©é˜µä¹˜æ³•"""
        input_data = task.input_data
        
        if len(input_data) != 2:
            raise ValueError("çŸ©é˜µä¹˜æ³•éœ€è¦ä¸¤ä¸ªè¾“å…¥çŸ©é˜µ")
            
        A, B = input_data[0], input_data[1]
        
        if CUDA_AVAILABLE:
            # ä½¿ç”¨CuPyè¿›è¡ŒGPUè®¡ç®—
            A_gpu = cp.asarray(A)
            B_gpu = cp.asarray(B)
            C_gpu = cp.dot(A_gpu, B_gpu)
            result = cp.asnumpy(C_gpu)
        else:
            # å›é€€åˆ°CPUè®¡ç®—
            result = np.dot(A, B)
            
        return result
        
    async def _execute_convolution(self, task: ComputeTask, device_id: int) -> np.ndarray:
        """æ‰§è¡Œå·ç§¯è®¡ç®—"""
        input_data = task.input_data
        kernel = task.parameters.get('kernel')
        
        if kernel is None:
            raise ValueError("å·ç§¯è®¡ç®—éœ€è¦æä¾›kernelå‚æ•°")
            
        if CUDA_AVAILABLE:
            # ä½¿ç”¨CuPyè¿›è¡ŒGPUå·ç§¯
            input_gpu = cp.asarray(input_data)
            kernel_gpu = cp.asarray(kernel)
            
            # ç®€åŒ–çš„2Då·ç§¯å®ç°
            result_gpu = cp.zeros((
                input_data.shape[0] - kernel.shape[0] + 1,
                input_data.shape[1] - kernel.shape[1] + 1
            ))
            
            for i in range(result_gpu.shape[0]):
                for j in range(result_gpu.shape[1]):
                    result_gpu[i, j] = cp.sum(
                        input_gpu[i:i+kernel.shape[0], j:j+kernel.shape[1]] * kernel_gpu
                    )
                    
            result = cp.asnumpy(result_gpu)
        else:
            # å›é€€åˆ°CPUè®¡ç®—
            from scipy import ndimage
            result = ndimage.convolve(input_data, kernel)
            
        return result
        
    async def _execute_fft(self, task: ComputeTask, device_id: int) -> np.ndarray:
        """æ‰§è¡ŒFFTè®¡ç®—"""
        input_data = task.input_data
        
        if CUDA_AVAILABLE:
            # ä½¿ç”¨CuPyè¿›è¡ŒGPU FFT
            input_gpu = cp.asarray(input_data)
            result_gpu = cp.fft.fft(input_gpu)
            result = cp.asnumpy(result_gpu)
        else:
            # å›é€€åˆ°CPUè®¡ç®—
            result = np.fft.fft(input_data)
            
        return result
        
    async def _execute_reduction(self, task: ComputeTask, device_id: int) -> np.ndarray:
        """æ‰§è¡Œå½’çº¦è®¡ç®—"""
        input_data = task.input_data
        operation = task.parameters.get('operation', 'sum')
        
        if CUDA_AVAILABLE:
            input_gpu = cp.asarray(input_data)
            
            if operation == 'sum':
                result_gpu = cp.sum(input_gpu)
            elif operation == 'mean':
                result_gpu = cp.mean(input_gpu)
            elif operation == 'max':
                result_gpu = cp.max(input_gpu)
            elif operation == 'min':
                result_gpu = cp.min(input_gpu)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å½’çº¦æ“ä½œ: {operation}")
                
            result = cp.asnumpy(result_gpu)
        else:
            # å›é€€åˆ°CPUè®¡ç®—
            if operation == 'sum':
                result = np.sum(input_data)
            elif operation == 'mean':
                result = np.mean(input_data)
            elif operation == 'max':
                result = np.max(input_data)
            elif operation == 'min':
                result = np.min(input_data)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å½’çº¦æ“ä½œ: {operation}")
                
        return np.array([result])
        
    async def _performance_monitoring_loop(self):
        """æ€§èƒ½ç›‘æ§å¾ªç¯"""
        while self.is_running:
            try:
                # æ›´æ–°è®¾å¤‡çŠ¶æ€
                await self._update_device_status()
                
                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                await self._collect_performance_metrics()
                
                await asyncio.sleep(5)  # 5ç§’æ›´æ–°ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"æ€§èƒ½ç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(10)
                
    async def _update_device_status(self):
        """æ›´æ–°è®¾å¤‡çŠ¶æ€"""
        try:
            gpus = GPUtil.getGPUs()
            for i, gpu in enumerate(gpus):
                if i < len(self.gpu_devices):
                    device = self.gpu_devices[i]
                    device.memory_free = int(gpu.memoryFree)
                    device.utilization = gpu.load * 100
                    device.temperature = gpu.temperature
                    device.is_available = gpu.temperature < 85  # æ¸©åº¦ä¿æŠ¤
                    
        except Exception as e:
            self.logger.error(f"è®¾å¤‡çŠ¶æ€æ›´æ–°å¤±è´¥: {e}")
            
    async def _collect_performance_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            'timestamp': datetime.now(),
            'devices': [],
            'system': {
                'cpu_usage': psutil.cpu_percent(),
                'memory_usage': psutil.virtual_memory().percent,
                'disk_usage': psutil.disk_usage('/').percent
            }
        }
        
        for device in self.gpu_devices:
            device_metrics = {
                'device_id': device.device_id,
                'name': device.name,
                'utilization': device.utilization,
                'memory_used': device.memory_total - device.memory_free,
                'memory_total': device.memory_total,
                'temperature': device.temperature,
                'is_available': device.is_available
            }
            metrics['devices'].append(device_metrics)
            
        self.performance_metrics.append(metrics)
        
    async def _memory_management_loop(self):
        """å†…å­˜ç®¡ç†å¾ªç¯"""
        while self.is_running:
            try:
                # æ¸…ç†è¿‡æœŸçš„ç»“æœç¼“å­˜
                await self._cleanup_result_cache()
                
                # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
                await self._optimize_memory_usage()
                
                await asyncio.sleep(30)  # 30ç§’æ¸…ç†ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"å†…å­˜ç®¡ç†é”™è¯¯: {e}")
                await asyncio.sleep(60)
                
    async def _cleanup_result_cache(self):
        """æ¸…ç†ç»“æœç¼“å­˜"""
        current_time = datetime.now()
        expired_keys = []
        
        for task_id, result in self.result_cache.items():
            # æ¸…ç†è¶…è¿‡1å°æ—¶çš„ç¼“å­˜
            if (current_time - result.completed_at).total_seconds() > 3600:
                expired_keys.append(task_id)
                
        for key in expired_keys:
            del self.result_cache[key]
            
        if expired_keys:
            self.logger.info(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜")
            
    async def _optimize_memory_usage(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        for device_id in self.memory_pools:
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
            if self.memory_usage[device_id] > self.gpu_devices[device_id].memory_total * 0.8:
                # å†…å­˜ä½¿ç”¨è¶…è¿‡80%ï¼Œè¿›è¡Œåƒåœ¾å›æ”¶
                if CUDA_AVAILABLE:
                    cp.cuda.Device(device_id).use()
                    cp.get_default_memory_pool().free_all_blocks()
                    
                self.logger.info(f"è®¾å¤‡ {device_id} æ‰§è¡Œå†…å­˜ä¼˜åŒ–")
                
    async def _device_health_monitoring_loop(self):
        """è®¾å¤‡å¥åº·ç›‘æ§å¾ªç¯"""
        while self.is_running:
            try:
                for device in self.gpu_devices:
                    # æ£€æŸ¥è®¾å¤‡æ¸©åº¦
                    if device.temperature > 80:
                        self.logger.warning(f"è®¾å¤‡ {device.device_id} æ¸©åº¦è¿‡é«˜: {device.temperature}Â°C")
                        
                    # æ£€æŸ¥è®¾å¤‡åˆ©ç”¨ç‡
                    if device.utilization > 95:
                        self.logger.warning(f"è®¾å¤‡ {device.device_id} åˆ©ç”¨ç‡è¿‡é«˜: {device.utilization}%")
                        
                    # æ£€æŸ¥å†…å­˜ä½¿ç”¨
                    memory_usage_ratio = (device.memory_total - device.memory_free) / device.memory_total
                    if memory_usage_ratio > 0.9:
                        self.logger.warning(f"è®¾å¤‡ {device.device_id} å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_usage_ratio:.1%}")
                        
                await asyncio.sleep(10)  # 10ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"è®¾å¤‡å¥åº·ç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(30)
                
    async def _record_performance_metrics(self, task: ComputeTask, result: ComputeResult):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            'task_id': task.task_id,
            'task_type': task.task_type,
            'device_id': result.device_id,
            'execution_time': result.execution_time,
            'memory_used': result.memory_used,
            'throughput': result.result_data.size / result.execution_time,
            'timestamp': result.completed_at
        }
        
        # è¿™é‡Œå¯ä»¥å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
        self.logger.debug(f"ä»»åŠ¡æ€§èƒ½: {metrics}")
        
    # å…¬å…±æ¥å£æ–¹æ³•
    async def submit_task(self, task: ComputeTask) -> str:
        """æäº¤è®¡ç®—ä»»åŠ¡"""
        await self.task_queue.put(task)
        self.active_tasks[task.task_id] = task
        return task.task_id
        
    async def get_result(self, task_id: str, timeout: float = 30.0) -> Optional[ComputeResult]:
        """è·å–è®¡ç®—ç»“æœ"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            if task_id in self.result_cache:
                result = self.result_cache[task_id]
                # æ¸…ç†å·²è·å–çš„ç»“æœ
                if task_id in self.active_tasks:
                    del self.active_tasks[task_id]
                return result
                
            await asyncio.sleep(0.1)
            
        return None
        
    async def matrix_multiply_async(self, A: np.ndarray, B: np.ndarray, 
                                  device_id: Optional[int] = None) -> np.ndarray:
        """å¼‚æ­¥çŸ©é˜µä¹˜æ³•"""
        task = ComputeTask(
            task_id=f"matmul_{int(time.time() * 1000000)}",
            task_type='matrix_multiply',
            input_data=[A, B],
            parameters={},
            device_id=device_id
        )
        
        task_id = await self.submit_task(task)
        result = await self.get_result(task_id)
        
        if result is None:
            raise TimeoutError("çŸ©é˜µä¹˜æ³•è®¡ç®—è¶…æ—¶")
            
        return result.result_data
        
    async def convolution_async(self, input_data: np.ndarray, kernel: np.ndarray,
                              device_id: Optional[int] = None) -> np.ndarray:
        """å¼‚æ­¥å·ç§¯è®¡ç®—"""
        task = ComputeTask(
            task_id=f"conv_{int(time.time() * 1000000)}",
            task_type='convolution',
            input_data=input_data,
            parameters={'kernel': kernel},
            device_id=device_id
        )
        
        task_id = await self.submit_task(task)
        result = await self.get_result(task_id)
        
        if result is None:
            raise TimeoutError("å·ç§¯è®¡ç®—è¶…æ—¶")
            
        return result.result_data
        
    def get_device_info(self) -> List[Dict[str, Any]]:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        return [
            {
                'device_id': device.device_id,
                'name': device.name,
                'memory_total': device.memory_total,
                'memory_free': device.memory_free,
                'utilization': device.utilization,
                'temperature': device.temperature,
                'is_available': device.is_available
            }
            for device in self.gpu_devices
        ]
        
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.performance_metrics:
            return {}
            
        recent_metrics = list(self.performance_metrics)[-100:]  # æœ€è¿‘100æ¡è®°å½•
        
        return {
            'total_devices': len(self.gpu_devices),
            'active_tasks': len(self.active_tasks),
            'cached_results': len(self.result_cache),
            'recent_metrics_count': len(recent_metrics),
            'average_device_utilization': np.mean([
                np.mean([d['utilization'] for d in m['devices']]) 
                for m in recent_metrics if m['devices']
            ]) if recent_metrics else 0
        }
