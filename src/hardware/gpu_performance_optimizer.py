#!/usr/bin/env python3
"""
ğŸš€ GPUæ€§èƒ½ä¼˜åŒ–å™¨ - ç¡¬ä»¶åŠ é€Ÿç³»ç»Ÿ
GPU Performance Optimizer - Hardware Acceleration System

ä¸“ä¸º20æ ¸CPU + GTX3060 12Gé…ç½®ä¼˜åŒ–
- GPUåŠ é€Ÿæ·±åº¦å­¦ä¹ è®­ç»ƒ
- å†…å­˜ç®¡ç†ä¼˜åŒ–
- å¹¶è¡Œè®¡ç®—è°ƒåº¦
- æ€§èƒ½ç›‘æ§å’Œè°ƒä¼˜
"""

import os
import time
import threading
import psutil
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from loguru import logger

try:
    import torch
    import torch.cuda as cuda
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("PyTorchæœªå®‰è£…ï¼ŒGPUåŠ é€ŸåŠŸèƒ½å°†è¢«ç¦ç”¨")

try:
    import cupy as cp
    CUPY_AVAILABLE = True
except ImportError:
    CUPY_AVAILABLE = False
    logger.warning("CuPyæœªå®‰è£…ï¼Œéƒ¨åˆ†GPUè®¡ç®—åŠŸèƒ½å°†è¢«ç¦ç”¨")


@dataclass
class GPUStatus:
    """GPUçŠ¶æ€ä¿¡æ¯"""
    device_id: int
    name: str
    total_memory: int
    used_memory: int
    free_memory: int
    utilization: float
    temperature: float
    power_usage: float
    is_available: bool


@dataclass
class CPUStatus:
    """CPUçŠ¶æ€ä¿¡æ¯"""
    core_count: int
    thread_count: int
    usage_percent: float
    frequency: float
    temperature: float
    load_average: List[float]


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    gpu_status: GPUStatus
    cpu_status: CPUStatus
    memory_usage: Dict[str, float]
    disk_usage: Dict[str, float]
    network_usage: Dict[str, float]
    timestamp: datetime


class GPUPerformanceOptimizer:
    """GPUæ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–GPUæ€§èƒ½ä¼˜åŒ–å™¨"""
        self.config = config or {}
        self.is_running = False
        self.performance_history = []
        self.optimization_tasks = []
        self.lock = threading.Lock()
        
        # ç³»ç»Ÿé…ç½®
        self.target_gpu_utilization = self.config.get('target_gpu_utilization', 85.0)
        self.max_memory_usage = self.config.get('max_memory_usage', 90.0)
        self.monitoring_interval = self.config.get('monitoring_interval', 5)
        self.optimization_interval = self.config.get('optimization_interval', 30)
        
        # åˆå§‹åŒ–ç¡¬ä»¶æ£€æµ‹
        self._initialize_hardware()
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self._start_monitoring()
        
        logger.info("ğŸš€ GPUæ€§èƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def optimize_performance(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–"""
        try:
            optimization_results = {}
            
            # GPUä¼˜åŒ–
            if self.gpu_available:
                gpu_optimization = self._optimize_gpu_performance()
                optimization_results['gpu'] = gpu_optimization
            
            # CPUä¼˜åŒ–
            cpu_optimization = self._optimize_cpu_performance()
            optimization_results['cpu'] = cpu_optimization
            
            # å†…å­˜ä¼˜åŒ–
            memory_optimization = self._optimize_memory_usage()
            optimization_results['memory'] = memory_optimization
            
            return {
                'status': 'success',
                'optimizations': optimization_results,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {str(e)}")
            return {
                'status': 'failed',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _optimize_gpu_performance(self) -> Dict[str, Any]:
        """ä¼˜åŒ–GPUæ€§èƒ½"""
        try:
            if not self.gpu_available:
                return {'status': 'skipped', 'reason': 'GPUä¸å¯ç”¨'}
            
            # æ¸…ç†GPUç¼“å­˜
            if TORCH_AVAILABLE:
                torch.cuda.empty_cache()
            
            # è·å–å½“å‰GPUçŠ¶æ€
            gpu_status = self.get_gpu_status()
            
            # åŠ¨æ€è°ƒæ•´GPUå†…å­˜åˆ†é…
            if gpu_status and gpu_status.utilization > 90:
                # GPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œæ¸…ç†ç¼“å­˜
                if TORCH_AVAILABLE:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
            
            return {
                'status': 'success',
                'actions': ['cache_cleared', 'memory_optimized'],
                'gpu_utilization': gpu_status.utilization if gpu_status else 0
            }
            
        except Exception as e:
            return {'status': 'failed', 'error': str(e)}
    
    def _optimize_cpu_performance(self) -> Dict[str, Any]:
        """ä¼˜åŒ–CPUæ€§èƒ½"""
        try:
            # è·å–CPUä½¿ç”¨ç‡
            cpu_usage = psutil.cpu_percent(interval=1)
            
            # åŠ¨æ€è°ƒæ•´çº¿ç¨‹æ•°
            if cpu_usage > 80:
                # CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œå‡å°‘çº¿ç¨‹æ•°
                new_threads = max(1, self.cpu_threads // 2)
            else:
                # CPUä½¿ç”¨ç‡æ­£å¸¸ï¼Œä½¿ç”¨æœ€ä¼˜çº¿ç¨‹æ•°
                new_threads = min(self.cpu_threads, 32)
            
            if TORCH_AVAILABLE:
                torch.set_num_threads(new_threads)
            
            return {
                'status': 'success',
                'actions': ['thread_count_adjusted'],
                'cpu_usage': cpu_usage,
                'thread_count': new_threads
            }
            
        except Exception as e:
            return {'status': 'failed', 'error': str(e)}
    
    def _optimize_memory_usage(self) -> Dict[str, Any]:
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        try:
            # è·å–å†…å­˜ä¿¡æ¯
            memory = psutil.virtual_memory()
            
            # å¦‚æœå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œè§¦å‘åƒåœ¾å›æ”¶
            if memory.percent > 85:
                import gc
                gc.collect()
                
                # å¦‚æœæœ‰GPUï¼Œä¹Ÿæ¸…ç†GPUå†…å­˜
                if self.gpu_available and TORCH_AVAILABLE:
                    torch.cuda.empty_cache()
            
            return {
                'status': 'success',
                'actions': ['garbage_collection', 'gpu_cache_cleared'] if memory.percent > 85 else ['monitoring'],
                'memory_usage_percent': memory.percent,
                'available_memory_gb': memory.available / (1024**3)
            }
            
        except Exception as e:
            return {'status': 'failed', 'error': str(e)}
    
    def get_optimization_suggestions(self) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        try:
            # æ£€æŸ¥GPUçŠ¶æ€
            if self.gpu_available:
                gpu_status = self.get_gpu_status()
                if gpu_status:
                    if gpu_status.utilization < 30:
                        suggestions.append("GPUåˆ©ç”¨ç‡è¾ƒä½ï¼Œè€ƒè™‘å¢åŠ æ‰¹å¤„ç†å¤§å°")
                    elif gpu_status.utilization > 90:
                        suggestions.append("GPUåˆ©ç”¨ç‡è¿‡é«˜ï¼Œè€ƒè™‘å‡å°‘æ‰¹å¤„ç†å¤§å°æˆ–æ¸…ç†ç¼“å­˜")
                    
                    if gpu_status.free_memory < gpu_status.total_memory * 0.1:
                        suggestions.append("GPUå†…å­˜ä¸è¶³ï¼Œå»ºè®®æ¸…ç†ç¼“å­˜æˆ–å‡å°‘æ¨¡å‹å¤§å°")
            else:
                suggestions.append("æœªæ£€æµ‹åˆ°GPUï¼Œå»ºè®®å®‰è£…CUDAå’ŒPyTorchä»¥å¯ç”¨GPUåŠ é€Ÿ")
            
            # æ£€æŸ¥CPUçŠ¶æ€
            cpu_usage = psutil.cpu_percent(interval=1)
            if cpu_usage > 80:
                suggestions.append("CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œè€ƒè™‘ä¼˜åŒ–ç®—æ³•æˆ–å¢åŠ ç¡¬ä»¶èµ„æº")
            elif cpu_usage < 20:
                suggestions.append("CPUåˆ©ç”¨ç‡è¾ƒä½ï¼Œå¯ä»¥å¢åŠ å¹¶è¡Œå¤„ç†ä»»åŠ¡")
            
            # æ£€æŸ¥å†…å­˜çŠ¶æ€
            memory = psutil.virtual_memory()
            if memory.percent > 85:
                suggestions.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
            
            # æ£€æŸ¥PyTorchå’ŒCuPyå®‰è£…
            if not TORCH_AVAILABLE:
                suggestions.append("å»ºè®®å®‰è£…PyTorchä»¥è·å¾—æ›´å¥½çš„æ·±åº¦å­¦ä¹ æ€§èƒ½")
            if not CUPY_AVAILABLE:
                suggestions.append("å»ºè®®å®‰è£…CuPyä»¥è·å¾—æ›´å¥½çš„GPUæ•°å€¼è®¡ç®—æ€§èƒ½")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆä¼˜åŒ–å»ºè®®å¤±è´¥: {str(e)}")
            suggestions.append("ç³»ç»ŸçŠ¶æ€æ£€æŸ¥å¼‚å¸¸ï¼Œå»ºè®®é‡å¯ä¼˜åŒ–å™¨")
        
        return suggestions if suggestions else ["ç³»ç»Ÿè¿è¡ŒçŠ¶æ€è‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šä¼˜åŒ–"]
    
    def optimize_gpu_memory(self) -> Dict[str, Any]:
        """ä¼˜åŒ–GPUå†…å­˜"""
        try:
            if not self.gpu_available:
                return {
                    'status': 'skipped',
                    'reason': 'GPUä¸å¯ç”¨',
                    'recommendations': ['å®‰è£…CUDAå’ŒPyTorchä»¥å¯ç”¨GPUåŠŸèƒ½']
                }
            
            initial_memory = torch.cuda.memory_allocated() if TORCH_AVAILABLE else 0
            
            # æ¸…ç†GPUç¼“å­˜
            if TORCH_AVAILABLE:
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            final_memory = torch.cuda.memory_allocated() if TORCH_AVAILABLE else 0
            freed_memory = initial_memory - final_memory
            
            return {
                'status': 'success',
                'initial_memory_mb': initial_memory / (1024**2),
                'final_memory_mb': final_memory / (1024**2),
                'freed_memory_mb': freed_memory / (1024**2),
                'actions': ['cache_cleared', 'memory_synchronized']
            }
            
        except Exception as e:
            return {
                'status': 'failed',
                'error': str(e),
                'recommendations': ['æ£€æŸ¥GPUé©±åŠ¨å’ŒCUDAå®‰è£…']
            }
    
    def _initialize_hardware(self):
        """åˆå§‹åŒ–ç¡¬ä»¶æ£€æµ‹"""
        # æ£€æµ‹GPU
        self.gpu_available = False
        self.gpu_devices = []
        
        if TORCH_AVAILABLE and torch.cuda.is_available():
            self.gpu_available = True
            device_count = torch.cuda.device_count()
            
            for i in range(device_count):
                device_props = torch.cuda.get_device_properties(i)
                self.gpu_devices.append({
                    'id': i,
                    'name': device_props.name,
                    'total_memory': device_props.total_memory,
                    'major': device_props.major,
                    'minor': device_props.minor,
                    'multi_processor_count': device_props.multi_processor_count
                })
            
            logger.info(f"æ£€æµ‹åˆ° {device_count} ä¸ªGPUè®¾å¤‡: {[d['name'] for d in self.gpu_devices]}")
        else:
            logger.warning("æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPUè®¾å¤‡")
        
        # æ£€æµ‹CPU
        self.cpu_count = psutil.cpu_count()
        self.cpu_threads = psutil.cpu_count(logical=True)
        
        logger.info(f"æ£€æµ‹åˆ°CPU: {self.cpu_count}æ ¸å¿ƒ {self.cpu_threads}çº¿ç¨‹")
        
        # è®¾ç½®ä¼˜åŒ–å‚æ•°
        self._configure_optimization()
    
    def _configure_optimization(self):
        """é…ç½®ä¼˜åŒ–å‚æ•°"""
        if self.gpu_available:
            # GPUä¼˜åŒ–é…ç½®
            torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–å·ç§¯æ€§èƒ½
            torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§ç®—æ³•
            
            # è®¾ç½®GPUå†…å­˜åˆ†é…ç­–ç•¥
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            
            logger.info("GPUä¼˜åŒ–é…ç½®å·²å¯ç”¨")
        
        # CPUä¼˜åŒ–é…ç½®
        if self.cpu_threads >= 16:  # 20æ ¸CPUé€šå¸¸æœ‰40çº¿ç¨‹
            # è®¾ç½®çº¿ç¨‹æ•°
            torch.set_num_threads(min(self.cpu_threads, 32))
            os.environ['OMP_NUM_THREADS'] = str(min(self.cpu_threads, 32))
            os.environ['MKL_NUM_THREADS'] = str(min(self.cpu_threads, 32))
            
            logger.info(f"CPUå¤šçº¿ç¨‹ä¼˜åŒ–å·²å¯ç”¨: {min(self.cpu_threads, 32)}çº¿ç¨‹")
    
    def _start_monitoring(self):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        self.is_running = True
        
        # æ€§èƒ½ç›‘æ§çº¿ç¨‹
        threading.Thread(
            target=self._performance_monitor_loop,
            daemon=True,
            name="PerformanceMonitorThread"
        ).start()
        
        # ä¼˜åŒ–è°ƒåº¦çº¿ç¨‹
        threading.Thread(
            target=self._optimization_loop,
            daemon=True,
            name="OptimizationThread"
        ).start()
    
    def get_gpu_status(self) -> Optional[GPUStatus]:
        """è·å–GPUçŠ¶æ€"""
        if not self.gpu_available:
            return None
        
        try:
            device_id = 0  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
            
            # è·å–å†…å­˜ä¿¡æ¯
            total_memory = torch.cuda.get_device_properties(device_id).total_memory
            allocated_memory = torch.cuda.memory_allocated(device_id)
            cached_memory = torch.cuda.memory_reserved(device_id)
            free_memory = total_memory - cached_memory
            
            # è·å–åˆ©ç”¨ç‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            utilization = (allocated_memory / total_memory) * 100
            
            return GPUStatus(
                device_id=device_id,
                name=self.gpu_devices[device_id]['name'],
                total_memory=total_memory,
                used_memory=allocated_memory,
                free_memory=free_memory,
                utilization=utilization,
                temperature=0.0,  # éœ€è¦nvidia-ml-pyåº“è·å–
                power_usage=0.0,  # éœ€è¦nvidia-ml-pyåº“è·å–
                is_available=True
            )
            
        except Exception as e:
            logger.error(f"è·å–GPUçŠ¶æ€å¤±è´¥: {e}")
            return None
    
    def get_cpu_status(self) -> CPUStatus:
        """è·å–CPUçŠ¶æ€"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # CPUé¢‘ç‡
            cpu_freq = psutil.cpu_freq()
            current_freq = cpu_freq.current if cpu_freq else 0.0
            
            # è´Ÿè½½å¹³å‡å€¼
            load_avg = os.getloadavg() if hasattr(os, 'getloadavg') else [0.0, 0.0, 0.0]
            
            return CPUStatus(
                core_count=self.cpu_count,
                thread_count=self.cpu_threads,
                usage_percent=cpu_percent,
                frequency=current_freq,
                temperature=0.0,  # éœ€è¦é¢å¤–åº“è·å–
                load_average=list(load_avg)
            )
            
        except Exception as e:
            logger.error(f"è·å–CPUçŠ¶æ€å¤±è´¥: {e}")
            return CPUStatus(
                core_count=self.cpu_count,
                thread_count=self.cpu_threads,
                usage_percent=0.0,
                frequency=0.0,
                temperature=0.0,
                load_average=[0.0, 0.0, 0.0]
            )
    
    def get_memory_status(self) -> Dict[str, float]:
        """è·å–å†…å­˜çŠ¶æ€"""
        try:
            memory = psutil.virtual_memory()
            swap = psutil.swap_memory()
            
            return {
                'total_gb': memory.total / (1024**3),
                'used_gb': memory.used / (1024**3),
                'available_gb': memory.available / (1024**3),
                'usage_percent': memory.percent,
                'swap_total_gb': swap.total / (1024**3),
                'swap_used_gb': swap.used / (1024**3),
                'swap_percent': swap.percent
            }
            
        except Exception as e:
            logger.error(f"è·å–å†…å­˜çŠ¶æ€å¤±è´¥: {e}")
            return {}
    
    def get_disk_status(self) -> Dict[str, float]:
        """è·å–ç£ç›˜çŠ¶æ€"""
        try:
            disk = psutil.disk_usage('/')
            
            return {
                'total_gb': disk.total / (1024**3),
                'used_gb': disk.used / (1024**3),
                'free_gb': disk.free / (1024**3),
                'usage_percent': (disk.used / disk.total) * 100
            }
            
        except Exception as e:
            logger.error(f"è·å–ç£ç›˜çŠ¶æ€å¤±è´¥: {e}")
            return {}
    
    def get_network_status(self) -> Dict[str, float]:
        """è·å–ç½‘ç»œçŠ¶æ€"""
        try:
            net_io = psutil.net_io_counters()
            
            return {
                'bytes_sent_mb': net_io.bytes_sent / (1024**2),
                'bytes_recv_mb': net_io.bytes_recv / (1024**2),
                'packets_sent': net_io.packets_sent,
                'packets_recv': net_io.packets_recv,
                'errors_in': net_io.errin,
                'errors_out': net_io.errout
            }
            
        except Exception as e:
            logger.error(f"è·å–ç½‘ç»œçŠ¶æ€å¤±è´¥: {e}")
            return {}
    
    def get_performance_metrics(self) -> PerformanceMetrics:
        """è·å–å®Œæ•´æ€§èƒ½æŒ‡æ ‡"""
        return PerformanceMetrics(
            gpu_status=self.get_gpu_status(),
            cpu_status=self.get_cpu_status(),
            memory_usage=self.get_memory_status(),
            disk_usage=self.get_disk_status(),
            network_usage=self.get_network_status(),
            timestamp=datetime.now()
        )
    
    def optimize_gpu_memory(self):
        """ä¼˜åŒ–GPUå†…å­˜"""
        if not self.gpu_available:
            return
        
        try:
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
            
            # è·å–å†…å­˜çŠ¶æ€
            gpu_status = self.get_gpu_status()
            if gpu_status:
                memory_usage = (gpu_status.used_memory / gpu_status.total_memory) * 100
                
                if memory_usage > self.max_memory_usage:
                    logger.warning(f"GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_usage:.1f}%ï¼Œæ‰§è¡Œæ¸…ç†")
                    
                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    import gc
                    gc.collect()
                    torch.cuda.empty_cache()
                    
                    # é‡æ–°æ£€æŸ¥
                    gpu_status_after = self.get_gpu_status()
                    if gpu_status_after:
                        new_usage = (gpu_status_after.used_memory / gpu_status_after.total_memory) * 100
                        logger.info(f"GPUå†…å­˜æ¸…ç†å®Œæˆ: {memory_usage:.1f}% -> {new_usage:.1f}%")
            
        except Exception as e:
            logger.error(f"GPUå†…å­˜ä¼˜åŒ–å¤±è´¥: {e}")
    
    def optimize_cpu_performance(self):
        """ä¼˜åŒ–CPUæ€§èƒ½"""
        try:
            cpu_status = self.get_cpu_status()
            
            # å¦‚æœCPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œè°ƒæ•´çº¿ç¨‹æ•°
            if cpu_status.usage_percent > 90.0:
                current_threads = torch.get_num_threads()
                new_threads = max(1, current_threads - 2)
                torch.set_num_threads(new_threads)
                
                logger.warning(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_status.usage_percent:.1f}%ï¼Œ"
                             f"è°ƒæ•´çº¿ç¨‹æ•°: {current_threads} -> {new_threads}")
            
            elif cpu_status.usage_percent < 50.0:
                current_threads = torch.get_num_threads()
                max_threads = min(self.cpu_threads, 32)
                new_threads = min(max_threads, current_threads + 2)
                
                if new_threads > current_threads:
                    torch.set_num_threads(new_threads)
                    logger.info(f"CPUä½¿ç”¨ç‡è¾ƒä½: {cpu_status.usage_percent:.1f}%ï¼Œ"
                               f"å¢åŠ çº¿ç¨‹æ•°: {current_threads} -> {new_threads}")
            
        except Exception as e:
            logger.error(f"CPUæ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
    
    def _performance_monitor_loop(self):
        """æ€§èƒ½ç›‘æ§å¾ªç¯"""
        while self.is_running:
            try:
                # è·å–æ€§èƒ½æŒ‡æ ‡
                metrics = self.get_performance_metrics()
                
                # è®°å½•å†å²
                with self.lock:
                    self.performance_history.append(metrics)
                    
                    # é™åˆ¶å†å²è®°å½•æ•°é‡
                    if len(self.performance_history) > 1000:
                        self.performance_history = self.performance_history[-1000:]
                
                # æ£€æŸ¥æ€§èƒ½è­¦å‘Š
                self._check_performance_warnings(metrics)
                
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")
                time.sleep(self.monitoring_interval)
    
    def _optimization_loop(self):
        """ä¼˜åŒ–è°ƒåº¦å¾ªç¯"""
        while self.is_running:
            try:
                time.sleep(self.optimization_interval)
                
                # æ‰§è¡Œä¼˜åŒ–
                self.optimize_gpu_memory()
                self.optimize_cpu_performance()
                
                logger.debug("æ€§èƒ½ä¼˜åŒ–è°ƒåº¦å®Œæˆ")
                
            except Exception as e:
                logger.error(f"æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
    
    def _check_performance_warnings(self, metrics: PerformanceMetrics):
        """æ£€æŸ¥æ€§èƒ½è­¦å‘Š"""
        warnings = []
        
        # GPUè­¦å‘Š
        if metrics.gpu_status:
            gpu = metrics.gpu_status
            memory_usage = (gpu.used_memory / gpu.total_memory) * 100
            
            if memory_usage > 95.0:
                warnings.append(f"GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_usage:.1f}%")
            
            if gpu.utilization > 98.0:
                warnings.append(f"GPUåˆ©ç”¨ç‡è¿‡é«˜: {gpu.utilization:.1f}%")
        
        # CPUè­¦å‘Š
        cpu = metrics.cpu_status
        if cpu.usage_percent > 95.0:
            warnings.append(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu.usage_percent:.1f}%")
        
        # å†…å­˜è­¦å‘Š
        memory = metrics.memory_usage
        if memory.get('usage_percent', 0) > 90.0:
            warnings.append(f"ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory['usage_percent']:.1f}%")
        
        # ç£ç›˜è­¦å‘Š
        disk = metrics.disk_usage
        if disk.get('usage_percent', 0) > 90.0:
            warnings.append(f"ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {disk['usage_percent']:.1f}%")
        
        # è¾“å‡ºè­¦å‘Š
        for warning in warnings:
            logger.warning(f"âš ï¸ æ€§èƒ½è­¦å‘Š: {warning}")
    
    def get_optimization_report(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æŠ¥å‘Š"""
        with self.lock:
            if not self.performance_history:
                return {"error": "æš‚æ— æ€§èƒ½æ•°æ®"}
            
            latest = self.performance_history[-1]
            
            report = {
                "timestamp": latest.timestamp.isoformat(),
                "gpu_info": {
                    "available": self.gpu_available,
                    "devices": len(self.gpu_devices),
                    "current_status": {
                        "name": latest.gpu_status.name if latest.gpu_status else "N/A",
                        "memory_usage_percent": (
                            (latest.gpu_status.used_memory / latest.gpu_status.total_memory) * 100
                            if latest.gpu_status else 0
                        ),
                        "utilization": latest.gpu_status.utilization if latest.gpu_status else 0
                    } if latest.gpu_status else None
                },
                "cpu_info": {
                    "cores": latest.cpu_status.core_count,
                    "threads": latest.cpu_status.thread_count,
                    "usage_percent": latest.cpu_status.usage_percent,
                    "frequency_mhz": latest.cpu_status.frequency,
                    "load_average": latest.cpu_status.load_average
                },
                "memory_info": latest.memory_usage,
                "disk_info": latest.disk_usage,
                "optimization_status": {
                    "gpu_memory_optimized": self.gpu_available,
                    "cpu_threads_optimized": True,
                    "monitoring_active": self.is_running
                }
            }
            
            return report
    
    def shutdown(self):
        """å…³é—­ä¼˜åŒ–å™¨"""
        logger.info("æ­£åœ¨å…³é—­GPUæ€§èƒ½ä¼˜åŒ–å™¨...")
        self.is_running = False
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        time.sleep(2)
        
        logger.info("GPUæ€§èƒ½ä¼˜åŒ–å™¨å·²å…³é—­")


# å…¨å±€å®ä¾‹
_optimizer = None

def get_gpu_optimizer(config: Dict[str, Any] = None) -> GPUPerformanceOptimizer:
    """è·å–GPUä¼˜åŒ–å™¨å®ä¾‹"""
    global _optimizer
    if _optimizer is None:
        _optimizer = GPUPerformanceOptimizer(config)
    return _optimizer


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    def test_gpu_optimizer():
        """æµ‹è¯•GPUä¼˜åŒ–å™¨"""
        optimizer = get_gpu_optimizer()
        
        # è·å–æ€§èƒ½æŠ¥å‘Š
        report = optimizer.get_optimization_report()
        print("æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š:")
        import json
        print(json.dumps(report, indent=2, ensure_ascii=False))
        
        # è¿è¡Œä¸€æ®µæ—¶é—´
        time.sleep(10)
        
        # å†æ¬¡è·å–æŠ¥å‘Š
        report = optimizer.get_optimization_report()
        print("\n10ç§’åçš„æ€§èƒ½æŠ¥å‘Š:")
        print(json.dumps(report, indent=2, ensure_ascii=False))
        
        optimizer.shutdown()
    
    test_gpu_optimizer()
