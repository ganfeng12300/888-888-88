"""
ğŸ§  æ·±åº¦å¼ºåŒ–å­¦ä¹ å¢å¼ºæ¨¡å— - ç”Ÿäº§çº§å®ç›˜äº¤æ˜“å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ
åŸºäºPPOã€SACã€TD3ç­‰å…ˆè¿›ç®—æ³•çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ äº¤æ˜“æ™ºèƒ½ä½“
æ”¯æŒå¤šèµ„äº§ã€å¤šæ—¶é—´æ¡†æ¶ã€åŠ¨æ€ç¯å¢ƒé€‚åº”çš„æ™ºèƒ½äº¤æ˜“å†³ç­–
"""
import asyncio
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Normal
import threading
import time
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

from loguru import logger

class RLAlgorithm(Enum):
    """å¼ºåŒ–å­¦ä¹ ç®—æ³•ç±»å‹"""
    PPO = "ppo"  # Proximal Policy Optimization
    SAC = "sac"  # Soft Actor-Critic
    TD3 = "td3"  # Twin Delayed Deep Deterministic Policy Gradient
    A2C = "a2c"  # Advantage Actor-Critic
    DDPG = "ddpg"  # Deep Deterministic Policy Gradient

class ActionType(Enum):
    """åŠ¨ä½œç±»å‹"""
    HOLD = 0  # æŒæœ‰
    BUY = 1   # ä¹°å…¥
    SELL = 2  # å–å‡º

@dataclass
class TradingState:
    """äº¤æ˜“çŠ¶æ€"""
    price: float  # å½“å‰ä»·æ ¼
    volume: float  # æˆäº¤é‡
    position: float  # å½“å‰ä»“ä½ (-1åˆ°1)
    cash: float  # ç°é‡‘ä½™é¢
    portfolio_value: float  # ç»„åˆä»·å€¼
    technical_indicators: np.ndarray  # æŠ€æœ¯æŒ‡æ ‡
    market_features: np.ndarray  # å¸‚åœºç‰¹å¾
    timestamp: float  # æ—¶é—´æˆ³

@dataclass
class TradingAction:
    """äº¤æ˜“åŠ¨ä½œ"""
    action_type: ActionType  # åŠ¨ä½œç±»å‹
    position_size: float  # ä»“ä½å¤§å° (0-1)
    confidence: float  # ç½®ä¿¡åº¦
    stop_loss: Optional[float] = None  # æ­¢æŸä»·æ ¼
    take_profit: Optional[float] = None  # æ­¢ç›ˆä»·æ ¼

@dataclass
class Experience:
    """ç»éªŒå›æ”¾æ•°æ®"""
    state: np.ndarray  # çŠ¶æ€
    action: np.ndarray  # åŠ¨ä½œ
    reward: float  # å¥–åŠ±
    next_state: np.ndarray  # ä¸‹ä¸€çŠ¶æ€
    done: bool  # æ˜¯å¦ç»“æŸ
    info: Dict[str, Any] = field(default_factory=dict)  # é¢å¤–ä¿¡æ¯

class ActorNetwork(nn.Module):
    """Actorç½‘ç»œ - ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(ActorNetwork, self).__init__()
        
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        
        # åŠ¨ä½œå‡å€¼å’Œæ ‡å‡†å·®
        self.action_mean = nn.Linear(hidden_dim, action_dim)
        self.action_std = nn.Linear(hidden_dim, action_dim)
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight)
            torch.nn.init.constant_(m.bias, 0)
    
    def forward(self, state):
        """å‰å‘ä¼ æ’­"""
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        
        action_mean = torch.tanh(self.action_mean(x))  # é™åˆ¶åœ¨[-1, 1]
        action_std = F.softplus(self.action_std(x)) + 1e-5  # ç¡®ä¿æ­£æ•°
        
        return action_mean, action_std

class CriticNetwork(nn.Module):
    """Criticç½‘ç»œ - ä»·å€¼ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(CriticNetwork, self).__init__()
        
        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.value = nn.Linear(hidden_dim, 1)
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight)
            torch.nn.init.constant_(m.bias, 0)
    
    def forward(self, state, action):
        """å‰å‘ä¼ æ’­"""
        x = torch.cat([state, action], dim=-1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        value = self.value(x)
        
        return value

class ExperienceReplay:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int = 100000):
        self.capacity = capacity
        self.buffer = []
        self.position = 0
        self.lock = threading.RLock()
    
    def push(self, experience: Experience):
        """æ·»åŠ ç»éªŒ"""
        with self.lock:
            if len(self.buffer) < self.capacity:
                self.buffer.append(experience)
            else:
                self.buffer[self.position] = experience
            
            self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size: int) -> List[Experience]:
        """é‡‡æ ·ç»éªŒ"""
        with self.lock:
            if len(self.buffer) < batch_size:
                return self.buffer.copy()
            
            indices = np.random.choice(len(self.buffer), batch_size, replace=False)
            return [self.buffer[i] for i in indices]
    
    def __len__(self):
        return len(self.buffer)

class PPOAgent:
    """PPOæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, lr: float = 3e-4,
                 gamma: float = 0.99, eps_clip: float = 0.2, k_epochs: int = 4):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.eps_clip = eps_clip
        self.k_epochs = k_epochs
        
        # ç½‘ç»œ
        self.actor = ActorNetwork(state_dim, action_dim)
        self.critic = CriticNetwork(state_dim, action_dim)
        self.actor_old = ActorNetwork(state_dim, action_dim)
        
        # å¤åˆ¶å‚æ•°åˆ°æ—§ç½‘ç»œ
        self.actor_old.load_state_dict(self.actor.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)
        
        # ç»éªŒç¼“å†²
        self.memory = []
        
        logger.info("PPOæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def select_action(self, state: np.ndarray, training: bool = True) -> Tuple[np.ndarray, float]:
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            action_mean, action_std = self.actor_old(state_tensor)
            
            if training:
                # è®­ç»ƒæ—¶ä½¿ç”¨éšæœºç­–ç•¥
                dist = Normal(action_mean, action_std)
                action = dist.sample()
                action_logprob = dist.log_prob(action).sum(dim=-1)
            else:
                # æµ‹è¯•æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
                action = action_mean
                action_logprob = torch.zeros(1)
        
        return action.squeeze(0).numpy(), action_logprob.item()
    
    def store_transition(self, state, action, reward, next_state, done, logprob):
        """å­˜å‚¨è½¬æ¢"""
        self.memory.append({
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state,
            'done': done,
            'logprob': logprob
        })
    
    def update(self):
        """æ›´æ–°ç½‘ç»œ"""
        if len(self.memory) == 0:
            return
        
        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.FloatTensor([m['state'] for m in self.memory])
        actions = torch.FloatTensor([m['action'] for m in self.memory])
        rewards = torch.FloatTensor([m['reward'] for m in self.memory])
        next_states = torch.FloatTensor([m['next_state'] for m in self.memory])
        dones = torch.BoolTensor([m['done'] for m in self.memory])
        old_logprobs = torch.FloatTensor([m['logprob'] for m in self.memory])
        
        # è®¡ç®—æŠ˜æ‰£å¥–åŠ±
        discounted_rewards = []
        discounted_reward = 0
        for reward, done in zip(reversed(rewards), reversed(dones)):
            if done:
                discounted_reward = 0
            discounted_reward = reward + self.gamma * discounted_reward
            discounted_rewards.insert(0, discounted_reward)
        
        discounted_rewards = torch.FloatTensor(discounted_rewards)
        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)
        
        # æ›´æ–°ç½‘ç»œ
        for _ in range(self.k_epochs):
            # è®¡ç®—å½“å‰ç­–ç•¥çš„åŠ¨ä½œæ¦‚ç‡
            action_mean, action_std = self.actor(states)
            dist = Normal(action_mean, action_std)
            new_logprobs = dist.log_prob(actions).sum(dim=-1)
            
            # è®¡ç®—æ¯”ç‡
            ratio = torch.exp(new_logprobs - old_logprobs)
            
            # è®¡ç®—ä¼˜åŠ¿
            values = self.critic(states, actions).squeeze()
            advantages = discounted_rewards - values.detach()
            
            # PPOæŸå¤±
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            
            # CriticæŸå¤±
            critic_loss = F.mse_loss(values, discounted_rewards)
            
            # æ›´æ–°Actor
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()
            
            # æ›´æ–°Critic
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            self.critic_optimizer.step()
        
        # æ›´æ–°æ—§ç­–ç•¥
        self.actor_old.load_state_dict(self.actor.state_dict())
        
        # æ¸…ç©ºå†…å­˜
        self.memory.clear()
        
        logger.debug(f"PPOæ›´æ–°å®Œæˆ - Actor Loss: {actor_loss.item():.4f}, Critic Loss: {critic_loss.item():.4f}")

class SACAgent:
    """SACæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, lr: float = 3e-4,
                 gamma: float = 0.99, tau: float = 0.005, alpha: float = 0.2):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.tau = tau
        self.alpha = alpha
        
        # ç½‘ç»œ
        self.actor = ActorNetwork(state_dim, action_dim)
        self.critic1 = CriticNetwork(state_dim, action_dim)
        self.critic2 = CriticNetwork(state_dim, action_dim)
        self.target_critic1 = CriticNetwork(state_dim, action_dim)
        self.target_critic2 = CriticNetwork(state_dim, action_dim)
        
        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=lr)
        
        # ç»éªŒå›æ”¾
        self.replay_buffer = ExperienceReplay()
        
        logger.info("SACæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def select_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            action_mean, action_std = self.actor(state_tensor)
            
            if training:
                # è®­ç»ƒæ—¶ä½¿ç”¨éšæœºç­–ç•¥
                dist = Normal(action_mean, action_std)
                action = dist.sample()
            else:
                # æµ‹è¯•æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
                action = action_mean
        
        return action.squeeze(0).numpy()
    
    def store_transition(self, state, action, reward, next_state, done):
        """å­˜å‚¨è½¬æ¢"""
        experience = Experience(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            done=done
        )
        self.replay_buffer.push(experience)
    
    def update(self, batch_size: int = 256):
        """æ›´æ–°ç½‘ç»œ"""
        if len(self.replay_buffer) < batch_size:
            return
        
        # é‡‡æ ·ç»éªŒ
        experiences = self.replay_buffer.sample(batch_size)
        
        states = torch.FloatTensor([e.state for e in experiences])
        actions = torch.FloatTensor([e.action for e in experiences])
        rewards = torch.FloatTensor([e.reward for e in experiences])
        next_states = torch.FloatTensor([e.next_state for e in experiences])
        dones = torch.BoolTensor([e.done for e in experiences])
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_action_mean, next_action_std = self.actor(next_states)
            next_dist = Normal(next_action_mean, next_action_std)
            next_actions = next_dist.sample()
            next_log_probs = next_dist.log_prob(next_actions).sum(dim=-1, keepdim=True)
            
            target_q1 = self.target_critic1(next_states, next_actions)
            target_q2 = self.target_critic2(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs
            
            target_q = rewards.unsqueeze(1) + self.gamma * target_q * (~dones).unsqueeze(1)
        
        # æ›´æ–°Critic
        current_q1 = self.critic1(states, actions)
        current_q2 = self.critic2(states, actions)
        
        critic1_loss = F.mse_loss(current_q1, target_q)
        critic2_loss = F.mse_loss(current_q2, target_q)
        
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        self.critic1_optimizer.step()
        
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        self.critic2_optimizer.step()
        
        # æ›´æ–°Actor
        action_mean, action_std = self.actor(states)
        dist = Normal(action_mean, action_std)
        new_actions = dist.rsample()  # é‡å‚æ•°åŒ–é‡‡æ ·
        log_probs = dist.log_prob(new_actions).sum(dim=-1, keepdim=True)
        
        q1_new = self.critic1(states, new_actions)
        q2_new = self.critic2(states, new_actions)
        q_new = torch.min(q1_new, q2_new)
        
        actor_loss = (self.alpha * log_probs - q_new).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self._soft_update(self.target_critic1, self.critic1)
        self._soft_update(self.target_critic2, self.critic2)
        
        logger.debug(f"SACæ›´æ–°å®Œæˆ - Actor Loss: {actor_loss.item():.4f}, Critic Loss: {(critic1_loss + critic2_loss).item():.4f}")
    
    def _soft_update(self, target_net, net):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(target_net.parameters(), net.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

class TradingEnvironment:
    """äº¤æ˜“ç¯å¢ƒ"""
    
    def __init__(self, data: pd.DataFrame, initial_balance: float = 100000,
                 transaction_cost: float = 0.001, max_position: float = 1.0):
        self.data = data
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost
        self.max_position = max_position
        
        # çŠ¶æ€å˜é‡
        self.current_step = 0
        self.balance = initial_balance
        self.position = 0.0
        self.portfolio_value = initial_balance
        self.total_trades = 0
        self.winning_trades = 0
        
        # ç‰¹å¾ç»´åº¦
        self.state_dim = self._calculate_state_dim()
        self.action_dim = 3  # hold, buy, sell
        
        logger.info(f"äº¤æ˜“ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - æ•°æ®é•¿åº¦: {len(data)}, çŠ¶æ€ç»´åº¦: {self.state_dim}")
    
    def _calculate_state_dim(self) -> int:
        """è®¡ç®—çŠ¶æ€ç»´åº¦"""
        # åŸºç¡€çŠ¶æ€: ä»·æ ¼ã€æˆäº¤é‡ã€ä»“ä½ã€ç°é‡‘æ¯”ä¾‹
        base_dim = 4
        
        # æŠ€æœ¯æŒ‡æ ‡ç»´åº¦ (å‡è®¾æœ‰20ä¸ªæŠ€æœ¯æŒ‡æ ‡)
        technical_dim = 20
        
        # å¸‚åœºç‰¹å¾ç»´åº¦ (å‡è®¾æœ‰10ä¸ªå¸‚åœºç‰¹å¾)
        market_dim = 10
        
        return base_dim + technical_dim + market_dim
    
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 0
        self.balance = self.initial_balance
        self.position = 0.0
        self.portfolio_value = self.initial_balance
        self.total_trades = 0
        self.winning_trades = 0
        
        return self._get_state()
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡ŒåŠ¨ä½œ"""
        if self.current_step >= len(self.data) - 1:
            return self._get_state(), 0, True, {}
        
        # è§£æåŠ¨ä½œ
        action_type = np.argmax(action[:3])  # 0: hold, 1: buy, 2: sell
        position_size = np.clip(action[3] if len(action) > 3 else 0.5, 0, 1)
        
        # è·å–å½“å‰ä»·æ ¼
        current_price = self.data.iloc[self.current_step]['close']
        next_price = self.data.iloc[self.current_step + 1]['close']
        
        # æ‰§è¡Œäº¤æ˜“
        reward = self._execute_trade(action_type, position_size, current_price, next_price)
        
        # æ›´æ–°çŠ¶æ€
        self.current_step += 1
        next_state = self._get_state()
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = self.current_step >= len(self.data) - 1
        
        # é¢å¤–ä¿¡æ¯
        info = {
            'portfolio_value': self.portfolio_value,
            'position': self.position,
            'balance': self.balance,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(self.total_trades, 1)
        }
        
        return next_state, reward, done, info
    
    def _execute_trade(self, action_type: int, position_size: float, 
                      current_price: float, next_price: float) -> float:
        """æ‰§è¡Œäº¤æ˜“"""
        old_portfolio_value = self.portfolio_value
        
        if action_type == 1:  # Buy
            # è®¡ç®—å¯ä¹°å…¥æ•°é‡
            available_cash = self.balance * position_size
            shares_to_buy = available_cash / current_price
            cost = shares_to_buy * current_price * (1 + self.transaction_cost)
            
            if cost <= self.balance:
                self.balance -= cost
                self.position += shares_to_buy
                self.total_trades += 1
        
        elif action_type == 2:  # Sell
            # è®¡ç®—å¯å–å‡ºæ•°é‡
            shares_to_sell = self.position * position_size
            
            if shares_to_sell > 0:
                proceeds = shares_to_sell * current_price * (1 - self.transaction_cost)
                self.balance += proceeds
                self.position -= shares_to_sell
                self.total_trades += 1
        
        # æ›´æ–°ç»„åˆä»·å€¼
        self.portfolio_value = self.balance + self.position * next_price
        
        # è®¡ç®—å¥–åŠ±
        reward = (self.portfolio_value - old_portfolio_value) / old_portfolio_value
        
        # æ›´æ–°èƒœç‡
        if reward > 0:
            self.winning_trades += 1
        
        return reward
    
    def _get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€"""
        if self.current_step >= len(self.data):
            return np.zeros(self.state_dim)
        
        row = self.data.iloc[self.current_step]
        
        # åŸºç¡€çŠ¶æ€
        price = row['close']
        volume = row['volume']
        position_ratio = self.position * price / self.portfolio_value if self.portfolio_value > 0 else 0
        cash_ratio = self.balance / self.portfolio_value if self.portfolio_value > 0 else 0
        
        base_state = np.array([price, volume, position_ratio, cash_ratio])
        
        # æŠ€æœ¯æŒ‡æ ‡ (æ¨¡æ‹Ÿæ•°æ®)
        technical_indicators = np.random.randn(20) * 0.1
        
        # å¸‚åœºç‰¹å¾ (æ¨¡æ‹Ÿæ•°æ®)
        market_features = np.random.randn(10) * 0.1
        
        # ç»„åˆçŠ¶æ€
        state = np.concatenate([base_state, technical_indicators, market_features])
        
        return state.astype(np.float32)

class RLTradingSystem:
    """å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç³»ç»Ÿ"""
    
    def __init__(self, algorithm: RLAlgorithm = RLAlgorithm.PPO):
        self.algorithm = algorithm
        self.agent = None
        self.environment = None
        self.training_data = None
        self.testing_data = None
        
        # è®­ç»ƒå‚æ•°
        self.episodes = 1000
        self.max_steps_per_episode = 1000
        self.update_frequency = 10
        
        # æ€§èƒ½æŒ‡æ ‡
        self.training_rewards = []
        self.training_portfolio_values = []
        self.testing_results = {}
        
        logger.info(f"å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ - ç®—æ³•: {algorithm.value}")
    
    def setup_environment(self, data: pd.DataFrame, train_ratio: float = 0.8):
        """è®¾ç½®ç¯å¢ƒ"""
        # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        split_idx = int(len(data) * train_ratio)
        self.training_data = data.iloc[:split_idx].copy()
        self.testing_data = data.iloc[split_idx:].copy()
        
        # åˆ›å»ºç¯å¢ƒ
        self.environment = TradingEnvironment(self.training_data)
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        if self.algorithm == RLAlgorithm.PPO:
            self.agent = PPOAgent(
                state_dim=self.environment.state_dim,
                action_dim=4  # 3ä¸ªåŠ¨ä½œç±»å‹ + 1ä¸ªä»“ä½å¤§å°
            )
        elif self.algorithm == RLAlgorithm.SAC:
            self.agent = SACAgent(
                state_dim=self.environment.state_dim,
                action_dim=4
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {self.algorithm}")
        
        logger.info(f"ç¯å¢ƒè®¾ç½®å®Œæˆ - è®­ç»ƒæ•°æ®: {len(self.training_data)}, æµ‹è¯•æ•°æ®: {len(self.testing_data)}")
    
    def train(self, episodes: int = None) -> Dict[str, Any]:
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        if not self.agent or not self.environment:
            raise ValueError("è¯·å…ˆè®¾ç½®ç¯å¢ƒ")
        
        episodes = episodes or self.episodes
        
        logger.info(f"å¼€å§‹è®­ç»ƒ - ç®—æ³•: {self.algorithm.value}, å›åˆæ•°: {episodes}")
        
        for episode in range(episodes):
            state = self.environment.reset()
            episode_reward = 0
            episode_steps = 0
            
            for step in range(self.max_steps_per_episode):
                # é€‰æ‹©åŠ¨ä½œ
                if self.algorithm == RLAlgorithm.PPO:
                    action, logprob = self.agent.select_action(state, training=True)
                else:
                    action = self.agent.select_action(state, training=True)
                    logprob = 0
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = self.environment.step(action)
                
                # å­˜å‚¨ç»éªŒ
                if self.algorithm == RLAlgorithm.PPO:
                    self.agent.store_transition(state, action, reward, next_state, done, logprob)
                else:
                    self.agent.store_transition(state, action, reward, next_state, done)
                
                state = next_state
                episode_reward += reward
                episode_steps += 1
                
                if done:
                    break
            
            # æ›´æ–°æ™ºèƒ½ä½“
            if episode % self.update_frequency == 0:
                self.agent.update()
            
            # è®°å½•æ€§èƒ½
            self.training_rewards.append(episode_reward)
            self.training_portfolio_values.append(info.get('portfolio_value', 0))
            
            # æ‰“å°è¿›åº¦
            if episode % 100 == 0:
                avg_reward = np.mean(self.training_rewards[-100:])
                logger.info(f"Episode {episode}, å¹³å‡å¥–åŠ±: {avg_reward:.4f}, ç»„åˆä»·å€¼: {info.get('portfolio_value', 0):.2f}")
        
        logger.info("è®­ç»ƒå®Œæˆ")
        
        return {
            'training_rewards': self.training_rewards,
            'training_portfolio_values': self.training_portfolio_values,
            'final_portfolio_value': self.training_portfolio_values[-1] if self.training_portfolio_values else 0
        }
    
    def test(self) -> Dict[str, Any]:
        """æµ‹è¯•æ™ºèƒ½ä½“"""
        if not self.agent or not self.testing_data is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ™ºèƒ½ä½“å¹¶å‡†å¤‡æµ‹è¯•æ•°æ®")
        
        logger.info("å¼€å§‹æµ‹è¯•")
        
        # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
        test_env = TradingEnvironment(self.testing_data)
        
        state = test_env.reset()
        total_reward = 0
        portfolio_values = []
        actions_taken = []
        
        for step in range(len(self.testing_data) - 1):
            # é€‰æ‹©åŠ¨ä½œ (æµ‹è¯•æ¨¡å¼)
            action = self.agent.select_action(state, training=False)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = test_env.step(action)
            
            # è®°å½•ç»“æœ
            total_reward += reward
            portfolio_values.append(info['portfolio_value'])
            actions_taken.append(np.argmax(action[:3]))
            
            state = next_state
            
            if done:
                break
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        initial_value = test_env.initial_balance
        final_value = portfolio_values[-1] if portfolio_values else initial_value
        total_return = (final_value - initial_value) / initial_value
        
        # è®¡ç®—å¤æ™®æ¯”ç‡
        returns = np.diff(portfolio_values) / portfolio_values[:-1]
        sharpe_ratio = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0
        
        # è®¡ç®—æœ€å¤§å›æ’¤
        peak = np.maximum.accumulate(portfolio_values)
        drawdown = (peak - portfolio_values) / peak
        max_drawdown = np.max(drawdown)
        
        self.testing_results = {
            'total_return': total_return,
            'final_portfolio_value': final_value,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'total_trades': test_env.total_trades,
            'win_rate': test_env.winning_trades / max(test_env.total_trades, 1),
            'portfolio_values': portfolio_values,
            'actions_taken': actions_taken
        }
        
        logger.info(f"æµ‹è¯•å®Œæˆ - æ€»æ”¶ç›Š: {total_return:.2%}, å¤æ™®æ¯”ç‡: {sharpe_ratio:.4f}, æœ€å¤§å›æ’¤: {max_drawdown:.2%}")
        
        return self.testing_results
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        if not self.agent:
            raise ValueError("æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡å‹")
        
        torch.save({
            'algorithm': self.algorithm.value,
            'actor_state_dict': self.agent.actor.state_dict(),
            'critic_state_dict': getattr(self.agent, 'critic', None).state_dict() if hasattr(self.agent, 'critic') else None,
            'training_rewards': self.training_rewards,
            'testing_results': self.testing_results
        }, filepath)
        
        logger.info(f"æ¨¡å‹ä¿å­˜å®Œæˆ: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath)
        
        # é‡æ–°åˆ›å»ºæ™ºèƒ½ä½“ (éœ€è¦å…ˆè®¾ç½®ç¯å¢ƒ)
        if not self.environment:
            raise ValueError("è¯·å…ˆè®¾ç½®ç¯å¢ƒ")
        
        self.algorithm = RLAlgorithm(checkpoint['algorithm'])
        
        if self.algorithm == RLAlgorithm.PPO:
            self.agent = PPOAgent(
                state_dim=self.environment.state_dim,
                action_dim=4
            )
        elif self.algorithm == RLAlgorithm.SAC:
            self.agent = SACAgent(
                state_dim=self.environment.state_dim,
                action_dim=4
            )
        
        # åŠ è½½æƒé‡
        self.agent.actor.load_state_dict(checkpoint['actor_state_dict'])
        if checkpoint['critic_state_dict'] and hasattr(self.agent, 'critic'):
            self.agent.critic.load_state_dict(checkpoint['critic_state_dict'])
        
        # åŠ è½½å†å²æ•°æ®
        self.training_rewards = checkpoint.get('training_rewards', [])
        self.testing_results = checkpoint.get('testing_results', {})
        
        logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆ: {filepath}")

# å…¨å±€å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç³»ç»Ÿå®ä¾‹
rl_trading_system = RLTradingSystem()
