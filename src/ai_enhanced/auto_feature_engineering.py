"""
ğŸ”§ è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹æ¨¡å— - ç”Ÿäº§çº§å®ç›˜äº¤æ˜“ç‰¹å¾è‡ªåŠ¨ç”Ÿæˆç³»ç»Ÿ
åŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½ç‰¹å¾å·¥ç¨‹ï¼Œè‡ªåŠ¨å‘ç°å’Œç”Ÿæˆæœ‰æ•ˆçš„äº¤æ˜“ç‰¹å¾
æ”¯æŒæŠ€æœ¯æŒ‡æ ‡ã€ç»Ÿè®¡ç‰¹å¾ã€æ—¶é—´åºåˆ—ç‰¹å¾ã€äº¤å‰ç‰¹å¾çš„è‡ªåŠ¨ç”Ÿæˆå’Œé€‰æ‹©
"""
import asyncio
import numpy as np
import pandas as pd
import time
import threading
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

try:
    from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
    from sklearn.decomposition import PCA, FastICA
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import cross_val_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("Scikit-learn not available, some feature engineering capabilities will be limited")

try:
    import talib
    TALIB_AVAILABLE = True
except ImportError:
    TALIB_AVAILABLE = False
    print("TA-Lib not available, using custom technical indicators")

from loguru import logger

class FeatureType(Enum):
    """ç‰¹å¾ç±»å‹"""
    TECHNICAL = "technical"  # æŠ€æœ¯æŒ‡æ ‡
    STATISTICAL = "statistical"  # ç»Ÿè®¡ç‰¹å¾
    TIME_SERIES = "time_series"  # æ—¶é—´åºåˆ—ç‰¹å¾
    CROSS = "cross"  # äº¤å‰ç‰¹å¾
    DERIVED = "derived"  # è¡ç”Ÿç‰¹å¾
    TRANSFORMED = "transformed"  # å˜æ¢ç‰¹å¾

class ScalingMethod(Enum):
    """ç¼©æ”¾æ–¹æ³•"""
    STANDARD = "standard"  # æ ‡å‡†åŒ–
    MINMAX = "minmax"  # æœ€å°æœ€å¤§ç¼©æ”¾
    ROBUST = "robust"  # é²æ£’ç¼©æ”¾
    NONE = "none"  # ä¸ç¼©æ”¾

@dataclass
class FeatureInfo:
    """ç‰¹å¾ä¿¡æ¯"""
    name: str  # ç‰¹å¾åç§°
    feature_type: FeatureType  # ç‰¹å¾ç±»å‹
    importance: float  # ç‰¹å¾é‡è¦æ€§
    correlation: float  # ä¸ç›®æ ‡çš„ç›¸å…³æ€§
    stability: float  # ç‰¹å¾ç¨³å®šæ€§
    description: str  # ç‰¹å¾æè¿°
    parameters: Dict[str, Any] = field(default_factory=dict)  # ç‰¹å¾å‚æ•°
    created_at: float = field(default_factory=time.time)  # åˆ›å»ºæ—¶é—´

@dataclass
class FeatureSet:
    """ç‰¹å¾é›†åˆ"""
    features: pd.DataFrame  # ç‰¹å¾æ•°æ®
    feature_info: Dict[str, FeatureInfo]  # ç‰¹å¾ä¿¡æ¯
    target: Optional[pd.Series] = None  # ç›®æ ‡å˜é‡
    metadata: Dict[str, Any] = field(default_factory=dict)  # å…ƒæ•°æ®

class TechnicalIndicatorGenerator:
    """æŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.indicators = {}
        logger.info("æŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_basic_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç”ŸæˆåŸºç¡€æŠ€æœ¯æŒ‡æ ‡"""
        try:
            features = pd.DataFrame(index=data.index)
            
            # ä»·æ ¼ç›¸å…³æŒ‡æ ‡
            features['price_change'] = data['close'].pct_change()
            features['price_change_abs'] = features['price_change'].abs()
            features['log_return'] = np.log(data['close'] / data['close'].shift(1))
            
            # ç§»åŠ¨å¹³å‡çº¿
            for period in [5, 10, 20, 50, 100, 200]:
                features[f'sma_{period}'] = data['close'].rolling(window=period).mean()
                features[f'ema_{period}'] = data['close'].ewm(span=period).mean()
                features[f'price_to_sma_{period}'] = data['close'] / features[f'sma_{period}']
                features[f'price_to_ema_{period}'] = data['close'] / features[f'ema_{period}']
            
            # å¸ƒæ—å¸¦
            for period in [20, 50]:
                sma = data['close'].rolling(window=period).mean()
                std = data['close'].rolling(window=period).std()
                features[f'bb_upper_{period}'] = sma + (2 * std)
                features[f'bb_lower_{period}'] = sma - (2 * std)
                features[f'bb_width_{period}'] = (features[f'bb_upper_{period}'] - features[f'bb_lower_{period}']) / sma
                features[f'bb_position_{period}'] = (data['close'] - features[f'bb_lower_{period}']) / (features[f'bb_upper_{period}'] - features[f'bb_lower_{period}'])
            
            # RSI
            for period in [14, 21, 30]:
                delta = data['close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
                rs = gain / loss
                features[f'rsi_{period}'] = 100 - (100 / (1 + rs))
            
            # MACD
            ema12 = data['close'].ewm(span=12).mean()
            ema26 = data['close'].ewm(span=26).mean()
            features['macd'] = ema12 - ema26
            features['macd_signal'] = features['macd'].ewm(span=9).mean()
            features['macd_histogram'] = features['macd'] - features['macd_signal']
            
            # éšæœºæŒ‡æ ‡
            for period in [14, 21]:
                low_min = data['low'].rolling(window=period).min()
                high_max = data['high'].rolling(window=period).max()
                features[f'stoch_k_{period}'] = 100 * (data['close'] - low_min) / (high_max - low_min)
                features[f'stoch_d_{period}'] = features[f'stoch_k_{period}'].rolling(window=3).mean()
            
            # å¨å»‰æŒ‡æ ‡
            for period in [14, 21]:
                high_max = data['high'].rolling(window=period).max()
                low_min = data['low'].rolling(window=period).min()
                features[f'williams_r_{period}'] = -100 * (high_max - data['close']) / (high_max - low_min)
            
            # æˆäº¤é‡æŒ‡æ ‡
            features['volume_sma_10'] = data['volume'].rolling(window=10).mean()
            features['volume_sma_20'] = data['volume'].rolling(window=20).mean()
            features['volume_ratio'] = data['volume'] / features['volume_sma_20']
            features['price_volume'] = data['close'] * data['volume']
            
            # OBV (On Balance Volume)
            features['obv'] = (np.sign(data['close'].diff()) * data['volume']).fillna(0).cumsum()
            
            # çœŸå®æ³¢åŠ¨èŒƒå›´ (ATR)
            high_low = data['high'] - data['low']
            high_close = np.abs(data['high'] - data['close'].shift())
            low_close = np.abs(data['low'] - data['close'].shift())
            true_range = np.maximum(high_low, np.maximum(high_close, low_close))
            features['atr_14'] = true_range.rolling(window=14).mean()
            features['atr_21'] = true_range.rolling(window=21).mean()
            
            logger.info(f"ç”ŸæˆåŸºç¡€æŠ€æœ¯æŒ‡æ ‡: {len(features.columns)}ä¸ª")
            return features
        
        except Exception as e:
            logger.error(f"ç”ŸæˆåŸºç¡€æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            return pd.DataFrame(index=data.index)
    
    def generate_advanced_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆé«˜çº§æŠ€æœ¯æŒ‡æ ‡"""
        try:
            features = pd.DataFrame(index=data.index)
            
            # å•†å“é€šé“æŒ‡æ•° (CCI)
            for period in [14, 20]:
                typical_price = (data['high'] + data['low'] + data['close']) / 3
                sma_tp = typical_price.rolling(window=period).mean()
                mad = typical_price.rolling(window=period).apply(lambda x: np.mean(np.abs(x - x.mean())))
                features[f'cci_{period}'] = (typical_price - sma_tp) / (0.015 * mad)
            
            # åŠ¨é‡æŒ‡æ ‡
            for period in [10, 14, 20]:
                features[f'momentum_{period}'] = data['close'] / data['close'].shift(period)
                features[f'roc_{period}'] = (data['close'] - data['close'].shift(period)) / data['close'].shift(period) * 100
            
            # ä»·æ ¼éœ‡è¡æŒ‡æ ‡
            for period in [14, 21]:
                features[f'price_oscillator_{period}'] = (data['close'] - data['close'].rolling(window=period).mean()) / data['close'].rolling(window=period).std()
            
            # æ”¯æ’‘é˜»åŠ›æŒ‡æ ‡
            for period in [20, 50]:
                features[f'support_{period}'] = data['low'].rolling(window=period).min()
                features[f'resistance_{period}'] = data['high'].rolling(window=period).max()
                features[f'support_distance_{period}'] = (data['close'] - features[f'support_{period}']) / data['close']
                features[f'resistance_distance_{period}'] = (features[f'resistance_{period}'] - data['close']) / data['close']
            
            # æ³¢åŠ¨ç‡æŒ‡æ ‡
            for period in [10, 20, 30]:
                features[f'volatility_{period}'] = data['close'].rolling(window=period).std()
                features[f'volatility_ratio_{period}'] = features[f'volatility_{period}'] / features[f'volatility_{period}'].rolling(window=period).mean()
            
            # è¶‹åŠ¿å¼ºåº¦æŒ‡æ ‡
            for period in [14, 21]:
                price_changes = data['close'].diff()
                up_moves = price_changes.where(price_changes > 0, 0)
                down_moves = -price_changes.where(price_changes < 0, 0)
                
                up_sum = up_moves.rolling(window=period).sum()
                down_sum = down_moves.rolling(window=period).sum()
                
                features[f'trend_strength_{period}'] = (up_sum - down_sum) / (up_sum + down_sum)
            
            # å¸‚åœºæ•ˆç‡æ¯”ç‡
            for period in [10, 20]:
                price_change = np.abs(data['close'] - data['close'].shift(period))
                volatility_sum = np.abs(data['close'].diff()).rolling(window=period).sum()
                features[f'efficiency_ratio_{period}'] = price_change / volatility_sum
            
            logger.info(f"ç”Ÿæˆé«˜çº§æŠ€æœ¯æŒ‡æ ‡: {len(features.columns)}ä¸ª")
            return features
        
        except Exception as e:
            logger.error(f"ç”Ÿæˆé«˜çº§æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            return pd.DataFrame(index=data.index)

class StatisticalFeatureGenerator:
    """ç»Ÿè®¡ç‰¹å¾ç”Ÿæˆå™¨"""
    
    def __init__(self):
        logger.info("ç»Ÿè®¡ç‰¹å¾ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_statistical_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆç»Ÿè®¡ç‰¹å¾"""
        try:
            features = pd.DataFrame(index=data.index)
            
            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            for period in [5, 10, 20, 50]:
                # å‡å€¼å’Œæ ‡å‡†å·®
                features[f'mean_{period}'] = data['close'].rolling(window=period).mean()
                features[f'std_{period}'] = data['close'].rolling(window=period).std()
                features[f'cv_{period}'] = features[f'std_{period}'] / features[f'mean_{period}']  # å˜å¼‚ç³»æ•°
                
                # ååº¦å’Œå³°åº¦
                features[f'skew_{period}'] = data['close'].rolling(window=period).skew()
                features[f'kurt_{period}'] = data['close'].rolling(window=period).kurt()
                
                # åˆ†ä½æ•°
                features[f'q25_{period}'] = data['close'].rolling(window=period).quantile(0.25)
                features[f'q75_{period}'] = data['close'].rolling(window=period).quantile(0.75)
                features[f'iqr_{period}'] = features[f'q75_{period}'] - features[f'q25_{period}']
                
                # æœ€å¤§æœ€å°å€¼
                features[f'max_{period}'] = data['close'].rolling(window=period).max()
                features[f'min_{period}'] = data['close'].rolling(window=period).min()
                features[f'range_{period}'] = features[f'max_{period}'] - features[f'min_{period}']
                
                # ç›¸å¯¹ä½ç½®
                features[f'position_in_range_{period}'] = (data['close'] - features[f'min_{period}']) / features[f'range_{period}']
            
            # é«˜çº§ç»Ÿè®¡ç‰¹å¾
            for period in [10, 20, 30]:
                # è‡ªç›¸å…³
                features[f'autocorr_{period}'] = data['close'].rolling(window=period).apply(
                    lambda x: x.autocorr() if len(x) > 1 else 0
                )
                
                # è¶‹åŠ¿ç‰¹å¾
                features[f'trend_slope_{period}'] = data['close'].rolling(window=period).apply(
                    lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0
                )
                
                # çº¿æ€§å›å½’RÂ²
                features[f'r_squared_{period}'] = data['close'].rolling(window=period).apply(
                    lambda x: np.corrcoef(range(len(x)), x)[0, 1]**2 if len(x) > 1 else 0
                )
            
            # ä»·æ ¼åˆ†å¸ƒç‰¹å¾
            for period in [20, 50]:
                # ä»·æ ¼åœ¨ä¸åŒåŒºé—´çš„åˆ†å¸ƒ
                rolling_data = data['close'].rolling(window=period)
                features[f'above_mean_ratio_{period}'] = rolling_data.apply(
                    lambda x: (x > x.mean()).sum() / len(x)
                )
                features[f'above_median_ratio_{period}'] = rolling_data.apply(
                    lambda x: (x > x.median()).sum() / len(x)
                )
            
            # æˆäº¤é‡ç»Ÿè®¡ç‰¹å¾
            for period in [10, 20]:
                features[f'volume_mean_{period}'] = data['volume'].rolling(window=period).mean()
                features[f'volume_std_{period}'] = data['volume'].rolling(window=period).std()
                features[f'volume_skew_{period}'] = data['volume'].rolling(window=period).skew()
                
                # ä»·æ ¼-æˆäº¤é‡å…³ç³»
                features[f'price_volume_corr_{period}'] = data['close'].rolling(window=period).corr(data['volume'])
            
            logger.info(f"ç”Ÿæˆç»Ÿè®¡ç‰¹å¾: {len(features.columns)}ä¸ª")
            return features
        
        except Exception as e:
            logger.error(f"ç”Ÿæˆç»Ÿè®¡ç‰¹å¾å¤±è´¥: {e}")
            return pd.DataFrame(index=data.index)

class TimeSeriesFeatureGenerator:
    """æ—¶é—´åºåˆ—ç‰¹å¾ç”Ÿæˆå™¨"""
    
    def __init__(self):
        logger.info("æ—¶é—´åºåˆ—ç‰¹å¾ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_time_series_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆæ—¶é—´åºåˆ—ç‰¹å¾"""
        try:
            features = pd.DataFrame(index=data.index)
            
            # æ»åç‰¹å¾
            for lag in [1, 2, 3, 5, 10, 20]:
                features[f'close_lag_{lag}'] = data['close'].shift(lag)
                features[f'volume_lag_{lag}'] = data['volume'].shift(lag)
                features[f'return_lag_{lag}'] = data['close'].pct_change().shift(lag)
            
            # å·®åˆ†ç‰¹å¾
            for order in [1, 2]:
                features[f'close_diff_{order}'] = data['close'].diff(order)
                features[f'volume_diff_{order}'] = data['volume'].diff(order)
            
            # å­£èŠ‚æ€§ç‰¹å¾ (åŸºäºæ—¶é—´ç´¢å¼•)
            if hasattr(data.index, 'hour'):
                features['hour'] = data.index.hour
                features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)
                features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)
            
            if hasattr(data.index, 'dayofweek'):
                features['dayofweek'] = data.index.dayofweek
                features['is_weekend'] = (features['dayofweek'] >= 5).astype(int)
            
            if hasattr(data.index, 'month'):
                features['month'] = data.index.month
                features['month_sin'] = np.sin(2 * np.pi * features['month'] / 12)
                features['month_cos'] = np.cos(2 * np.pi * features['month'] / 12)
            
            # å‘¨æœŸæ€§ç‰¹å¾
            for period in [24, 168, 720]:  # å°æ—¶ã€å‘¨ã€æœˆ
                if len(data) > period:
                    features[f'seasonal_{period}'] = data['close'].shift(period)
                    features[f'seasonal_diff_{period}'] = data['close'] - data['close'].shift(period)
            
            # è¶‹åŠ¿åˆ†è§£ç‰¹å¾
            for window in [50, 100, 200]:
                if len(data) > window:
                    # è¶‹åŠ¿
                    features[f'trend_{window}'] = data['close'].rolling(window=window, center=True).mean()
                    # å»è¶‹åŠ¿
                    features[f'detrended_{window}'] = data['close'] - features[f'trend_{window}']
            
            # å˜åŒ–ç‚¹æ£€æµ‹ç‰¹å¾
            for window in [20, 50]:
                # å‡å€¼å˜åŒ–
                mean_before = data['close'].rolling(window=window).mean()
                mean_after = data['close'].shift(-window).rolling(window=window).mean()
                features[f'mean_change_{window}'] = mean_after - mean_before
                
                # æ–¹å·®å˜åŒ–
                var_before = data['close'].rolling(window=window).var()
                var_after = data['close'].shift(-window).rolling(window=window).var()
                features[f'var_change_{window}'] = var_after - var_before
            
            # åºåˆ—æ¨¡å¼ç‰¹å¾
            for window in [5, 10, 20]:
                # è¿ç»­ä¸Šæ¶¨/ä¸‹è·Œå¤©æ•°
                price_change = data['close'].diff()
                up_streak = (price_change > 0).astype(int)
                down_streak = (price_change < 0).astype(int)
                
                features[f'up_streak_{window}'] = up_streak.rolling(window=window).sum()
                features[f'down_streak_{window}'] = down_streak.rolling(window=window).sum()
            
            logger.info(f"ç”Ÿæˆæ—¶é—´åºåˆ—ç‰¹å¾: {len(features.columns)}ä¸ª")
            return features
        
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ—¶é—´åºåˆ—ç‰¹å¾å¤±è´¥: {e}")
            return pd.DataFrame(index=data.index)

class CrossFeatureGenerator:
    """äº¤å‰ç‰¹å¾ç”Ÿæˆå™¨"""
    
    def __init__(self):
        logger.info("äº¤å‰ç‰¹å¾ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_cross_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆäº¤å‰ç‰¹å¾"""
        try:
            cross_features = pd.DataFrame(index=features.index)
            
            # é€‰æ‹©é‡è¦çš„åŸºç¡€ç‰¹å¾è¿›è¡Œäº¤å‰
            important_features = [
                'close', 'volume', 'sma_20', 'ema_20', 'rsi_14', 'macd',
                'bb_position_20', 'atr_14', 'volatility_20'
            ]
            
            # è¿‡æ»¤å­˜åœ¨çš„ç‰¹å¾
            available_features = [f for f in important_features if f in features.columns]
            
            # æ¯”ç‡ç‰¹å¾
            for i, feat1 in enumerate(available_features):
                for feat2 in available_features[i+1:]:
                    if features[feat2].abs().min() > 1e-8:  # é¿å…é™¤é›¶
                        cross_features[f'{feat1}_to_{feat2}_ratio'] = features[feat1] / features[feat2]
            
            # å·®å€¼ç‰¹å¾
            for i, feat1 in enumerate(available_features):
                for feat2 in available_features[i+1:]:
                    cross_features[f'{feat1}_{feat2}_diff'] = features[feat1] - features[feat2]
            
            # ä¹˜ç§¯ç‰¹å¾
            for i, feat1 in enumerate(available_features):
                for feat2 in available_features[i+1:]:
                    cross_features[f'{feat1}_{feat2}_product'] = features[feat1] * features[feat2]
            
            # æ¡ä»¶ç‰¹å¾
            if 'rsi_14' in features.columns and 'bb_position_20' in features.columns:
                cross_features['rsi_bb_signal'] = (
                    (features['rsi_14'] < 30) & (features['bb_position_20'] < 0.2)
                ).astype(int)
                cross_features['rsi_bb_overbought'] = (
                    (features['rsi_14'] > 70) & (features['bb_position_20'] > 0.8)
                ).astype(int)
            
            # åŠ¨é‡äº¤å‰ç‰¹å¾
            if 'macd' in features.columns and 'macd_signal' in features.columns:
                cross_features['macd_cross_up'] = (
                    (features['macd'] > features['macd_signal']) & 
                    (features['macd'].shift(1) <= features['macd_signal'].shift(1))
                ).astype(int)
                cross_features['macd_cross_down'] = (
                    (features['macd'] < features['macd_signal']) & 
                    (features['macd'].shift(1) >= features['macd_signal'].shift(1))
                ).astype(int)
            
            # ä»·æ ¼ä½ç½®ç‰¹å¾
            if 'close' in features.columns:
                for ma_col in [col for col in features.columns if 'sma_' in col or 'ema_' in col]:
                    cross_features[f'price_above_{ma_col}'] = (features['close'] > features[ma_col]).astype(int)
                    cross_features[f'price_distance_{ma_col}'] = (features['close'] - features[ma_col]) / features[ma_col]
            
            logger.info(f"ç”Ÿæˆäº¤å‰ç‰¹å¾: {len(cross_features.columns)}ä¸ª")
            return cross_features
        
        except Exception as e:
            logger.error(f"ç”Ÿæˆäº¤å‰ç‰¹å¾å¤±è´¥: {e}")
            return pd.DataFrame(index=features.index)

class FeatureSelector:
    """ç‰¹å¾é€‰æ‹©å™¨"""
    
    def __init__(self):
        self.selected_features = []
        self.feature_scores = {}
        logger.info("ç‰¹å¾é€‰æ‹©å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def select_features_by_importance(self, features: pd.DataFrame, target: pd.Series, 
                                    method: str = 'random_forest', k: int = 50) -> List[str]:
        """åŸºäºé‡è¦æ€§é€‰æ‹©ç‰¹å¾"""
        try:
            if not SKLEARN_AVAILABLE:
                logger.warning("Scikit-learnä¸å¯ç”¨ï¼Œè¿”å›æ‰€æœ‰ç‰¹å¾")
                return features.columns.tolist()
            
            # ç§»é™¤æ— æ•ˆç‰¹å¾
            valid_features = features.select_dtypes(include=[np.number]).dropna(axis=1)
            
            if len(valid_features.columns) == 0:
                return []
            
            # å¯¹é½æ•°æ®
            common_index = valid_features.index.intersection(target.index)
            X = valid_features.loc[common_index]
            y = target.loc[common_index]
            
            if len(X) == 0 or len(y) == 0:
                return []
            
            if method == 'random_forest':
                # éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§
                rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
                rf.fit(X, y)
                
                feature_importance = pd.DataFrame({
                    'feature': X.columns,
                    'importance': rf.feature_importances_
                }).sort_values('importance', ascending=False)
                
                selected = feature_importance.head(k)['feature'].tolist()
                self.feature_scores = dict(zip(feature_importance['feature'], feature_importance['importance']))
            
            elif method == 'correlation':
                # ç›¸å…³æ€§é€‰æ‹©
                correlations = X.corrwith(y).abs().sort_values(ascending=False)
                selected = correlations.head(k).index.tolist()
                self.feature_scores = correlations.to_dict()
            
            elif method == 'mutual_info':
                # äº’ä¿¡æ¯é€‰æ‹©
                mi_scores = mutual_info_regression(X, y, random_state=42)
                mi_df = pd.DataFrame({
                    'feature': X.columns,
                    'mi_score': mi_scores
                }).sort_values('mi_score', ascending=False)
                
                selected = mi_df.head(k)['feature'].tolist()
                self.feature_scores = dict(zip(mi_df['feature'], mi_df['mi_score']))
            
            else:
                # é»˜è®¤ä½¿ç”¨Fç»Ÿè®¡é‡
                selector = SelectKBest(score_func=f_regression, k=min(k, len(X.columns)))
                selector.fit(X, y)
                
                selected_mask = selector.get_support()
                selected = X.columns[selected_mask].tolist()
                
                scores = selector.scores_
                self.feature_scores = dict(zip(X.columns, scores))
            
            self.selected_features = selected
            logger.info(f"é€‰æ‹©ç‰¹å¾: {len(selected)}ä¸ª (æ–¹æ³•: {method})")
            
            return selected
        
        except Exception as e:
            logger.error(f"ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            return features.columns.tolist()[:k]  # è¿”å›å‰kä¸ªç‰¹å¾ä½œä¸ºå¤‡é€‰
    
    def remove_correlated_features(self, features: pd.DataFrame, threshold: float = 0.95) -> List[str]:
        """ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾"""
        try:
            # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            corr_matrix = features.corr().abs()
            
            # æ‰¾åˆ°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
            upper_triangle = corr_matrix.where(
                np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
            )
            
            # æ‰¾åˆ°éœ€è¦åˆ é™¤çš„ç‰¹å¾
            to_drop = [column for column in upper_triangle.columns 
                      if any(upper_triangle[column] > threshold)]
            
            # ä¿ç•™çš„ç‰¹å¾
            remaining_features = [col for col in features.columns if col not in to_drop]
            
            logger.info(f"ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾: {len(to_drop)}ä¸ªï¼Œä¿ç•™: {len(remaining_features)}ä¸ª")
            
            return remaining_features
        
        except Exception as e:
            logger.error(f"ç§»é™¤ç›¸å…³æ€§ç‰¹å¾å¤±è´¥: {e}")
            return features.columns.tolist()
    
    def select_stable_features(self, features: pd.DataFrame, stability_threshold: float = 0.1) -> List[str]:
        """é€‰æ‹©ç¨³å®šçš„ç‰¹å¾"""
        try:
            stable_features = []
            
            for col in features.columns:
                if features[col].dtype in [np.float64, np.int64]:
                    # è®¡ç®—å˜å¼‚ç³»æ•°ä½œä¸ºç¨³å®šæ€§æŒ‡æ ‡
                    mean_val = features[col].mean()
                    std_val = features[col].std()
                    
                    if mean_val != 0:
                        cv = std_val / abs(mean_val)
                        if cv > stability_threshold:  # å˜å¼‚ç³»æ•°å¤§äºé˜ˆå€¼è®¤ä¸ºæ˜¯ç¨³å®šçš„
                            stable_features.append(col)
            
            logger.info(f"é€‰æ‹©ç¨³å®šç‰¹å¾: {len(stable_features)}ä¸ª")
            return stable_features
        
        except Exception as e:
            logger.error(f"é€‰æ‹©ç¨³å®šç‰¹å¾å¤±è´¥: {e}")
            return features.columns.tolist()

class FeatureTransformer:
    """ç‰¹å¾å˜æ¢å™¨"""
    
    def __init__(self):
        self.scalers = {}
        self.transformers = {}
        logger.info("ç‰¹å¾å˜æ¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def scale_features(self, features: pd.DataFrame, method: ScalingMethod = ScalingMethod.STANDARD) -> pd.DataFrame:
        """ç¼©æ”¾ç‰¹å¾"""
        try:
            if not SKLEARN_AVAILABLE or method == ScalingMethod.NONE:
                return features
            
            scaled_features = features.copy()
            
            # é€‰æ‹©ç¼©æ”¾å™¨
            if method == ScalingMethod.STANDARD:
                scaler = StandardScaler()
            elif method == ScalingMethod.MINMAX:
                scaler = MinMaxScaler()
            elif method == ScalingMethod.ROBUST:
                scaler = RobustScaler()
            else:
                return features
            
            # åªç¼©æ”¾æ•°å€¼ç‰¹å¾
            numeric_columns = features.select_dtypes(include=[np.number]).columns
            
            if len(numeric_columns) > 0:
                scaled_data = scaler.fit_transform(features[numeric_columns])
                scaled_features[numeric_columns] = scaled_data
                
                # ä¿å­˜ç¼©æ”¾å™¨
                self.scalers[method.value] = scaler
            
            logger.info(f"ç‰¹å¾ç¼©æ”¾å®Œæˆ: {method.value}")
            return scaled_features
        
        except Exception as e:
            logger.error(f"ç‰¹å¾ç¼©æ”¾å¤±è´¥: {e}")
            return features
    
    def apply_pca(self, features: pd.DataFrame, n_components: int = 50) -> pd.DataFrame:
        """åº”ç”¨PCAé™ç»´"""
        try:
            if not SKLEARN_AVAILABLE:
                logger.warning("Scikit-learnä¸å¯ç”¨ï¼Œè·³è¿‡PCA")
                return features
            
            # åªå¯¹æ•°å€¼ç‰¹å¾åº”ç”¨PCA
            numeric_features = features.select_dtypes(include=[np.number]).dropna(axis=1)
            
            if len(numeric_features.columns) == 0:
                return features
            
            # åº”ç”¨PCA
            n_components = min(n_components, len(numeric_features.columns), len(numeric_features))
            pca = PCA(n_components=n_components, random_state=42)
            
            pca_features = pca.fit_transform(numeric_features)
            
            # åˆ›å»ºPCAç‰¹å¾DataFrame
            pca_columns = [f'pca_{i}' for i in range(n_components)]
            pca_df = pd.DataFrame(pca_features, index=features.index, columns=pca_columns)
            
            # ä¿å­˜å˜æ¢å™¨
            self.transformers['pca'] = pca
            
            logger.info(f"PCAé™ç»´å®Œæˆ: {len(numeric_features.columns)} -> {n_components}")
            return pca_df
        
        except Exception as e:
            logger.error(f"PCAé™ç»´å¤±è´¥: {e}")
            return features
    
    def apply_polynomial_features(self, features: pd.DataFrame, degree: int = 2, 
                                 max_features: int = 100) -> pd.DataFrame:
        """ç”Ÿæˆå¤šé¡¹å¼ç‰¹å¾"""
        try:
            # é€‰æ‹©é‡è¦ç‰¹å¾è¿›è¡Œå¤šé¡¹å¼å˜æ¢
            important_cols = features.columns[:min(10, len(features.columns))]  # é™åˆ¶ç‰¹å¾æ•°é‡
            selected_features = features[important_cols]
            
            poly_features = pd.DataFrame(index=features.index)
            
            # ç”ŸæˆäºŒæ¬¡é¡¹
            if degree >= 2:
                for i, col1 in enumerate(important_cols):
                    for col2 in important_cols[i:]:
                        if len(poly_features.columns) >= max_features:
                            break
                        
                        if col1 == col2:
                            poly_features[f'{col1}_squared'] = selected_features[col1] ** 2
                        else:
                            poly_features[f'{col1}_{col2}_interaction'] = selected_features[col1] * selected_features[col2]
            
            # ç”Ÿæˆä¸‰æ¬¡é¡¹ï¼ˆé™åˆ¶æ•°é‡ï¼‰
            if degree >= 3 and len(poly_features.columns) < max_features:
                for col in important_cols[:5]:  # åªå¯¹å‰5ä¸ªç‰¹å¾ç”Ÿæˆä¸‰æ¬¡é¡¹
                    if len(poly_features.columns) >= max_features:
                        break
                    poly_features[f'{col}_cubed'] = selected_features[col] ** 3
            
            logger.info(f"ç”Ÿæˆå¤šé¡¹å¼ç‰¹å¾: {len(poly_features.columns)}ä¸ª")
            return poly_features
        
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤šé¡¹å¼ç‰¹å¾å¤±è´¥: {e}")
            return pd.DataFrame(index=features.index)

class AutoFeatureEngineer:
    """è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹ä¸»ç±»"""
    
    def __init__(self):
        self.technical_generator = TechnicalIndicatorGenerator()
        self.statistical_generator = StatisticalFeatureGenerator()
        self.time_series_generator = TimeSeriesFeatureGenerator()
        self.cross_generator = CrossFeatureGenerator()
        self.feature_selector = FeatureSelector()
        self.feature_transformer = FeatureTransformer()
        
        # ç‰¹å¾é›†åˆå†å²
        self.feature_sets = {}
        
        logger.info("è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    async def generate_features(self, data: pd.DataFrame, target: pd.Series = None,
                              feature_types: List[FeatureType] = None,
                              max_features: int = 200) -> FeatureSet:
        """ç”Ÿæˆç‰¹å¾é›†åˆ"""
        try:
            if feature_types is None:
                feature_types = [FeatureType.TECHNICAL, FeatureType.STATISTICAL, 
                               FeatureType.TIME_SERIES, FeatureType.CROSS]
            
            all_features = pd.DataFrame(index=data.index)
            feature_info = {}
            
            # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
            if FeatureType.TECHNICAL in feature_types:
                logger.info("ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")
                basic_tech = self.technical_generator.generate_basic_indicators(data)
                advanced_tech = self.technical_generator.generate_advanced_indicators(data)
                
                tech_features = pd.concat([basic_tech, advanced_tech], axis=1)
                all_features = pd.concat([all_features, tech_features], axis=1)
                
                # è®°å½•ç‰¹å¾ä¿¡æ¯
                for col in tech_features.columns:
                    feature_info[col] = FeatureInfo(
                        name=col,
                        feature_type=FeatureType.TECHNICAL,
                        importance=0.0,
                        correlation=0.0,
                        stability=0.0,
                        description=f"æŠ€æœ¯æŒ‡æ ‡: {col}"
                    )
            
            # ç”Ÿæˆç»Ÿè®¡ç‰¹å¾
            if FeatureType.STATISTICAL in feature_types:
                logger.info("ç”Ÿæˆç»Ÿè®¡ç‰¹å¾...")
                stat_features = self.statistical_generator.generate_statistical_features(data)
                all_features = pd.concat([all_features, stat_features], axis=1)
                
                for col in stat_features.columns:
                    feature_info[col] = FeatureInfo(
                        name=col,
                        feature_type=FeatureType.STATISTICAL,
                        importance=0.0,
                        correlation=0.0,
                        stability=0.0,
                        description=f"ç»Ÿè®¡ç‰¹å¾: {col}"
                    )
            
            # ç”Ÿæˆæ—¶é—´åºåˆ—ç‰¹å¾
            if FeatureType.TIME_SERIES in feature_types:
                logger.info("ç”Ÿæˆæ—¶é—´åºåˆ—ç‰¹å¾...")
                ts_features = self.time_series_generator.generate_time_series_features(data)
                all_features = pd.concat([all_features, ts_features], axis=1)
                
                for col in ts_features.columns:
                    feature_info[col] = FeatureInfo(
                        name=col,
                        feature_type=FeatureType.TIME_SERIES,
                        importance=0.0,
                        correlation=0.0,
                        stability=0.0,
                        description=f"æ—¶é—´åºåˆ—ç‰¹å¾: {col}"
                    )
            
            # ç”Ÿæˆäº¤å‰ç‰¹å¾
            if FeatureType.CROSS in feature_types and len(all_features.columns) > 0:
                logger.info("ç”Ÿæˆäº¤å‰ç‰¹å¾...")
                cross_features = self.cross_generator.generate_cross_features(all_features)
                all_features = pd.concat([all_features, cross_features], axis=1)
                
                for col in cross_features.columns:
                    feature_info[col] = FeatureInfo(
                        name=col,
                        feature_type=FeatureType.CROSS,
                        importance=0.0,
                        correlation=0.0,
                        stability=0.0,
                        description=f"äº¤å‰ç‰¹å¾: {col}"
                    )
            
            # æ¸…ç†ç‰¹å¾
            all_features = self._clean_features(all_features)
            
            # ç‰¹å¾é€‰æ‹©
            if target is not None and len(all_features.columns) > max_features:
                logger.info("æ‰§è¡Œç‰¹å¾é€‰æ‹©...")
                
                # ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾
                selected_cols = self.feature_selector.remove_correlated_features(all_features, threshold=0.95)
                all_features = all_features[selected_cols]
                
                # åŸºäºé‡è¦æ€§é€‰æ‹©ç‰¹å¾
                if len(all_features.columns) > max_features:
                    selected_cols = self.feature_selector.select_features_by_importance(
                        all_features, target, method='random_forest', k=max_features
                    )
                    all_features = all_features[selected_cols]
                
                # æ›´æ–°ç‰¹å¾é‡è¦æ€§
                for col in selected_cols:
                    if col in feature_info and col in self.feature_selector.feature_scores:
                        feature_info[col].importance = self.feature_selector.feature_scores[col]
                        if target is not None:
                            feature_info[col].correlation = abs(all_features[col].corr(target))
            
            # åˆ›å»ºç‰¹å¾é›†åˆ
            feature_set = FeatureSet(
                features=all_features,
                feature_info=feature_info,
                target=target,
                metadata={
                    'generation_time': time.time(),
                    'data_shape': data.shape,
                    'feature_types': [ft.value for ft in feature_types],
                    'total_features': len(all_features.columns)
                }
            )
            
            logger.info(f"ç‰¹å¾ç”Ÿæˆå®Œæˆ: {len(all_features.columns)}ä¸ªç‰¹å¾")
            return feature_set
        
        except Exception as e:
            logger.error(f"ç‰¹å¾ç”Ÿæˆå¤±è´¥: {e}")
            return FeatureSet(
                features=pd.DataFrame(index=data.index),
                feature_info={},
                target=target
            )
    
    def _clean_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†ç‰¹å¾"""
        try:
            # ç§»é™¤æ— é™å€¼å’ŒNaNå€¼è¿‡å¤šçš„ç‰¹å¾
            cleaned_features = features.copy()
            
            for col in features.columns:
                # æ£€æŸ¥æ— é™å€¼
                if np.isinf(features[col]).any():
                    cleaned_features[col] = features[col].replace([np.inf, -np.inf], np.nan)
                
                # ç§»é™¤NaNå€¼è¿‡å¤šçš„ç‰¹å¾ï¼ˆè¶…è¿‡50%ï¼‰
                nan_ratio = cleaned_features[col].isna().sum() / len(cleaned_features)
                if nan_ratio > 0.5:
                    cleaned_features = cleaned_features.drop(columns=[col])
                    continue
                
                # å¡«å……å‰©ä½™çš„NaNå€¼
                if cleaned_features[col].isna().any():
                    if cleaned_features[col].dtype in [np.float64, np.int64]:
                        cleaned_features[col] = cleaned_features[col].fillna(cleaned_features[col].median())
                    else:
                        cleaned_features[col] = cleaned_features[col].fillna(0)
            
            # ç§»é™¤å¸¸æ•°ç‰¹å¾
            constant_features = []
            for col in cleaned_features.columns:
                if cleaned_features[col].nunique() <= 1:
                    constant_features.append(col)
            
            if constant_features:
                cleaned_features = cleaned_features.drop(columns=constant_features)
                logger.info(f"ç§»é™¤å¸¸æ•°ç‰¹å¾: {len(constant_features)}ä¸ª")
            
            return cleaned_features
        
        except Exception as e:
            logger.error(f"ç‰¹å¾æ¸…ç†å¤±è´¥: {e}")
            return features
    
    def get_feature_importance_report(self, feature_set: FeatureSet) -> pd.DataFrame:
        """è·å–ç‰¹å¾é‡è¦æ€§æŠ¥å‘Š"""
        try:
            report_data = []
            
            for name, info in feature_set.feature_info.items():
                report_data.append({
                    'feature_name': name,
                    'feature_type': info.feature_type.value,
                    'importance': info.importance,
                    'correlation': info.correlation,
                    'stability': info.stability,
                    'description': info.description
                })
            
            report_df = pd.DataFrame(report_data)
            report_df = report_df.sort_values('importance', ascending=False)
            
            return report_df
        
        except Exception as e:
            logger.error(f"ç”Ÿæˆç‰¹å¾é‡è¦æ€§æŠ¥å‘Šå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def save_feature_set(self, feature_set: FeatureSet, filepath: str):
        """ä¿å­˜ç‰¹å¾é›†åˆ"""
        try:
            import pickle
            
            with open(filepath, 'wb') as f:
                pickle.dump(feature_set, f)
            
            logger.info(f"ç‰¹å¾é›†åˆä¿å­˜å®Œæˆ: {filepath}")
        
        except Exception as e:
            logger.error(f"ä¿å­˜ç‰¹å¾é›†åˆå¤±è´¥: {e}")
    
    def load_feature_set(self, filepath: str) -> Optional[FeatureSet]:
        """åŠ è½½ç‰¹å¾é›†åˆ"""
        try:
            import pickle
            
            with open(filepath, 'rb') as f:
                feature_set = pickle.load(f)
            
            logger.info(f"ç‰¹å¾é›†åˆåŠ è½½å®Œæˆ: {filepath}")
            return feature_set
        
        except Exception as e:
            logger.error(f"åŠ è½½ç‰¹å¾é›†åˆå¤±è´¥: {e}")
            return None

# å…¨å±€è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹å®ä¾‹
auto_feature_engineer = AutoFeatureEngineer()


def initialize_auto_feature_engineering():
    """åˆå§‹åŒ–è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹ç³»ç»Ÿ"""
    from src.ai_enhanced.auto_feature_engineering import AutoFeatureEngineeringSystem
    system = AutoFeatureEngineeringSystem()
    logger.success("âœ… è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    return system


# åˆ›å»ºå…¨å±€å®ä¾‹ä¾›å¯¼å…¥ä½¿ç”¨
auto_feature_engineering = auto_feature_engineer
